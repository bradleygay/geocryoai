{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d630398e-6ef6-49c4-8f5b-0bd20838d151",
   "metadata": {},
   "source": [
    "# **GeoCryoAI Model | Modeling Codebase**\n",
    "---\n",
    "Decoding the Spatiotemporal Complexities of the Permafrost Carbon Feedback with Multimodal Ensemble Learning </br>\n",
    "Journal of Geophysical Research - Machine Learning and Computation (2024JH000402)</br>\n",
    "\n",
    "Bradley A. Gay, PhD | Jet Propulsion Laboratory, California Institute of Technology</br>\n",
    "31 December 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c620e23c-8cc8-46cc-bd7b-52e8e599a05c",
   "metadata": {},
   "source": [
    "# JGR-MLC Code\n",
    "To skip ahead to modeling analyses, access the ensemble tensors in the [ORNL DAAC repository](https://doi.org/10.3334/ORNLDAAC/2371), or reference the chunked parquet files in the GitHub subfolders and execute the `reassembly.py` script to stack the data chunks and generate the original dataframe (i.e., scaled, detrended, and normalized). Thereafter, navigate to the **Tuning** heading below. To proceed forward with the training, validation, and testing formulation, and model simulation code, access the code following the **Load** heading. </br>\n",
    "</br>\n",
    "For a more intricate deluge into the details associated with creating each individual dataset that was concatenated together to form the base dataset used for training the GeoCryoAI framework, navigate to the Jupyter notebook located in the _/geocryoai/preprocessing/code/notebook directory and proceed forward with the individual headings labeled, **In Situ**, **UAVSAR**, **AVIRIS-NG**, **SIBBORK-TTE**, and **TCFM-Arctic** followed by the concatenation of these individual datasets into the harmonized base dataset used for model development, simulations, and forecasts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a31e1ff1-987d-4000-8738-be96df4d5d06",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Libraries and Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba947686-c86a-4607-a80c-f599f27f28e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"1\"\n",
    "import os, sys\n",
    "import datetime\n",
    "import re\n",
    "import glob\n",
    "#from glob import glob,iglob\n",
    "import requests\n",
    "import warnings, intake\n",
    "import shutil\n",
    "#os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "#\n",
    "import bs4\n",
    "import eofs\n",
    "import pyts\n",
    "import pyarrow\n",
    "import fastparquet\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import subprocess\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow as pa\n",
    "from tqdm import tqdm\n",
    "#import polars\n",
    "#import polars as pl\n",
    "import pickle\n",
    "import math\n",
    "import time\n",
    "import cftime\n",
    "import netCDF4 as nc\n",
    "import xarray as xr\n",
    "from netCDF4 import Dataset\n",
    "import h5py\n",
    "os.environ['HDF5_USE_FILE_LOCKING']='FALSE'\n",
    "import progressbar\n",
    "from dask.diagnostics import ProgressBar\n",
    "ProgressBar().register()\n",
    "#\n",
    "import sklearn\n",
    "#!pip install --upgrade tensorflow\n",
    "import tensorflow as tf\n",
    "tf.config.run_functions_eagerly(True)\n",
    "#import tensorflow_addons\n",
    "import tensorflow.keras\n",
    "import tensorflow.keras.backend as K\n",
    "import keras.backend as K\n",
    "import keras.optimizers\n",
    "import keras_tuner\n",
    "import statsmodels\n",
    "import pyrsgis\n",
    "#\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as col\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.colors as col\n",
    "import matplotlib.cm as cm\n",
    "%matplotlib inline\n",
    "mpl.rcParams['agg.path.chunksize'] = 10000\n",
    "import seaborn as sns\n",
    "sns.set(font_scale=1.5, style=\"white\")\n",
    "import csv\n",
    "import io\n",
    "#\n",
    "import dask.dataframe as dd\n",
    "import geopandas as gpd\n",
    "import rioxarray as rxr\n",
    "import spectral\n",
    "import earthpy as et\n",
    "import geopandas as gpd\n",
    "import utm\n",
    "#\n",
    "import pydot\n",
    "import pydotplus\n",
    "import graphviz\n",
    "import datetime\n",
    "import cv2 as cv\n",
    "import rasterio\n",
    "import tiledb\n",
    "import polars as pl\n",
    "import pyproj\n",
    "import dask.dataframe as dd\n",
    "import tqdm\n",
    "import rasterio\n",
    "import rasterio as rio\n",
    "import rioxarray\n",
    "import pandas as pd\n",
    "import psutil\n",
    "import dask.dataframe as dd\n",
    "import time\n",
    "import memory_profiler\n",
    "import zipfile\n",
    "import getpass\n",
    "import pyinterp\n",
    "import pyinterp.backends.xarray\n",
    "import requests\n",
    "import h5py\n",
    "import rasterio\n",
    "import h5py\n",
    "import pyproj\n",
    "import affine\n",
    "import rasterio\n",
    "import rioxarray\n",
    "import matplotlib.pyplot as plt\n",
    "import dask\n",
    "import dask.array as da\n",
    "import scipy\n",
    "import tensorflow as tf\n",
    "import warnings\n",
    "import cdsapi\n",
    "import libpysal\n",
    "#\n",
    "import rasterio\n",
    "import codecs # for text parsing code\n",
    "import netrc\n",
    "import rioxarray as rxr\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import dask.dataframe as dd\n",
    "import time\n",
    "import tensorboard\n",
    "tensorboard.__version__\n",
    "# Clear logs\n",
    "#rm -rf ./logs/\n",
    "# Load TensorBoard\n",
    "%load_ext tensorboard\n",
    "import gc\n",
    "##\n",
    "\n",
    "from osgeo import gdal\n",
    "from pyproj import Proj, Transformer, CRS\n",
    "from affine import Affine\n",
    "from rasterio import enums\n",
    "from rasterio.enums import Resampling\n",
    "from rasterio.warp import calculate_default_transform, reproject\n",
    "from rasterio.warp import calculate_default_transform, reproject, Resampling\n",
    "from rasterio.errors import NotGeoreferencedWarning\n",
    "from collections import defaultdict\n",
    "from functools import partial\n",
    "from dask import delayed, compute\n",
    "from dask.diagnostics import ProgressBar\n",
    "from scipy.ndimage import zoom\n",
    "from scipy.interpolate import griddata\n",
    "from scipy.ndimage import zoom\n",
    "from uncertain_panda import pandas as up\n",
    "from numpy import isnan, array, count_nonzero\n",
    "from pandas import read_csv, DataFrame, concat\n",
    "from collections import defaultdict\n",
    "from openpyxl import Workbook\n",
    "from itertools import groupby, islice\n",
    "from zipfile import ZipFile\n",
    "from operator import itemgetter\n",
    "from pathlib import Path\n",
    "from datetime import datetime as dt\n",
    "from datetime import timedelta\n",
    "from pyrsgis import raster\n",
    "#\n",
    "from pywaffle import Waffle\n",
    "from eofs.standard import Eof\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "#\n",
    "from sklearn import linear_model\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler, PowerTransformer, Normalizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error, accuracy_score, \\\n",
    "confusion_matrix, ConfusionMatrixDisplay, consensus_score, explained_variance_score,r2_score, \\\n",
    "roc_auc_score, roc_curve\n",
    "from sklearn.metrics import jaccard_score, mean_squared_error, nan_euclidean_distances, precision_score\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.callbacks import *\n",
    "from tensorflow.keras import mixed_precision\n",
    "from tensorflow.python.client import device_lib\n",
    "from keras_tuner.engine.hyperparameters import HyperParameters\n",
    "from keras_tuner.tuners import RandomSearch, BayesianOptimization, Hyperband\n",
    "from keras_tuner import HyperModel\n",
    "#import tensorflow_addons as tfa\n",
    "#from tensorflow_addons.metrics.r_square import RSquare\n",
    "from keras_tuner.tuners import *\n",
    "from keras import backend as K\n",
    "from keras import optimizers \n",
    "from keras.preprocessing import *\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, InputLayer, Dropout, Flatten, BatchNormalization, Conv1D, Bidirectional\n",
    "from keras.layers import *\n",
    "from keras.activations import swish, elu, gelu, selu, sigmoid, relu, tanh, linear, softmax, swish\n",
    "from keras.layers import MaxPool1D\n",
    "from keras import *\n",
    "from keras.models import Model, Sequential\n",
    "#\n",
    "from spectral import *\n",
    "from libpysal import weights\n",
    "from libpysal.weights import lat2W\n",
    "from esda.moran import Moran\n",
    "#\n",
    "from keras.layers import Reshape\n",
    "from keras import layers\n",
    "from keras import models\n",
    "from keras.utils import plot_model\n",
    "from datetime import datetime\n",
    "from dateutil.rrule import DAILY,rrule\n",
    "from time import gmtime, strftime \n",
    "#\n",
    "from rasterio.enums import Resampling\n",
    "from rasterio.mask import mask\n",
    "from rasterio.plot import show\n",
    "from rasterio.transform import from_origin\n",
    "from pyrsgis.ml import array_to_chips\n",
    "from shapely import geometry\n",
    "from pyproj import Proj, transform\n",
    "from pyrsgis import ml\n",
    "from pyrsgis import ml, raster, convert\n",
    "from itertools import chain\n",
    "from shapely.geometry import Point, box\n",
    "from pyproj import Proj, transform, CRS\n",
    "from scipy.interpolate import griddata\n",
    "from scipy.stats import bootstrap\n",
    "from scipy.interpolate import interpn\n",
    "from rasterio.mask import mask\n",
    "from rasterio import sample\n",
    "from scipy.interpolate import Rbf\n",
    "from scipy.interpolate import griddata\n",
    "from scipy.spatial import cKDTree\n",
    "from rasterio.plot import show\n",
    "from scipy.interpolate import griddata\n",
    "from scipy.interpolate import interp1d\n",
    "#\n",
    "from dask import delayed, compute\n",
    "from dask.distributed import Client, LocalCluster\n",
    "from tqdm import tqdm\n",
    "from dask import delayed, compute\n",
    "from dask.distributed import Client, LocalCluster\n",
    "from rasterio.windows import Window\n",
    "from rasterio.warp import calculate_default_transform, reproject, Resampling\n",
    "from rasterio.plot import show\n",
    "from rasterio.plot import show_hist\n",
    "from rasterio.transform import from_origin\n",
    "from tqdm import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "from osgeo import gdal\n",
    "from rasterio.windows import Window\n",
    "\n",
    "##########If error thrown, Just run cell again!\n",
    "\n",
    "from netCDF4 import Dataset\n",
    "from datetime import datetime\n",
    "from packaging import version\n",
    "from osgeo import gdal\n",
    "from osgeo import gdal_array\n",
    "from pyproj import Transformer, ProjError, Proj\n",
    "from IPython.display import display\n",
    "#\n",
    "from skimage import data, io   # Import skimage library (data - Test images and example data.\n",
    "#                          io - Reading, saving, and displaying images.)\n",
    "from skimage.color import rgb2gray\n",
    "from pandas.plotting import autocorrelation_plot\n",
    "#\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from tensorflow.keras import utils\n",
    "from tensorflow.keras.models import Sequential,load_model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow import keras\n",
    "from tensorflow.python.keras.utils import conv_utils, tf_utils\n",
    "#from tensorflow_addons.metrics.r_square import RSquare\n",
    "from keras_tuner import HyperModel\n",
    "from keras_tuner.engine.hyperparameters import HyperParameters\n",
    "from keras_tuner.tuners import RandomSearch, BayesianOptimization, Hyperband\n",
    "#\n",
    "from uncertain_panda import pandas as pd\n",
    "tqdm.pandas()\n",
    "#\n",
    "from esda.moran import Moran\n",
    "from libpysal.weights import KNN  # libpysal now contains the weights module\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statsmodels.stats.diagnostic import acorr_ljungbox\n",
    "from statsmodels.stats.stattools import durbin_watson\n",
    "from statsmodels.tsa.stattools import acf\n",
    "from statsmodels.tsa.stattools import adfuller, acf, pacf, kpss\n",
    "from esda.moran import Moran\n",
    "from esda.geary import Geary\n",
    "from pysal.lib import weights\n",
    "from scipy.stats import linregress\n",
    "from libpysal.weights import KNN\n",
    "from shapely.geometry import Point\n",
    "\n",
    "print(\"TensorFlow version: \", tf.__version__)\n",
    "assert version.parse(tf.__version__).release[0] >= 2, \\\n",
    "    \"This notebook requires TensorFlow 2.0 or above.\"\n",
    "\n",
    "print(\"==========================\")\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "print(sys.getrecursionlimit())\n",
    "sys.setrecursionlimit(1000000000)\n",
    "print(sys.getrecursionlimit())\n",
    "\n",
    "\n",
    "\n",
    "##########################\n",
    "##FUNCTIONS##\n",
    "##########################\n",
    "\n",
    "def divisorGenerator(n):\n",
    "    large_divisors = []\n",
    "    for i in range(1, int(math.sqrt(n) + 1)):\n",
    "        if n % i == 0:\n",
    "            yield i\n",
    "            if i*i != n:\n",
    "                large_divisors.append(n / i)\n",
    "    for divisor in reversed(large_divisors):\n",
    "        yield divisor\n",
    "\n",
    "def series_to_supervised(data, lags = 1, forecasting_steps = 1, dropna=True):\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = pd.DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    for i in range(lags, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [(df.columns[j], str('t-%d') %  i) for j in range(n_vars)]\n",
    "    for a in range(0, forecasting_steps):\n",
    "        cols.append(df.shift(-a))\n",
    "        if a == 0:\n",
    "            names += [(df.columns[b], str('t')) for b in range(n_vars)]\n",
    "        else:\n",
    "            names += [(df.columns[b], str('t+%d') %  a) for b in range(n_vars)]\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    agg = agg.loc[:,~agg.columns.duplicated()]\n",
    "    if dropna:\n",
    "        agg.dropna(inplace=True)\n",
    "    return agg\n",
    "\n",
    "def haversine(lon1, lat1, lon2, lat2):\n",
    "    \"\"\"\n",
    "    Calculate the great-circle distance between two points on the Earth.\n",
    "    :param lon1: Longitude of the first point in decimal degrees.\n",
    "    :param lat1: Latitude of the first point in decimal degrees.\n",
    "    :param lon2: Longitude of the second point in decimal degrees.\n",
    "    :param lat2: Latitude of the second point in decimal degrees.\n",
    "    :return: Distance between the two points in meters.\n",
    "    \"\"\"\n",
    "    # Radius of the Earth in meters\n",
    "    R = 6371000\n",
    "    # Convert decimal degrees to radians\n",
    "    lon1, lat1, lon2, lat2 = map(math.radians, [lon1, lat1, lon2, lat2])\n",
    "    # Differences in coordinates\n",
    "    dlon = lon2 - lon1\n",
    "    dlat = lat2 - lat1\n",
    "    # Haversine formula\n",
    "    a = math.sin(dlat / 2)**2 + math.cos(lat1) * math.cos(lat2) * math.sin(dlon / 2)**2\n",
    "    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))\n",
    "    # Distance in meters\n",
    "    distance = R * c\n",
    "    return distance\n",
    "\n",
    "def fast_flatten(input_list):\n",
    "    return list(chain.from_iterable(input_list))\n",
    "\n",
    "def convert_to_numeric(value):\n",
    "    if isinstance(value, str):\n",
    "        if value == '-9999':\n",
    "            return np.nan\n",
    "        try:\n",
    "            return int(value)\n",
    "        except ValueError:\n",
    "            try:\n",
    "                return float(value)\n",
    "            except ValueError:\n",
    "                return value\n",
    "    return value\n",
    "\n",
    "def convert_to_datetime_underscore(match):\n",
    "    parts = match.group(1).split('_')\n",
    "    if len(parts) == 3:\n",
    "        day = int(parts[1])\n",
    "        month = int(parts[0])\n",
    "        year = int(parts[2])\n",
    "        if year < 100:\n",
    "            year += 2000\n",
    "        try:\n",
    "            date = datetime(year, month, day)\n",
    "            return date.strftime('%Y%m%d')\n",
    "        except ValueError:\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "def convert_to_datetime_slash(match):\n",
    "    parts = match.group(1).split('/')\n",
    "    if len(parts) == 3:\n",
    "        month = int(parts[0])\n",
    "        day = int(parts[1])\n",
    "        year = int(parts[2])\n",
    "        try:\n",
    "            date = datetime(year, month, day)\n",
    "            return date.strftime('%Y%m%d')\n",
    "        except ValueError:\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "def process_csv(file_path):\n",
    "    try:\n",
    "        df = pd.read_csv(file_path, encoding='utf-8')\n",
    "    except UnicodeDecodeError:\n",
    "        df = pd.read_csv(file_path, encoding='ISO-8859-1')\n",
    "    \n",
    "    required_columns = ['collectDate', 'siteID', 'decimalLatitude', 'decimalLongitude']\n",
    "    thaw_depth_columns = [col for col in df.columns if 'thawProbeDepth' in col]\n",
    "    \n",
    "    if not all(col in df.columns for col in required_columns):\n",
    "        #print(f\"Skipping file {file_path} due to missing required columns\")\n",
    "        return None\n",
    "    \n",
    "    columns_to_extract = required_columns + thaw_depth_columns\n",
    "    \n",
    "    # Extract the necessary columns\n",
    "    df_extracted = df[columns_to_extract]\n",
    "    \n",
    "    # Melt the dataframe to have thawProbeDepth columns as rows\n",
    "    df_melted = df_extracted.melt(\n",
    "        id_vars=required_columns,\n",
    "        value_vars=thaw_depth_columns,\n",
    "        var_name='thawProbeDepthType',\n",
    "        value_name='thawProbeDepth'\n",
    "    )\n",
    "    return df_melted\n",
    "\n",
    "def decimal_year_to_datetime(decimal_years):\n",
    "    dates = []\n",
    "    for decimal_year in decimal_years:\n",
    "        year = int(decimal_year)\n",
    "        remainder = decimal_year - year\n",
    "        start_of_year = datetime(year, 1, 1)\n",
    "        days_in_year = (datetime(year + 1, 1, 1) - start_of_year).days\n",
    "        date = start_of_year + timedelta(days=remainder * days_in_year)\n",
    "        dates.append(date)\n",
    "    return np.array(dates)\n",
    "\n",
    "def enhanced_save_as_xlsx(file_path, df_dict):\n",
    "    with pd.ExcelWriter(file_path, engine='xlsxwriter') as writer:\n",
    "        for sheet_name, df in df_dict.items():\n",
    "            df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "            \n",
    "def move_files(source_folder, destination_folder):\n",
    "    for filename in os.listdir(source_folder):\n",
    "        shutil.move(os.path.join(source_folder, filename), os.path.join(destination_folder, filename))\n",
    "\n",
    "# def extract_keywords_from_excel_robust(file_path):\n",
    "#     try:\n",
    "#         # Load the Excel file\n",
    "#         xls = pd.ExcelFile(file_path)\n",
    "#         # Get all sheet names\n",
    "#         sheets = xls.sheet_names\n",
    "#         # Container for keywords\n",
    "#         keywords = set()\n",
    "#         # Loop over all sheets\n",
    "#         for sheet in sheets:\n",
    "#             # Read the sheet\n",
    "#             df = pd.read_excel(xls, sheet_name=sheet)\n",
    "#             # Extract column names (assuming they are keywords)\n",
    "#             keywords.update(df.columns.str.strip().tolist())\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error processing {file_path}: {str(e)}\")\n",
    "#         return None\n",
    "#     return keywords\n",
    "\n",
    "def extract_keywords_from_excel_robust(file_path):\n",
    "    try:\n",
    "        xls = pd.ExcelFile(file_path)  # Load the Excel file\n",
    "        keywords = set()\n",
    "        for sheet in xls.sheet_names:  # Iterate through each sheet\n",
    "            df = pd.read_excel(xls, sheet_name=sheet)\n",
    "            # Collect all headers, converting each to string for uniform processing\n",
    "            keywords.update(map(str, df.columns))\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {str(e)}\")\n",
    "        return None\n",
    "    return keywords\n",
    "\n",
    "def convert_to_snake_case(name):\n",
    "    # Convert to lowercase\n",
    "    name = name.lower()\n",
    "    # Replace spaces and hyphens with underscores\n",
    "    name = re.sub(r'[\\s\\-]+', '_', name)\n",
    "    # Remove invalid characters, keeping only alphanumerics, underscores, and dots (for file extensions)\n",
    "    name = re.sub(r'[^a-z0-9_\\.]', '', name)\n",
    "    return name\n",
    "\n",
    "def rename_files_in_directory(root_dir):\n",
    "    for root, dirs, files in os.walk(root_dir, topdown=False):\n",
    "        # Rename files\n",
    "        for filename in files:\n",
    "            new_filename = convert_to_snake_case(filename)\n",
    "            old_file_path = os.path.join(root, filename)\n",
    "            new_file_path = os.path.join(root, new_filename)\n",
    "\n",
    "            if old_file_path != new_file_path:\n",
    "                os.rename(old_file_path, new_file_path)\n",
    "                print(f\"Renamed file: {old_file_path} -> {new_file_path}\")\n",
    "\n",
    "        # Rename directories\n",
    "        for dirname in dirs:\n",
    "            new_dirname = convert_to_snake_case(dirname)\n",
    "            old_dir_path = os.path.join(root, dirname)\n",
    "            new_dir_path = os.path.join(root, new_dirname)\n",
    "\n",
    "            if old_dir_path != new_dir_path:\n",
    "                os.rename(old_dir_path, new_dir_path)\n",
    "                print(f\"Renamed directory: {old_dir_path} -> {new_dir_path}\")\n",
    "\n",
    "def extract_site_id(filename):\n",
    "    return os.path.basename(filename).split('_')[1]\n",
    "    \n",
    "def get_replicate_columns(base_var, df):\n",
    "    #pattern = rf'^{base_var}(_\\d+)*$'\n",
    "    pattern = rf'^{base_var}(_[A-Z0-9]+)*(_PI_F)*$'\n",
    "    return [col for col in df.columns if re.match(pattern, col)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd6388b8-7f86-4e9e-9d6f-09c53ecf2623",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# import sys\n",
    "# import math\n",
    "# import time\n",
    "# import warnings\n",
    "# import shutil\n",
    "# import subprocess\n",
    "# from datetime import datetime, timedelta\n",
    "# from collections import defaultdict\n",
    "# from functools import partial\n",
    "# from itertools import chain\n",
    "# from pathlib import Path\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# # Environmental settings\n",
    "# os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"1\"\n",
    "# os.environ['HDF5_USE_FILE_LOCKING'] = 'FALSE'\n",
    "\n",
    "# # Suppress warnings\n",
    "# warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "\n",
    "# # General-purpose libraries\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import pickle\n",
    "# import csv\n",
    "# import io\n",
    "# import glob\n",
    "# import re\n",
    "\n",
    "# # Visualization libraries\n",
    "# import matplotlib as mpl\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# sns.set(font_scale=1.5, style=\"white\")\n",
    "# %matplotlib inline\n",
    "\n",
    "# # Geospatial and data processing\n",
    "# import geopandas as gpd\n",
    "# import rasterio\n",
    "# import rioxarray as rxr\n",
    "# import xarray as xr\n",
    "# import dask\n",
    "# import dask.dataframe as dd\n",
    "# from dask import delayed, compute\n",
    "# from dask.diagnostics import ProgressBar\n",
    "# ProgressBar().register()\n",
    "\n",
    "# # Scientific computation and interpolation\n",
    "# from scipy.interpolate import griddata, Rbf, interp1d\n",
    "# from scipy.ndimage import zoom\n",
    "# from scipy.stats import bootstrap, linregress\n",
    "# from scipy.spatial import cKDTree\n",
    "\n",
    "# # TensorFlow and machine learning\n",
    "# import tensorflow as tf\n",
    "# from tensorflow import keras\n",
    "# from tensorflow.keras import layers, models, callbacks, optimizers\n",
    "# from keras_tuner import HyperModel, RandomSearch, BayesianOptimization, Hyperband\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler\n",
    "# from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# # Statsmodels and spatial statistics\n",
    "# import statsmodels.api as sm\n",
    "# from statsmodels.tsa.stattools import adfuller, kpss\n",
    "# from esda.moran import Moran\n",
    "# from libpysal.weights import lat2W\n",
    "\n",
    "# # Geospatial utilities\n",
    "# from shapely.geometry import Point, box\n",
    "# from pyproj import Transformer, CRS\n",
    "\n",
    "# # File handling\n",
    "# import zipfile\n",
    "# from osgeo import gdal\n",
    "\n",
    "# # Debugging and system utilities\n",
    "# import gc\n",
    "# import psutil\n",
    "\n",
    "# # Print TensorFlow info\n",
    "# print(f\"TensorFlow version: {tf.__version__}\")\n",
    "# print(f\"Num GPUs Available: {len(tf.config.list_physical_devices('GPU'))}\")\n",
    "\n",
    "# # Recursion limit setup\n",
    "# print(\"Default Recursion Limit:\", sys.getrecursionlimit())\n",
    "# sys.setrecursionlimit(10**6)\n",
    "# print(\"Updated Recursion Limit:\", sys.getrecursionlimit())\n",
    "\n",
    "# # Function definitions\n",
    "# def divisorGenerator(n):\n",
    "#     for i in range(1, int(math.sqrt(n)) + 1):\n",
    "#         if n % i == 0:\n",
    "#             yield i\n",
    "#             if i * i != n:\n",
    "#                 yield n // i\n",
    "\n",
    "# def series_to_supervised(data, lags=1, forecasting_steps=1, dropna=True):\n",
    "#     df = pd.DataFrame(data)\n",
    "#     cols, names = [], []\n",
    "#     for i in range(lags, 0, -1):\n",
    "#         cols.append(df.shift(i))\n",
    "#         names += [(df.columns[j], f't-{i}') for j in range(df.shape[1])]\n",
    "#     for i in range(forecasting_steps):\n",
    "#         cols.append(df.shift(-i))\n",
    "#         names += [(df.columns[j], f't+{i}') if i else (df.columns[j], 't') for j in range(df.shape[1])]\n",
    "#     agg = pd.concat(cols, axis=1)\n",
    "#     agg.columns = names\n",
    "#     if dropna:\n",
    "#         agg.dropna(inplace=True)\n",
    "#     return agg\n",
    "\n",
    "# def haversine(lon1, lat1, lon2, lat2):\n",
    "#     R = 6371000  # Earth's radius in meters\n",
    "#     lon1, lat1, lon2, lat2 = map(math.radians, [lon1, lat1, lon2, lat2])\n",
    "#     dlon, dlat = lon2 - lon1, lat2 - lat1\n",
    "#     a = math.sin(dlat / 2)**2 + math.cos(lat1) * math.cos(lat2) * math.sin(dlon / 2)**2\n",
    "#     return 2 * R * math.atan2(math.sqrt(a), math.sqrt(1 - a))\n",
    "\n",
    "# def fast_flatten(input_list):\n",
    "#     return list(chain.from_iterable(input_list))\n",
    "\n",
    "# def convert_to_snake_case(name):\n",
    "#     return re.sub(r'[^a-z0-9_\\.]', '', re.sub(r'[\\s\\-]+', '_', name.lower()))\n",
    "\n",
    "# def rename_files_in_directory(root_dir):\n",
    "#     for root, dirs, files in os.walk(root_dir):\n",
    "#         for filename in files:\n",
    "#             new_filename = convert_to_snake_case(filename)\n",
    "#             os.rename(os.path.join(root, filename), os.path.join(root, new_filename))\n",
    "\n",
    "# def decimal_year_to_datetime(decimal_years):\n",
    "#     dates = []\n",
    "#     for decimal_year in decimal_years:\n",
    "#         year = int(decimal_year)\n",
    "#         remainder = decimal_year - year\n",
    "#         start_of_year = datetime(year, 1, 1)\n",
    "#         days_in_year = (datetime(year + 1, 1, 1) - start_of_year).days\n",
    "#         date = start_of_year + timedelta(days=remainder * days_in_year)\n",
    "#         dates.append(date)\n",
    "#     return np.array(dates)\n",
    "\n",
    "# def process_csv(file_path):\n",
    "#     try:\n",
    "#         df = pd.read_csv(file_path, encoding='utf-8')\n",
    "#     except UnicodeDecodeError:\n",
    "#         df = pd.read_csv(file_path, encoding='ISO-8859-1')\n",
    "#     required_columns = ['collectDate', 'siteID', 'decimalLatitude', 'decimalLongitude']\n",
    "#     thaw_depth_columns = [col for col in df.columns if 'thawProbeDepth' in col]\n",
    "#     if not all(col in df.columns for col in required_columns):\n",
    "#         return None\n",
    "#     columns_to_extract = required_columns + thaw_depth_columns\n",
    "#     df_extracted = df[columns_to_extract]\n",
    "#     df_melted = df_extracted.melt(\n",
    "#         id_vars=required_columns,\n",
    "#         value_vars=thaw_depth_columns,\n",
    "#         var_name='thawProbeDepthType',\n",
    "#         value_name='thawProbeDepth'\n",
    "#     )\n",
    "#     return df_melted\n",
    "\n",
    "# def enhanced_save_as_xlsx(file_path, df_dict):\n",
    "#     with pd.ExcelWriter(file_path, engine='xlsxwriter') as writer:\n",
    "#         for sheet_name, df in df_dict.items():\n",
    "#             df.to_excel(writer, sheet_name=sheet_name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace5382c-3bd2-4dde-93a1-70cb56464c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27928e9a-9a96-404b-bbe2-4d493fbe3073",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Other Functionality Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f827371f-4ba6-4d4e-a1ee-b4cc1b266ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To disable mixed precision and verbosely assign float32 data type to output (saves computational cost):\n",
    "mixed_precision.set_global_policy('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8641cbb6-4e1f-4319-8987-0f91d36576a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you do not want to use the first GPU and instead the CPU\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eee9031-eb41-4b13-b3a1-304feb83d227",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you want to use the first GPU\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a652c5-8235-4539-afa7-bb85426fa7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7132fa0-1087-4116-9dae-7dd76de37c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Clean up filenaming nomenclature ###\n",
    "########################################\n",
    "\n",
    "#root_dir='/Users/bgay/NEON_permafrost-measures/'\n",
    "#rename_files_in_directory(root_dir)\n",
    "\n",
    "#root_dir='/Volumes/JPL/geocryoai/insitu/alt/calm/source/'\n",
    "#rename_files_in_directory(root_dir)\n",
    "\n",
    "#root_dir='/Volumes/JPL/geocryoai/insitu/alt/gtnp/source/'\n",
    "#rename_files_in_directory(root_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a03fe25-08b4-4c9b-9548-ec263b2ef9b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ce156374-af9b-4d8b-a2d9-54d35ff4493b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2888775-ec2d-47da-a391-ad8eb50f539a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Cleaning datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b152ef4c-68bc-4221-852c-5cfe7865c3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet('/Users/bgay/Downloads/final_fcfch4alt_monthly_1km_ds.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bbc8fbf-e026-4b9f-9da4-e89d48e0b63d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#df.alt.isna().sum()\n",
    "#96654894\n",
    "#df.fc.isna().sum()\n",
    "#3163257965\n",
    "#df.fch4.isna().sum()\n",
    "#3163257965"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5a4316-a0f4-4667-90e9-3bd215b81078",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# for col in ['alt', 'fc', 'fch4']:\n",
    "#     df[col] = (df[col] - df[col].min()) / (df[col].max() - df[col].min())\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4215fb1-919a-4955-bc6c-57fea460e510",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['datetime','lat','lon','alt']].to_parquet('alt_df.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c07d69-e3e0-4d26-89c8-68625b95d837",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['datetime','lat','lon','fc']].to_parquet('fc_df.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37c0791-26d5-496f-be27-9e0b9c9cc261",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['datetime','lat','lon','fch4']].to_parquet('fch4_df.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece75212-5f74-4e38-aeee-dbf045f49b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "del df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f776f9e-fe86-49b7-a7ab-4db9e7695572",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced89192-0fbc-44a7-a29f-a46543fe1675",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_df = pd.read_parquet('alt_df.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5f54b3-a0b7-4db9-a08a-706525829629",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f115d33c-dd8f-4963-a182-a65221d56e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_df = alt_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e432e9-6d6a-428b-a04e-6d5b177aad99",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_df['alt'] = (alt_df['alt'] - alt_df['alt'].min()) / (alt_df['alt'].max() - alt_df['alt'].min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693bec31-3e1d-484d-afa7-0c6016ac9502",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_df.to_parquet('alt_df_nonan_norm.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd782f9-2f9c-45ad-9c33-124610d5c5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "del alt_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab26e91-d6f3-4949-88b2-3bb491cf817d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2642563d-9856-4704-9521-c81f70ed9066",
   "metadata": {},
   "outputs": [],
   "source": [
    "fc_df = pd.read_parquet('fc_df.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc67f36-92af-44a2-9072-cbbeb6dd56a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fc_df = fc_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ce2d22-6184-4e3e-b1d0-86de109ad552",
   "metadata": {},
   "outputs": [],
   "source": [
    "fc_df['fc'] = (fc_df['fc'] - fc_df['fc'].min()) / (fc_df['fc'].max() - fc_df['fc'].min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6699804a-c59c-402e-914e-94c3ba4b9f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "fc_df.to_parquet('fc_df_nonan_norm.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b451dd-bf5b-4959-bb19-47ebd2ccbe9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "del fc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5fcda6-8bce-4988-8dcf-76010d278405",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760b59b8-7409-442e-ae50-92a0cd28cfd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fch4_df = pd.read_parquet('fch4_df.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21cd501b-05c5-4e82-acfe-6204faa60f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "fch4_df = fch4_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be40d9c2-801d-45c3-8957-fc6b15ec55d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fch4_df['fch4'] = (fch4_df['fch4'] - fch4_df['fch4'].min()) / (fch4_df['fch4'].max() - fch4_df['fch4'].min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685b9e7c-621d-4b84-9a47-41025455f6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "fch4_df.to_parquet('fch4_df_nonan_norm.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a83d9bd-0ab2-4123-8939-e8ff7af71c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "del fch4_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "534e92a8-333f-4fd0-8627-bf428f9bc462",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4e558338-4ae2-4058-9887-9747e9d2793d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Reshaping datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a813aa46-4777-47cf-8d96-0b4b1f843562",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### ALT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e2374f4-0e36-487c-8630-ae659e9eb021",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "alt_df = pd.read_parquet('alt_df_nonan_norm.parquet')\n",
    "#alt_df = pd.read_parquet('/Volumes/JPL/geocryoai/modeling/data/input/alt/alt_df_nonan_norm.parquet')\n",
    "alt_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de66f055-92d7-4dd1-869e-f8d51e5c86cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_df['datetime'] = pd.to_datetime(alt_df['datetime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870c4b49-f53a-41e3-9dc6-b21ac6d36285",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_df['datetime_index'] = pd.Categorical(alt_df['datetime']).codes\n",
    "alt_df['lat_index'] = pd.Categorical(alt_df['lat']).codes\n",
    "alt_df['lon_index'] = pd.Categorical(alt_df['lon']).codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d9a4dd-0dfe-4412-a437-81c195833093",
   "metadata": {},
   "outputs": [],
   "source": [
    "# alt_df['datetime_index'] = pd.factorize(alt_df['datetime'])[0]\n",
    "# alt_df['lat_index'] = pd.factorize(alt_df['lat'])[0]\n",
    "# alt_df['lon_index'] = pd.factorize(alt_df['lon'])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11264a9-780f-47b9-a4cb-cf2c8d6bfa0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_df[['datetime_index','lat_index','lon_index','alt']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97413515-b908-441d-be09-f43133a1b8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "datetime_mapping = alt_df[['datetime', 'datetime_index']].drop_duplicates().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e058b1-4e6d-498b-9f49-d76d3a070848",
   "metadata": {},
   "outputs": [],
   "source": [
    "datetime_mapping.to_parquet('alt_datetime_mapping.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292d7202-77b4-4397-9590-969b7175bf61",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_df = alt_df[['datetime_index','lat','lon','lat_index','lon_index','alt']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c84ddf2-8516-4cfd-a99f-68b415f81f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_df.to_parquet('alt_first.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f76164-25e0-4a65-869c-c4b97eb1d04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "alt_df = pd.read_parquet('alt_first.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9b378f-81e0-48f1-a90a-5dae8249b756",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_df['alt'] = alt_df['alt'].astype('float32')\n",
    "alt_df['datetime_index'] = alt_df['datetime_index'].astype('int32')\n",
    "alt_df['lat_index'] = alt_df['lat_index'].astype('int32')\n",
    "alt_df['lon_index'] = alt_df['lon_index'].astype('int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba25fc2-32e5-43aa-8411-d5ab020e6b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lat_mapping = alt_df[['lat', 'lat_index']].drop_duplicates().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4cb9e5d-6a05-4f47-83b5-624a182e1a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "lat_mapping.to_parquet('alt_lat_mapping.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6ab6e9-e0f3-42b4-a593-f4c96b71cb1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_df = alt_df[['datetime_index','lat_index','lon','lon_index','alt']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42da973-feb8-4ddc-abe2-b4b5925b952a",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_df.to_parquet('alt_second.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a48792-bcea-4a49-a011-424e3b53bc7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcdb9783-703c-43df-8096-6efdf96edc95",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# alt_df = pd.read_parquet('alt_second.parquet')\n",
    "\n",
    "# lon_mapping = alt_df[['lon', 'lon_index']].drop_duplicates()\n",
    "# lon_mapping = lon_mapping.reset_index(drop=True)\n",
    "# lon_mapping.to_parquet('alt_lon_mapping.parquet')\n",
    "# alt_df = alt_df[['datetime_index','lat_index','lon_index','alt']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c47a3e-b730-489f-bda1-c8a6e7f6e8ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "alt_df = pd.read_parquet('alt_second.parquet', columns=['lon', 'lon_index'])\n",
    "\n",
    "chunk_size = 10_000_000\n",
    "lon_mapping = pd.DataFrame()\n",
    "\n",
    "for start in tqdm(range(0, len(alt_df), chunk_size), desc=\"Processing Chunks\"):\n",
    "    chunk = alt_df.iloc[start:start + chunk_size]\n",
    "    chunk = chunk.drop_duplicates()\n",
    "    lon_mapping = pd.concat([lon_mapping, chunk])\n",
    "\n",
    "lon_mapping = lon_mapping.drop_duplicates().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b313591-42d2-4896-abab-3045cfb2933b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lon_mapping.to_parquet('lon_mapping.parquet', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6badc572-4620-46aa-a48f-243a4274848d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "alt_df = pd.read_parquet('alt_second.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75801db-3182-428a-8a32-037309828fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_df = alt_df[['datetime_index','lat_index','lon_index','alt']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33812949-8837-4697-a113-3868340e39aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_df.to_parquet('alt_third.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fede98a3-8718-419c-86fe-4795869a18a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d5756a-9a12-4cd5-a5f2-5ea47f8a9026",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.ndimage import gaussian_filter\n",
    "import h5py\n",
    "from tqdm import tqdm\n",
    "\n",
    "alt_df = pd.read_parquet('alt_third.parquet')\n",
    "alt_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83066af1-9c6d-4eb8-b28d-e8d2ffe8e444",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_indices = alt_df['datetime_index'].unique()\n",
    "lat_indices = alt_df['lat_index'].unique()\n",
    "lon_indices = alt_df['lon_index'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2efcfdfe-aea3-4f1d-b304-37b8a3fd34e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_steps = alt_df['datetime_index'].max() + 1\n",
    "lat_steps = alt_df['lat_index'].max() + 1\n",
    "lon_steps = alt_df['lon_index'].max() + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021a19dd-9228-48b7-b6b4-10b9ac23bbc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor = np.full((time_steps, lat_steps, lon_steps), np.nan, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a2a5d0-4d86-4a27-8ccd-f5389a0ae197",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_indices = alt_df['datetime_index'].to_numpy(dtype=np.int32)\n",
    "lat_indices = alt_df['lat_index'].to_numpy(dtype=np.int32)\n",
    "lon_indices = alt_df['lon_index'].to_numpy(dtype=np.int32)\n",
    "alt_values = alt_df['alt'].to_numpy(dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef6a58b1-d5cf-46f8-8cf2-b12f29c2f96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor[t_indices, lat_indices, lon_indices] = alt_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e296b3b-7476-4a73-922a-163383a19499",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensor_filled = np.where(np.isnan(tensor), gaussian_filter(tensor, sigma=1), tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b71083d5-a393-4c4a-9f8e-42af08a8bdc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File('alt_tensor.h5', 'w') as f:\n",
    "    dset = f.create_dataset('alt', shape=tensor.shape, dtype=np.float32)\n",
    "    for t in tqdm(range(time_steps), desc='Processing time steps in tensor...'):\n",
    "        if not np.isnan(tensor[t]).all():\n",
    "            slice_min = np.nanmin(tensor[t])\n",
    "            slice_to_filter = np.where(np.isnan(tensor[t]), slice_min, tensor[t])\n",
    "            dset[t] = gaussian_filter(slice_to_filter, sigma=1)\n",
    "        else:\n",
    "            dset[t] = tensor[t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6a784e-8eb2-4f17-a9f5-cad5cc1f748b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File('alt_tensor.h5', 'r') as f:\n",
    "    tensor_filled_from_file = f['alt'][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d629bec7-9e6e-4f8e-a3a3-b126add93446",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# nan_count = np.isnan(tensor_filled_from_file).sum()\n",
    "# print(f\"Number of NaN values in reloaded tensor: {nan_count}\")\n",
    "# 6653413736"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a9bf4f-5ba8-4f86-a7ce-de46b667e99c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# nan_count = np.isnan(tensor).sum()\n",
    "# print(f\"Number of NaN values in reloaded tensor: {nan_count}\")\n",
    "# 4836086858"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6966f74f-b92f-4bc5-b47a-14f16a998894",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After interpolating via nanmin() and Gaussian filtration\n",
    "# nan_count = np.isnan(tensor_filled_from_file).sum()\n",
    "# print(f\"Number of NaN values in reloaded tensor: {nan_count}\")\n",
    "# 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5c9d7e-fa68-402f-80e9-eb2241a9ea73",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_filled_from_file = tensor_filled_from_file.reshape(6708,1092,1092,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95cda7ed-9340-4a39-a6e5-0792c25ab095",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor = tensor_filled_from_file; del tensor_filled_from_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c27b272-d1aa-4cb7-889a-7d247f8324dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File('alt_tensor.h5', 'w') as f:\n",
    "    f.create_dataset('alt', data=tensor, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1387dad6-f4f7-4dcd-879d-0fd0ba2d9a2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89981e18-f19a-4df2-9ddd-d9962ba81b8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "74608153-b0e8-41cf-be12-42e454632e12",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### FC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f917699-c004-41f2-9257-3739a6b34c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "fc_df = pd.read_parquet('fc_df_nonan_norm.parquet')\n",
    "#fc_df = pd.read_parquet('/Volumes/JPL/geocryoai/modeling/data/input/fc/fc_df_nonan_norm.parquet')\n",
    "fc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f72cb5c7-81eb-40b6-9d7c-9bdc405d44b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fc_df['datetime'] = pd.to_datetime(fc_df['datetime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1553101-0099-451d-8782-041ca47221d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fc_df['lat'] = fc_df['lat'].round(4)\n",
    "fc_df['lon'] = fc_df['lon'].round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b26a78-bd39-410b-b071-553cc2eba5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fc_df['datetime_index'] = pd.Categorical(fc_df['datetime']).codes\n",
    "fc_df['lat_index'] = pd.Categorical(fc_df['lat']).codes\n",
    "fc_df['lon_index'] = pd.Categorical(fc_df['lon']).codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5cd652-650e-4c6f-82e5-36a8bf240b69",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# fc_df['datetime_index'] = pd.factorize(fc_df['datetime'])[0]\n",
    "# fc_df['lat_index'] = pd.factorize(fc_df['lat'])[0]\n",
    "# fc_df['lon_index'] = pd.factorize(fc_df['lon'])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44f89b5-9786-4692-a6e8-870ef28d46d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fc_df[['datetime_index','lat_index','lon_index','fc']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b75c24-5f92-4701-b4d0-0ec60dc25982",
   "metadata": {},
   "outputs": [],
   "source": [
    "datetime_mapping = fc_df[['datetime', 'datetime_index']].drop_duplicates().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058cd029-5beb-441b-99ff-c934f54a39c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "datetime_mapping.to_parquet('fc_datetime_mapping.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4eb76f-f381-47e3-bb87-18a12ce4f671",
   "metadata": {},
   "outputs": [],
   "source": [
    "fc_df = fc_df[['datetime_index','lat','lon','lat_index','lon_index','fc']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094a0db1-2aa6-4d6c-b3e8-aa08e67c33be",
   "metadata": {},
   "outputs": [],
   "source": [
    "fc_df.to_parquet('fc_first.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6037d45e-b8f9-46fd-a02a-b21244db14b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "fc_df = pd.read_parquet('fc_first.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9016d7b-b67c-43c0-906b-1a720a396cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fc_df['fc'] = fc_df['fc'].astype('float32')\n",
    "fc_df['datetime_index'] = fc_df['datetime_index'].astype('int32')\n",
    "fc_df['lat_index'] = fc_df['lat_index'].astype('int32')\n",
    "fc_df['lon_index'] = fc_df['lon_index'].astype('int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7827275-3521-4d1c-a8ba-6fce73881297",
   "metadata": {},
   "outputs": [],
   "source": [
    "lat_mapping = fc_df[['lat', 'lat_index']].drop_duplicates().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7967fb5c-a452-4610-8d87-3e6b924cd416",
   "metadata": {},
   "outputs": [],
   "source": [
    "lat_mapping.to_parquet('fc_lat_mapping.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901ed2a7-3d5b-4ac2-b2fa-fab8537db20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fc_df = fc_df[['datetime_index','lat_index','lon','lon_index','fc']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7db9b8b-604b-4723-957e-dc40b05efc04",
   "metadata": {},
   "outputs": [],
   "source": [
    "fc_df.to_parquet('fc_second.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1041dfd-af41-4e85-b578-ecd9bfbe1718",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "fc_df = pd.read_parquet('fc_second.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f378cf-575b-4937-abb2-df5e23e7dcb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "lon_mapping = fc_df[['lon', 'lon_index']].drop_duplicates()\n",
    "lon_mapping = lon_mapping.reset_index(drop=True)\n",
    "lon_mapping.to_parquet('fc_lon_mapping.parquet')\n",
    "fc_df = fc_df[['datetime_index','lat_index','lon_index','fc']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd929d7f-24a0-443d-9942-7194c82981a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fc_df.to_parquet('fc_third.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656027b9-c380-48e6-a00f-b687ac34c82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.ndimage import gaussian_filter\n",
    "import h5py\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e080ce30-808d-44e8-a8fe-6b41b72a2b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "fc_df = pd.read_parquet('fc_third.parquet')\n",
    "fc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9b8489-e9ac-438e-b9de-08eae71730d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_indices = fc_df['datetime_index'].unique()\n",
    "lat_indices = fc_df['lat_index'].unique()\n",
    "lon_indices = fc_df['lon_index'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0271d8dd-d860-420b-b74e-b52b47c7c59e",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_steps = fc_df['datetime_index'].max() + 1\n",
    "lat_steps = fc_df['lat_index'].max() + 1\n",
    "lon_steps = fc_df['lon_index'].max() + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67cb5a4e-41c7-4a94-90ed-dc56e614d1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_steps, lat_steps, lon_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61058eb5-36bc-409e-bdd5-17582b2f9a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import coo_matrix\n",
    "\n",
    "with h5py.File('fc_tensor_sparse.h5', 'w') as f:\n",
    "    grp = f.create_group('sparse_tensor')\n",
    "    for t in tqdm(range(time_steps), desc='Processing time slices'):\n",
    "        current_slice = fc_df[fc_df['datetime_index'] == t]\n",
    "        lat_indices = current_slice['lat_index'].to_numpy(dtype=np.int32)\n",
    "        lon_indices = current_slice['lon_index'].to_numpy(dtype=np.int32)\n",
    "        fc_values = current_slice['fc'].to_numpy(dtype=np.float32)\n",
    "        sparse_matrix = coo_matrix((fc_values, (lat_indices, lon_indices)), shape=(lat_steps, lon_steps))\n",
    "        grp.create_dataset(f'time_{t}_data', data=sparse_matrix.data)\n",
    "        grp.create_dataset(f'time_{t}_row', data=sparse_matrix.row)\n",
    "        grp.create_dataset(f'time_{t}_col', data=sparse_matrix.col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30396ad-7415-4413-8175-c333c9fc86cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_steps = 314\n",
    "lat_steps = 95896\n",
    "lon_steps = 194761\n",
    "\n",
    "tensor = np.full((time_steps, lat_steps, lon_steps), np.nan, dtype=np.float32)\n",
    "\n",
    "with h5py.File('fc_tensor_sparse.h5', 'r') as f:\n",
    "    for t in tqdm(range(time_steps), desc=\"Reconstructing dense tensor\"):\n",
    "        data = f[f'sparse_tensor/time_{t}_data'][:]\n",
    "        row = f[f'sparse_tensor/time_{t}_row'][:]\n",
    "        col = f[f'sparse_tensor/time_{t}_col'][:]\n",
    "        sparse_matrix = coo_matrix((data, (row, col)), shape=(lat_steps, lon_steps))\n",
    "        tensor[t, :, :] = sparse_matrix.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265c03ed-3ece-426d-b577-60f7477a3aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891aa775-ac23-4f31-a983-8ef3d7563aa9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2d3ffd-58a6-4a86-bec0-85a360dce906",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7684b03f-a8b9-4eee-9073-20a8b2f5586b",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_indices = fc_df['datetime_index'].to_numpy(dtype=np.int32)\n",
    "lat_indices = fc_df['lat_index'].to_numpy(dtype=np.int32)\n",
    "lon_indices = fc_df['lon_index'].to_numpy(dtype=np.int32)\n",
    "fc_values = fc_df['fc'].to_numpy(dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd2ca2f-c4f0-4ff0-a954-2270414b6fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor[t_indices, lat_indices, lon_indices] = fc_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682c725b-6858-4a62-adcc-5435473b307e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensor_filled = np.where(np.isnan(tensor), gaussian_filter(tensor, sigma=1), tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ddd243-27f1-4b7f-aece-3b334b4a2bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File('alt_tensor.h5', 'w') as f:\n",
    "    dset = f.create_dataset('alt', shape=tensor.shape, dtype=np.float32)\n",
    "    for t in tqdm(range(time_steps), desc='Processing time steps in tensor...'):\n",
    "        if not np.isnan(tensor[t]).all():\n",
    "            slice_min = np.nanmin(tensor[t])\n",
    "            slice_to_filter = np.where(np.isnan(tensor[t]), slice_min, tensor[t])\n",
    "            dset[t] = gaussian_filter(slice_to_filter, sigma=1)\n",
    "        else:\n",
    "            dset[t] = tensor[t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b79a3fd6-c530-4320-b928-0b58ce855549",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed5d361-129c-4c98-89cd-2c0cb21b3f32",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# nan_count = np.isnan(tensor_filled_from_file).sum()\n",
    "# print(f\"Number of NaN values in reloaded tensor: {nan_count}\")\n",
    "# 6653413736"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3b5e91-765b-45ee-b9af-25a20836658e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# nan_count = np.isnan(tensor).sum()\n",
    "# print(f\"Number of NaN values in reloaded tensor: {nan_count}\")\n",
    "# 4836086858"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53882800-53f6-4818-aed7-56f56ed0632d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After interpolating via nanmin() and Gaussian filtration\n",
    "# nan_count = np.isnan(tensor_filled_from_file).sum()\n",
    "# print(f\"Number of NaN values in reloaded tensor: {nan_count}\")\n",
    "# 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f78a39-163c-46ec-bb58-0365b66fd970",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_filled_from_file = tensor_filled_from_file.reshape(6708,1092,1092,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5afac1e6-ba4c-4197-8535-e7f1d04d202a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor = tensor_filled_from_file; del tensor_filled_from_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb13053e-b64e-4ce5-acf1-5571db1a39dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File('alt_tensor.h5', 'w') as f:\n",
    "    f.create_dataset('alt', data=tensor, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df7c46d-ba4d-4bf2-9db3-949fa609c846",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5900c14-2339-4bfc-838b-9b0f77a89e8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3e6c956e-782a-4bfe-b42e-03d71e2361b3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### FCH4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff87b892-8a38-44f2-87c7-f435538891c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6760e820-3333-49ae-a470-b1096ec99857",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "fch4_df = pd.read_parquet('fch4_df_nonan_norm.parquet')\n",
    "#fch4_df = pd.read_parquet('/Volumes/JPL/geocryoai/modeling/data/input/fch4/fch4_df_nonan_norm.parquet')\n",
    "fch4_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383c5143-7294-41d0-b325-7bb19cd535a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_df['datetime'] = pd.to_datetime(alt_df['datetime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99ad0af-f749-4b36-a04b-09a01ecb9f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_df['datetime_index'] = pd.Categorical(alt_df['datetime']).codes\n",
    "alt_df['lat_index'] = pd.Categorical(alt_df['lat']).codes\n",
    "alt_df['lon_index'] = pd.Categorical(alt_df['lon']).codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247055c2-a2aa-4674-b846-af0cff2079f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# alt_df['datetime_index'] = pd.factorize(alt_df['datetime'])[0]\n",
    "# alt_df['lat_index'] = pd.factorize(alt_df['lat'])[0]\n",
    "# alt_df['lon_index'] = pd.factorize(alt_df['lon'])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4fd3dad-668b-42d7-9b2a-2ce785d216c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_df[['datetime_index','lat_index','lon_index','alt']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa0f7c7-34f9-4a50-a44a-d7a6f58a7f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "datetime_mapping = alt_df[['datetime', 'datetime_index']].drop_duplicates().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0f2d86-4380-493e-8ec3-095e2df7ced1",
   "metadata": {},
   "outputs": [],
   "source": [
    "datetime_mapping.to_parquet('alt_datetime_mapping.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10bf7ed2-5773-43ad-ab4a-4e9a4934569f",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_df = alt_df[['datetime_index','lat','lon','lat_index','lon_index','alt']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948bf171-a814-4e3e-9834-3829c6e6a423",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_df.to_parquet('alt_first.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ddd912d-cf2b-4037-a25c-4641a03cbbe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "alt_df = pd.read_parquet('alt_first.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1fa000d-25f2-4a8b-b6f3-f8a9a44fbe7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_df['alt'] = alt_df['alt'].astype('float32')\n",
    "alt_df['datetime_index'] = alt_df['datetime_index'].astype('int32')\n",
    "alt_df['lat_index'] = alt_df['lat_index'].astype('int32')\n",
    "alt_df['lon_index'] = alt_df['lon_index'].astype('int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1636aa3e-5bea-4176-9923-bd460d7130c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "lat_mapping = alt_df[['lat', 'lat_index']].drop_duplicates().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b7a9b0-892a-488f-9e0c-3c23d08c93b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "lat_mapping.to_parquet('alt_lat_mapping.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26db10f-cecc-4411-a59f-68ca9f6c4fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_df = alt_df[['datetime_index','lat_index','lon','lon_index','alt']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2103953-2794-4f28-856e-6f4b585abd34",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_df.to_parquet('alt_second.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe7a821-db53-4a02-ac1a-e56ff4500bf9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5467a4-7574-4790-857f-8439f931e69c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# alt_df = pd.read_parquet('alt_second.parquet')\n",
    "\n",
    "# lon_mapping = alt_df[['lon', 'lon_index']].drop_duplicates()\n",
    "# lon_mapping = lon_mapping.reset_index(drop=True)\n",
    "# lon_mapping.to_parquet('alt_lon_mapping.parquet')\n",
    "# alt_df = alt_df[['datetime_index','lat_index','lon_index','alt']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d210d3cc-9ee9-464a-81f1-a686862bfe3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "alt_df = pd.read_parquet('alt_second.parquet', columns=['lon', 'lon_index'])\n",
    "\n",
    "chunk_size = 10_000_000\n",
    "lon_mapping = pd.DataFrame()\n",
    "\n",
    "for start in tqdm(range(0, len(alt_df), chunk_size), desc=\"Processing Chunks\"):\n",
    "    chunk = alt_df.iloc[start:start + chunk_size]\n",
    "    chunk = chunk.drop_duplicates()\n",
    "    lon_mapping = pd.concat([lon_mapping, chunk])\n",
    "\n",
    "lon_mapping = lon_mapping.drop_duplicates().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bbd859d-bd26-4d5e-8a13-f78d126aaacf",
   "metadata": {},
   "outputs": [],
   "source": [
    "lon_mapping.to_parquet('lon_mapping.parquet', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5bbad8-8a1b-426d-9801-30a0bbad0fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "alt_df = pd.read_parquet('alt_second.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3866a767-5274-41ef-8b02-28e4f05d1218",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_df = alt_df[['datetime_index','lat_index','lon_index','alt']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3b29ec-b95a-41e0-a83c-9bd8e08d1e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_df.to_parquet('alt_third.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f23975-78fa-4cff-ad5b-fb9c7643040f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a851bd-3c5d-4f1f-98bf-3efc90f9fb1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.ndimage import gaussian_filter\n",
    "import h5py\n",
    "from tqdm import tqdm\n",
    "\n",
    "alt_df = pd.read_parquet('alt_third.parquet')\n",
    "alt_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb32a468-a6fa-429e-b04f-c9a6de0b717a",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_indices = alt_df['datetime_index'].unique()\n",
    "lat_indices = alt_df['lat_index'].unique()\n",
    "lon_indices = alt_df['lon_index'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0453db-3c8d-4787-8c92-38b7757773fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_steps = alt_df['datetime_index'].max() + 1\n",
    "lat_steps = alt_df['lat_index'].max() + 1\n",
    "lon_steps = alt_df['lon_index'].max() + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d3ed56-d7c4-4235-ba2e-ee274787a72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor = np.full((time_steps, lat_steps, lon_steps), np.nan, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbcce3ef-8ecc-4743-b4d9-2dd732fe2d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_indices = alt_df['datetime_index'].to_numpy(dtype=np.int32)\n",
    "lat_indices = alt_df['lat_index'].to_numpy(dtype=np.int32)\n",
    "lon_indices = alt_df['lon_index'].to_numpy(dtype=np.int32)\n",
    "alt_values = alt_df['alt'].to_numpy(dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6878a2-c761-40d6-80bc-04a70a248da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor[t_indices, lat_indices, lon_indices] = alt_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e76ccff-58b8-4c98-9993-60a6eb653ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensor_filled = np.where(np.isnan(tensor), gaussian_filter(tensor, sigma=1), tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82839928-83f3-47ab-be5a-605f7f187f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File('alt_tensor.h5', 'w') as f:\n",
    "    dset = f.create_dataset('alt', shape=tensor.shape, dtype=np.float32)\n",
    "    for t in tqdm(range(time_steps), desc='Processing time steps in tensor...'):\n",
    "        if not np.isnan(tensor[t]).all():\n",
    "            slice_min = np.nanmin(tensor[t])\n",
    "            slice_to_filter = np.where(np.isnan(tensor[t]), slice_min, tensor[t])\n",
    "            dset[t] = gaussian_filter(slice_to_filter, sigma=1)\n",
    "        else:\n",
    "            dset[t] = tensor[t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac77cb9-5745-4c44-8306-5fdddf0855f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File('alt_tensor.h5', 'r') as f:\n",
    "    tensor_filled_from_file = f['alt'][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b28203-3fb5-4bbd-ba41-343ebeaacd5d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# nan_count = np.isnan(tensor_filled_from_file).sum()\n",
    "# print(f\"Number of NaN values in reloaded tensor: {nan_count}\")\n",
    "# 6653413736"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6d7b64-af4a-4a88-b5ee-b1439b574368",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# nan_count = np.isnan(tensor).sum()\n",
    "# print(f\"Number of NaN values in reloaded tensor: {nan_count}\")\n",
    "# 4836086858"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1280ce1e-8dcf-454d-b243-0f60521c81b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After interpolating via nanmin() and Gaussian filtration\n",
    "# nan_count = np.isnan(tensor_filled_from_file).sum()\n",
    "# print(f\"Number of NaN values in reloaded tensor: {nan_count}\")\n",
    "# 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a343e89-176a-4d00-bede-85f46e039556",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_filled_from_file = tensor_filled_from_file.reshape(6708,1092,1092,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3f1399-3270-4f9c-bbfe-2b4e18945e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor = tensor_filled_from_file; del tensor_filled_from_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee012f7-4964-4fed-8562-ffbe81d77cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File('alt_tensor.h5', 'w') as f:\n",
    "    f.create_dataset('alt', data=tensor, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0e2792-8eb1-46ea-8cf0-09fb779757d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c2460c-b710-4be1-94b1-4e3f344819ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f568621-b501-48f8-8384-c8d4323fc38f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "06ec0b6b-6478-4277-a145-fcc55a291bb2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Mapping indices back to original values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f522bd-a846-419f-86d1-a96a597f77f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping indices back to original values\n",
    "datetime_mapping = pd.read_parquet('datetime_mapping.parquet')\n",
    "lat_mapping = pd.read_parquet('lat_mapping.parquet')\n",
    "lon_mapping = pd.read_parquet('lon_mapping.parquet')\n",
    "\n",
    "# Use mappings for interpretation of predictions\n",
    "datetime_mapping.set_index('datetime_index', inplace=True)\n",
    "lat_mapping.set_index('lat_index', inplace=True)\n",
    "lon_mapping.set_index('lon_index', inplace=True)\n",
    "\n",
    "original_datetime = datetime_mapping.loc[0, 'datetime']  # Example\n",
    "original_lat = lat_mapping.loc[915, 'lat']\n",
    "original_lon = lon_mapping.loc[635, 'lon']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa66c681-031d-4272-b9bf-8aebd9c7b5e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e9c4e82c-0dd3-4497-aae2-f187add2a195",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ec39e4-afe3-48a5-9041-c52613cf55f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from scipy.sparse import coo_matrix\n",
    "from scipy.sparse import coo_matrix\n",
    "from scipy.ndimage import gaussian_filter\n",
    "\n",
    "alt_df = pd.read_parquet('alt_df_nonan_norm.parquet')\n",
    "fch4_df = pd.read_parquet('fch4_df_nonan_norm.parquet')\n",
    "fc_df = pd.read_parquet('fc_df_nonan_norm.parquet')\n",
    "#merged = pd.read_parquet('/Volumes/JPL/geocryoai/preprocessing/data/ds/merged/final_fcfch4alt_monthly_1km_ds.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4bbbab-cc38-4b18-84f9-a46ba4b915ba",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# print(alt_df.head())\n",
    "# print(fc_df.head())\n",
    "# print(fch4_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3cdf58-06e2-45e7-a3ce-5f7a2261edea",
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in [alt_df, fc_df, fch4_df]:\n",
    "    df['datetime'] = pd.to_datetime(df['datetime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58898bde-e6d7-441f-9e2a-cb828600372f",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_time = alt_df['datetime'].to_numpy()\n",
    "fc_time = fc_df['datetime'].to_numpy()\n",
    "fch4_time = fch4_df['datetime'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae69cabf-b6fb-4965-86ec-09539d56f52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "common_time = np.intersect1d(alt_time, fc_time)\n",
    "common_time = np.intersect1d(common_time, fch4_time)\n",
    "common_time = pd.to_datetime(common_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6471e394-e167-4c0d-ac18-e4789cdd3329",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(common_time).to_parquet('common_time.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d671aeb6-4035-4109-8d3d-8d13b2f21d0c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# common_time #1994-06-01 to 2022-12-01 (28 years), i.e., 314 values\n",
    "# DatetimeIndex(['1994-06-01', '1994-07-01', '1994-08-01', '1994-09-01',\n",
    "#                '1994-10-01', '1994-11-01', '1994-12-01', '1995-01-01',\n",
    "#                '1995-02-01', '1995-03-01',\n",
    "#                ...\n",
    "#                '2022-03-01', '2022-04-01', '2022-05-01', '2022-06-01',\n",
    "#                '2022-07-01', '2022-08-01', '2022-09-01', '2022-10-01',\n",
    "#                '2022-11-01', '2022-12-01'],\n",
    "#               dtype='datetime64[ns]', length=314, freq=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d73830-0209-489b-abc0-a4b11bed88a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_df = alt_df[alt_df['datetime'].isin(common_time)].reset_index(drop=True)\n",
    "fc_df = fc_df[fc_df['datetime'].isin(common_time)].reset_index(drop=True)\n",
    "fch4_df = fch4_df[fch4_df['datetime'].isin(common_time)].reset_index(drop=True)\n",
    "print(f\"Common time steps: {len(common_time)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2003372-7adf-479e-aefc-19075e3a2e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139cc50b-6311-467a-9560-96e84e910c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lat_min = min(alt_df['lat'].min(), fc_df['lat'].min(), fch4_df['lat'].min())\n",
    "lat_max = max(alt_df['lat'].max(), fc_df['lat'].max(), fch4_df['lat'].max())\n",
    "lon_min = min(alt_df['lon'].min(), fc_df['lon'].min(), fch4_df['lon'].min())\n",
    "lon_max = max(alt_df['lon'].max(), fc_df['lon'].max(), fch4_df['lon'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82efe0d-1b93-4e6d-92f2-28218c918286",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_spacing = 1 / 111  # 1 km grid in degrees\n",
    "lat_grid = np.arange(lat_min, lat_max + grid_spacing, grid_spacing)\n",
    "lon_grid = np.arange(lon_min, lon_max + grid_spacing, grid_spacing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a473329f-9c98-4f8d-a056-f46b7659ab21",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_lat, grid_lon = np.meshgrid(lat_grid, lon_grid, indexing='ij')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8dc20b-363c-4525-afab-03d4f5379fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_lat = grid_lat.round(3)\n",
    "grid_lon = grid_lon.round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db766667-121f-4479-829a-9a3e5527d3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_and_interpolate(df, var, method='mean'):\n",
    "    tensors = []\n",
    "    for dt in tqdm(common_time, desc=f\"Processing {var} data\"):\n",
    "        time_slice = df[df['datetime'] == dt]\n",
    "        points = time_slice[['lat', 'lon']].values\n",
    "        values = time_slice[var].values\n",
    "\n",
    "        # Interpolate onto the grid\n",
    "        interpolated = griddata(\n",
    "            points, values, (grid_lat, grid_lon), method='linear'\n",
    "        )\n",
    "\n",
    "        # Handle missing values\n",
    "        if method == 'min':\n",
    "            interpolated[np.isnan(interpolated)] = np.nanmin(interpolated)\n",
    "        elif method == 'mean':\n",
    "            interpolated[np.isnan(interpolated)] = np.nanmean(interpolated)\n",
    "\n",
    "        tensors.append(interpolated)\n",
    "\n",
    "    return np.stack(tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be91454-54d7-445c-9ea1-f63d510b7f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.interpolate import griddata\n",
    "alt_tensor = align_and_interpolate(alt_df, 'alt', method='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5567f5ad-5715-46c6-bd41-311bdbed8498",
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_and_interpolate(df, var, method='mean'):\n",
    "    tensors = []\n",
    "    for dt in tqdm(common_time, desc=f\"Processing {var} data\"):\n",
    "        time_slice = df[df['datetime'] == dt]\n",
    "        points = time_slice[['lat', 'lon']].values\n",
    "        values = time_slice[var].values\n",
    "\n",
    "        if len(points) < 4:  # Not enough points for triangulation\n",
    "            print(f\"Skipping datetime {dt} due to insufficient points.\")\n",
    "            # Fill entire grid with global min/mean as fallback\n",
    "            if method == 'min':\n",
    "                fallback_value = np.nanmin(values) if len(values) > 0 else 0\n",
    "            elif method == 'mean':\n",
    "                fallback_value = np.nanmean(values) if len(values) > 0 else 0\n",
    "            interpolated = np.full(grid_lat.shape, fallback_value)\n",
    "        else:\n",
    "            # Interpolate onto the grid\n",
    "            interpolated = griddata(\n",
    "                points, values, (grid_lat, grid_lon), method='linear'\n",
    "            )\n",
    "            # Handle missing values\n",
    "            if method == 'min':\n",
    "                interpolated[np.isnan(interpolated)] = np.nanmin(values)\n",
    "            elif method == 'mean':\n",
    "                interpolated[np.isnan(interpolated)] = np.nanmean(values)\n",
    "\n",
    "        tensors.append(interpolated)\n",
    "\n",
    "    return np.stack(tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b0f964-5932-41c9-84dc-88c672456d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fc_tensor = align_and_interpolate(fc_df, 'fc', method='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16fe05ac-9ac7-4ce8-8298-d304c7e6b80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fch4_tensor = align_and_interpolate(fch4_df, 'fch4', method='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6f311a-655c-492e-bc2e-4b2773e71217",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_tensor = alt_tensor.reshape(314,2070,4178,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf2b1e5-c9f9-4dc4-aca2-b9238e0f1c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "fc_tensor = fc_tensor.reshape(314,2070,4178,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9be6ce-6378-40fc-9e82-871512d1df3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fch4_tensor = fch4_tensor.reshape(314,2070,4178,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7309f57-a04e-4d4e-a9bf-5ca61c97b823",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"alt_tensor shape: {alt_tensor.shape}\")\n",
    "print(f\"fc_tensor shape: {fc_tensor.shape}\")\n",
    "print(f\"fch4_tensor shape: {fch4_tensor.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfaffb60-ee05-4493-88d3-e29dab90fe06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "with h5py.File('alt_tensor.h5', 'w') as f:\n",
    "    f.create_dataset('alt', data=alt_tensor, dtype=np.float32)\n",
    "\n",
    "with h5py.File('fc_tensor.h5', 'w') as f:\n",
    "    f.create_dataset('fc', data=fc_tensor, dtype=np.float32)\n",
    "\n",
    "with h5py.File('fch4_tensor.h5', 'w') as f:\n",
    "    f.create_dataset('fch4', data=fch4_tensor, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9395a5e9-8036-422d-a294-12e3e242efbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File('ensemble_tensor.h5', 'w') as f:\n",
    "    f.create_dataset('alt', data=alt_tensor, dtype=np.float32)\n",
    "    f.create_dataset('fc', data=fc_tensor, dtype=np.float32)\n",
    "    f.create_dataset('fch4', data=fch4_tensor, dtype=np.float32)\n",
    "print(\"Tensors saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39cbb74b-5e1f-46d4-981a-036b26fd12bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6473f9-5fbc-42a2-9c38-3fc5ad550d22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cd198762-2391-430f-9a2c-dab9d07975f7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Archived"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e678964-31b5-46a3-95b6-fa93d723030f",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_df['lat'] = alt_df.lat.round(3)\n",
    "alt_df['lon'] = alt_df.lon.round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3236d97-49a6-4409-8861-584f69dc71e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fc_df['lat'] = fc_df.lat.round(3)\n",
    "fc_df['lon'] = fc_df.lon.round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f6b3fe-51be-4b5f-93b3-70ac164d6e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "fch4_df['lat'] = fch4_df.lat.round(3)\n",
    "fch4_df['lon'] = fch4_df.lon.round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ffd26b-9f95-4f9c-b481-03c1dc3ed35a",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_lat_lon = pd.concat([alt_df['lat'], alt_df['lon']], axis=1).drop_duplicates()\n",
    "fc_lat_lon = pd.concat([fc_df['lat'], fc_df['lon']], axis=1).drop_duplicates()\n",
    "fch4_lat_lon = pd.concat([fch4_df['lat'], fch4_df['lon']], axis=1).drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b6eed1-c44b-4513-8aad-edfafc539521",
   "metadata": {},
   "outputs": [],
   "source": [
    "lat_mapping = pd.Categorical(pd.concat([alt_df['lat'], fc_df['lat'], fch4_df['lat']])).codes\n",
    "lon_mapping = pd.Categorical(pd.concat([alt_df['lon'], fc_df['lon'], fch4_df['lon']])).codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9489d94-2cfa-414a-a727-b2de2fe641fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9996e44-b921-4501-b7f1-d4a18ccb0538",
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in [alt_df, fc_df, fch4_df]:\n",
    "    df['lat_index'] = pd.Categorical(df['lat']).codes\n",
    "    df['lon_index'] = pd.Categorical(df['lon']).codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b578e08b-62b8-485b-a2f0-368742d5a71c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e331f1-46db-4dc3-8271-90400a8e1d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all unique latitudes and longitudes\n",
    "lat_union = sorted(set(alt_df['lat']).union(fc_df['lat']).union(fch4_df['lat']))\n",
    "lon_union = sorted(set(alt_df['lon']).union(fc_df['lon']).union(fch4_df['lon']))\n",
    "\n",
    "print(f\"Unified latitudes: {len(lat_union)}\")\n",
    "print(f\"Unified longitudes: {len(lon_union)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f83a66-6cdb-460d-96cc-879e7dd14625",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ecbf49-9458-4c58-b3f8-8d07ca4f823a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e027e492-04f6-45e3-8f76-88f57a0c1436",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c074e8-10f9-45f0-a99a-f61ec7eab143",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199df4b6-9eb8-4172-a4b8-ca0b542a63bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d8de05-14bc-47f8-9033-bf92b7fc336e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46d7671-514a-4891-bea8-1847690626e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b8e7b9-913c-407a-9256-2049882d45a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc9b92a-b047-4bb2-80d7-b41e84891920",
   "metadata": {},
   "outputs": [],
   "source": [
    "lat_mapping = {lat: idx for idx, lat in enumerate(lat_union)}\n",
    "lon_mapping = {lon: idx for idx, lon in enumerate(lon_union)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174dffce-066f-4ae0-80de-14a944b39fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in [alt_df, fc_df, fch4_df]:\n",
    "    df['lat_index'] = df['lat'].map(lat_mapping)\n",
    "    df['lon_index'] = df['lon'].map(lon_mapping)\n",
    "\n",
    "print(\"Lat/Lon indexing complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3faf13-f542-40ca-be76-191b8a25d5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import coo_matrix\n",
    "\n",
    "# Function to create a sparse matrix\n",
    "def to_sparse_matrix(df, var, time_steps, lat_steps, lon_steps):\n",
    "    t_indices = pd.Categorical(df['datetime']).codes\n",
    "    lat_indices = df['lat_index'].to_numpy()\n",
    "    lon_indices = df['lon_index'].to_numpy()\n",
    "    values = df[var].to_numpy()\n",
    "\n",
    "    return coo_matrix(\n",
    "        (values, (t_indices, lat_indices * lon_steps + lon_indices)),\n",
    "        shape=(time_steps, lat_steps * lon_steps)\n",
    "    )\n",
    "\n",
    "# Define grid dimensions\n",
    "time_steps = len(pd.unique(alt_df['datetime']))\n",
    "lat_steps = len(lat_union)\n",
    "lon_steps = len(lon_union)\n",
    "\n",
    "# Convert datasets to sparse matrices\n",
    "alt_sparse = to_sparse_matrix(alt_df, 'alt', time_steps, lat_steps, lon_steps)\n",
    "fc_sparse = to_sparse_matrix(fc_df, 'fc', time_steps, lat_steps, lon_steps)\n",
    "fch4_sparse = to_sparse_matrix(fch4_df, 'fch4', time_steps, lat_steps, lon_steps)\n",
    "\n",
    "print(\"Sparse matrices created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73f07b1-6c84-42bd-b118-45fdaaa0caa6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "befc4301-e060-40cb-8739-a21df48cac40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af8c13d-9c87-45be-be9d-16e05a1e8995",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70589ba-2267-440d-8434-a0759123e283",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10fd7f73-e766-4007-903c-886dc95b418f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd468582-0967-483b-a302-30c50ec9f68f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27868978-a7ea-4015-b82b-1c13900102ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9140fe-e911-4bcb-b3a1-ac83b5455f64",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# print(alt_df[['lat', 'lon']].drop_duplicates().head())\n",
    "# print(fc_df[['lat', 'lon']].drop_duplicates().head())\n",
    "# print(fch4_df[['lat', 'lon']].drop_duplicates().head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6dd9a1-9d93-415f-8b4d-cb08fc82def3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35cd87a4-8837-4f25-b95b-85b64967185c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e8fa6a-6453-4b5a-b27e-681e1d3c15c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in [alt_df, fc_df, fch4_df]:\n",
    "    df['lat_index'] = pd.Categorical(df['lat']).codes\n",
    "    df['lon_index'] = pd.Categorical(df['lon']).codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb469f28-75fd-46c9-b0dd-2a30fce2ff10",
   "metadata": {},
   "outputs": [],
   "source": [
    "#del df, alt_time, fc_time, fch4_time, common_time, alt_lat_lon, fc_lat_lon, fch4_lat_lon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb8d1db-99dc-485d-a331-28de38e21b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define tensor dimensions\n",
    "time_steps = len(pd.unique(alt_df['datetime']))\n",
    "lat_steps = len(pd.unique(lat_mapping))\n",
    "lon_steps = len(pd.unique(lon_mapping))\n",
    "\n",
    "print(f\"Tensor dimensions: time_steps={time_steps}, lat_steps={lat_steps}, lon_steps={lon_steps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83748a9d-cbbb-499f-8b9d-c0dc28a2ddd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define tensor dimensions\n",
    "time_steps = len(pd.unique(fc_df['datetime']))\n",
    "lat_steps = len(pd.unique(lat_mapping))\n",
    "lon_steps = len(pd.unique(lon_mapping))\n",
    "\n",
    "print(f\"Tensor dimensions: time_steps={time_steps}, lat_steps={lat_steps}, lon_steps={lon_steps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df7ca07-8a01-4138-88d8-5af268228e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define tensor dimensions\n",
    "time_steps = len(pd.unique(fc_df['datetime']))\n",
    "lat_steps = len(pd.unique(lat_mapping))\n",
    "lon_steps = len(pd.unique(lon_mapping))\n",
    "\n",
    "print(f\"Tensor dimensions: time_steps={time_steps}, lat_steps={lat_steps}, lon_steps={lon_steps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d9b546-d38b-4336-8dee-e4e32c7e15cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.array_equal(pd.unique(alt_df['datetime']), pd.unique(fc_df['datetime'])), \"Temporal misalignment between ALT and FC datasets.\"\n",
    "assert np.array_equal(pd.unique(alt_df['datetime']), pd.unique(fch4_df['datetime'])), \"Temporal misalignment between ALT and FCH4 datasets.\"\n",
    "assert np.array_equal(pd.unique(fc_df['datetime']), pd.unique(fch4_df['datetime'])), \"Temporal misalignment between FC and FCH4 datasets.\"\n",
    "\n",
    "assert np.array_equal(alt_df['lat'], fc_df['lat']), \"Spatial misalignment between ALT and FC in latitude.\"\n",
    "assert np.array_equal(alt_df['lon'], fc_df['lon']), \"Spatial misalignment between ALT and FC in longitude.\"\n",
    "assert np.array_equal(alt_df['lat'], fch4_df['lat']), \"Spatial misalignment between ALT and FCH4 in latitude.\"\n",
    "assert np.array_equal(alt_df['lon'], fch4_df['lon']), \"Spatial misalignment between ALT and FCH4 in longitude.\"\n",
    "assert np.array_equal(fc_df['lat'], fch4_df['lat']), \"Spatial misalignment between FC and FCH4 in latitude.\"\n",
    "assert np.array_equal(fc_df['lon'], fch4_df['lon']), \"Spatial misalignment between FC and FCH4 in longitude.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ff26f9-6469-4f57-8ef6-73521a4c4cd8",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from tqdm import tqdm\n",
    "# from scipy.sparse import coo_matrix\n",
    "# from scipy.sparse import coo_matrix\n",
    "# from scipy.ndimage import gaussian_filter\n",
    "\n",
    "# alt_df = pd.read_parquet('alt_df_nonan_norm.parquet')\n",
    "# fch4_df = pd.read_parquet('fch4_df_nonan_norm.parquet')\n",
    "# fc_df = pd.read_parquet('fc_df_nonan_norm.parquet')\n",
    "\n",
    "# for df in [alt_df, fc_df, fch4_df]:\n",
    "#     df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "\n",
    "# alt_time = alt_df['datetime'].to_numpy()\n",
    "# fc_time = fc_df['datetime'].to_numpy()\n",
    "# fch4_time = fch4_df['datetime'].to_numpy()\n",
    "\n",
    "# common_time = np.intersect1d(alt_time, fc_time)\n",
    "# common_time = np.intersect1d(common_time, fch4_time)\n",
    "# common_time = pd.to_datetime(common_time)\n",
    "\n",
    "# alt_df = alt_df[alt_df['datetime'].isin(common_time)]\n",
    "# fc_df = fc_df[fc_df['datetime'].isin(common_time)]\n",
    "# fch4_df = fch4_df[fch4_df['datetime'].isin(common_time)]\n",
    "# print(f\"Common time steps: {len(common_time)}\")\n",
    "\n",
    "# Common time steps: 314\n",
    "\n",
    "# alt_df = alt_df.reset_index(drop=True)\n",
    "# fc_df = fc_df.reset_index(drop=True)\n",
    "# fch4_df = fch4_df.reset_index(drop=True)\n",
    "\n",
    "# grid_spacing = 1 / 111\n",
    "\n",
    "# chunk_size = 1_000_000\n",
    "# for df in [alt_df, fc_df, fch4_df]:\n",
    "#     chunks = [df[i:i + chunk_size] for i in tqdm(range(0, len(df), chunk_size), desc='Processing chunks...')]\n",
    "#     aligned_chunks = []\n",
    "#     for chunk in chunks:\n",
    "#         chunk['lat'] = np.floor(chunk['lat'] / grid_spacing) * grid_spacing\n",
    "#         chunk['lon'] = np.floor(chunk['lon'] / grid_spacing) * grid_spacing\n",
    "#         aligned_chunks.append(chunk)\n",
    "#     df = pd.concat(aligned_chunks, ignore_index=True)\n",
    "\n",
    "# alt_lat_lon = pd.concat([alt_df['lat'], alt_df['lon']], axis=1).drop_duplicates()\n",
    "# fc_lat_lon = pd.concat([fc_df['lat'], fc_df['lon']], axis=1).drop_duplicates()\n",
    "# fch4_lat_lon = pd.concat([fch4_df['lat'], fch4_df['lon']], axis=1).drop_duplicates()\n",
    "\n",
    "# lat_mapping = pd.Categorical(pd.concat([alt_df['lat'], fc_df['lat'], fch4_df['lat']])).codes\n",
    "# lon_mapping = pd.Categorical(pd.concat([alt_df['lon'], fc_df['lon'], fch4_df['lon']])).codes\n",
    "\n",
    "# for df in [alt_df, fc_df, fch4_df]:\n",
    "#     df['lat_index'] = pd.Categorical(df['lat']).codes\n",
    "#     df['lon_index'] = pd.Categorical(df['lon']).codes\n",
    "\n",
    "# # Define tensor dimensions\n",
    "# time_steps = len(pd.unique(alt_df['datetime']))\n",
    "# lat_steps = len(pd.unique(lat_mapping))\n",
    "# lon_steps = len(pd.unique(lon_mapping))\n",
    "\n",
    "# Tensor dimensions: time_steps=314, lat_steps=431300, lon_steps=467680\n",
    "\n",
    "# # Define tensor dimensions\n",
    "# time_steps = len(pd.unique(fc_df['datetime']))\n",
    "# lat_steps = len(pd.unique(lat_mapping))\n",
    "# lon_steps = len(pd.unique(lon_mapping))\n",
    "\n",
    "# print(f\"Tensor dimensions: time_steps={time_steps}, lat_steps={lat_steps}, lon_steps={lon_steps}\")\n",
    "\n",
    "# print(f\"Tensor dimensions: time_steps={time_steps}, lat_steps={lat_steps}, lon_steps={lon_steps}\")\n",
    "\n",
    "# Tensor dimensions: time_steps=314, lat_steps=431300, lon_steps=467680\n",
    "\n",
    "# # Define tensor dimensions\n",
    "# time_steps = len(pd.unique(fch4_df['datetime']))\n",
    "# lat_steps = len(pd.unique(lat_mapping))\n",
    "# lon_steps = len(pd.unique(lon_mapping))\n",
    "\n",
    "# print(f\"Tensor dimensions: time_steps={time_steps}, lat_steps={lat_steps}, lon_steps={lon_steps}\")\n",
    "\n",
    "# Tensor dimensions: time_steps=314, lat_steps=431300, lon_steps=467680\n",
    "\n",
    "# assert np.array_equal(pd.unique(alt_df['datetime']), pd.unique(fc_df['datetime'])), \"Temporal misalignment between ALT and FC datasets.\"\n",
    "# assert np.array_equal(pd.unique(alt_df['datetime']), pd.unique(fch4_df['datetime'])), \"Temporal misalignment between ALT and FCH4 datasets.\"\n",
    "# assert np.array_equal(pd.unique(fc_df['datetime']), pd.unique(fch4_df['datetime'])), \"Temporal misalignment between FC and FCH4 datasets.\"\n",
    "\n",
    "# assert np.array_equal(alt_df['lat'], fc_df['lat']), \"Spatial misalignment between ALT and FC in latitude.\"\n",
    "# assert np.array_equal(alt_df['lon'], fc_df['lon']), \"Spatial misalignment between ALT and FC in longitude.\"\n",
    "# assert np.array_equal(alt_df['lat'], fch4_df['lat']), \"Spatial misalignment between ALT and FCH4 in latitude.\"\n",
    "# assert np.array_equal(alt_df['lon'], fch4_df['lon']), \"Spatial misalignment between ALT and FCH4 in longitude.\"\n",
    "# assert np.array_equal(fc_df['lat'], fch4_df['lat']), \"Spatial misalignment between FC and FCH4 in latitude.\"\n",
    "# assert np.array_equal(fc_df['lon'], fch4_df['lon']), \"Spatial misalignment between FC and FCH4 in longitude.\"\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# AssertionError                            Traceback (most recent call last)\n",
    "# Cell In[18], line 5\n",
    "#       2 assert np.array_equal(pd.unique(alt_df['datetime']), pd.unique(fch4_df['datetime'])), \"Temporal misalignment between ALT and FCH4 datasets.\"\n",
    "#       3 assert np.array_equal(pd.unique(fc_df['datetime']), pd.unique(fch4_df['datetime'])), \"Temporal misalignment between FC and FCH4 datasets.\"\n",
    "# ----> 5 assert np.array_equal(alt_df['lat'], fc_df['lat']), \"Spatial misalignment between ALT and FC in latitude.\"\n",
    "#       6 assert np.array_equal(alt_df['lon'], fc_df['lon']), \"Spatial misalignment between ALT and FC in longitude.\"\n",
    "#       7 assert np.array_equal(alt_df['lat'], fch4_df['lat']), \"Spatial misalignment between ALT and FCH4 in latitude.\"\n",
    "\n",
    "# AssertionError: Spatial misalignment between ALT and FC in latitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3baa5bdb-4f24-45fe-875f-f3f7b259b446",
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in [alt_df, fc_df, fch4_df]:\n",
    "    print(f\"Before deduplication: {len(df)} rows\")\n",
    "    df = df.drop_duplicates(subset=['datetime', 'lat', 'lon']).reset_index(drop=True)\n",
    "    print(f\"After deduplication: {len(df)} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef0ced2-e412-4361-9b12-71bdca32c87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_df.to_parquet('alt_new_df.parquet')\n",
    "fc_df.to_parquet('fc_new_df.parquet')\n",
    "fch4_df.to_parquet('fch4_new_df.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e0329a-eaab-4d8d-aa29-8a5791be4687",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "alt_df = pd.read_parquet('alt_new_df.parquet')\n",
    "fc_df = pd.read_parquet('fc_new_df.parquet')\n",
    "fch4_df = pd.read_parquet('fch4_new_df.parquet')\n",
    "common_time = pd.to_datetime(pd.read_parquet('common_time.parquet')[0].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3f49c6-2c13-43ab-9a90-9b182aee2de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Unique latitudes in ALT: {alt_df['lat'].nunique()}\")\n",
    "print(f\"Unique latitudes in FC: {fc_df['lat'].nunique()}\")\n",
    "print(f\"Unique latitudes in FCH4: {fch4_df['lat'].nunique()}\")\n",
    "\n",
    "print(f\"Unique longitudes in ALT: {alt_df['lon'].nunique()}\")\n",
    "print(f\"Unique longitudes in FC: {fc_df['lon'].nunique()}\")\n",
    "print(f\"Unique longitudes in FCH4: {fch4_df['lon'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87817938-9860-4f1d-8a5c-2b0f1b9113b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_lat_fc = set(alt_df['lat']) - set(fc_df['lat'])\n",
    "missing_lon_fc = set(alt_df['lon']) - set(fc_df['lon'])\n",
    "\n",
    "print(f\"Missing latitudes in FC compared to ALT: {missing_lat_fc}\")\n",
    "print(f\"Missing longitudes in FC compared to ALT: {missing_lon_fc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c485051f-a25e-4c0e-a963-645c8fb0cc24",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_spacing = 1 / 111\n",
    "\n",
    "for df in [alt_df, fc_df, fch4_df]:\n",
    "    df['lat'] = (df['lat'] // grid_spacing) * grid_spacing\n",
    "    df['lon'] = (df['lon'] // grid_spacing) * grid_spacing\n",
    "    if df is alt_df:\n",
    "        df = df.groupby(['datetime', 'lat', 'lon'], as_index=False)['alt'].min()\n",
    "    else:\n",
    "        df = df.groupby(['datetime', 'lat', 'lon'], as_index=False).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af240c08-b457-4d61-93ef-ff4ab46df321",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.array_equal(pd.unique(alt_df['datetime']), pd.unique(fc_df['datetime'])), \"Temporal misalignment between ALT and FC datasets.\"\n",
    "assert np.array_equal(pd.unique(alt_df['datetime']), pd.unique(fch4_df['datetime'])), \"Temporal misalignment between ALT and FCH4 datasets.\"\n",
    "assert np.array_equal(pd.unique(fc_df['datetime']), pd.unique(fch4_df['datetime'])), \"Temporal misalignment between FC and FCH4 datasets.\"\n",
    "\n",
    "assert np.array_equal(alt_df['lat'], fc_df['lat']), \"Spatial misalignment between ALT and FC in latitude.\"\n",
    "assert np.array_equal(alt_df['lon'], fc_df['lon']), \"Spatial misalignment between ALT and FC in longitude.\"\n",
    "assert np.array_equal(alt_df['lat'], fch4_df['lat']), \"Spatial misalignment between ALT and FCH4 in latitude.\"\n",
    "assert np.array_equal(alt_df['lon'], fch4_df['lon']), \"Spatial misalignment between ALT and FCH4 in longitude.\"\n",
    "assert np.array_equal(fc_df['lat'], fch4_df['lat']), \"Spatial misalignment between FC and FCH4 in latitude.\"\n",
    "assert np.array_equal(fc_df['lon'], fch4_df['lon']), \"Spatial misalignment between FC and FCH4 in longitude.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f998da3-bc2b-4147-8c6d-bfade342b080",
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in [alt_df, fc_df, fch4_df]:\n",
    "    print(f\"Before deduplication: {len(df)} rows\")\n",
    "    df = df.drop_duplicates(subset=['datetime', 'lat', 'lon']).reset_index(drop=True)\n",
    "    print(f\"After deduplication: {len(df)} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1df4119-66ce-4a43-a70a-dc3a28c93c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Unique latitudes in ALT: {alt_df['lat'].nunique()}\")\n",
    "print(f\"Unique latitudes in FC: {fc_df['lat'].nunique()}\")\n",
    "print(f\"Unique latitudes in FCH4: {fch4_df['lat'].nunique()}\")\n",
    "\n",
    "print(f\"Unique longitudes in ALT: {alt_df['lon'].nunique()}\")\n",
    "print(f\"Unique longitudes in FC: {fc_df['lon'].nunique()}\")\n",
    "print(f\"Unique longitudes in FCH4: {fch4_df['lon'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89d79b3-79ff-49db-a4cc-88616d51ea13",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_lat_fc = set(alt_df['lat']) - set(fc_df['lat'])\n",
    "missing_lon_fc = set(alt_df['lon']) - set(fc_df['lon'])\n",
    "\n",
    "print(f\"Missing latitudes in FC compared to ALT: {missing_lat_fc}\")\n",
    "print(f\"Missing longitudes in FC compared to ALT: {missing_lon_fc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe555d63-d387-4b52-b50c-71c5951d6594",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_df.to_parquet('alt_new_df_2.parquet')\n",
    "fc_df.to_parquet('fc_new_df_2.parquet')\n",
    "fch4_df.to_parquet('fch4_new_df_2.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab888a3-6ad2-4047-99d0-c8b5eb6781cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "alt_df = pd.read_parquet('alt_new_df_2.parquet')\n",
    "fc_df = pd.read_parquet('fc_new_df_2.parquet')\n",
    "fch4_df = pd.read_parquet('fch4_new_df_2.parquet')\n",
    "common_time = pd.to_datetime(pd.read_parquet('common_time.parquet')[0].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4aef282-5004-4e33-9ff3-db1fc333bbdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt = alt_df.sample(n=100)\n",
    "fc = fc_df.sample(n=100)\n",
    "fch4 = fch4_df.sample(n=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9eb966-8900-45d6-8526-af498f565f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt = alt[alt['datetime'].isin(common_time)].reset_index(drop=True)\n",
    "fc = fc[fc['datetime'].isin(common_time)].reset_index(drop=True)\n",
    "fch4 = fch4[fch4['datetime'].isin(common_time)].reset_index(drop=True)\n",
    "\n",
    "print(f\"Common time steps: {len(common_time)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dce0c5d-ec6a-4c2b-93d6-c40aea8b5f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all unique latitudes and longitudes\n",
    "lat_union = sorted(set(alt['lat']).union(fc['lat']).union(fch4['lat']))\n",
    "lon_union = sorted(set(alt['lon']).union(fc['lon']).union(fch4['lon']))\n",
    "\n",
    "print(f\"Unified latitudes: {len(lat_union)}\")\n",
    "print(f\"Unified longitudes: {len(lon_union)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ecc01f6-f5e2-4b70-9dd3-c467fd5ef1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Create the full spatial grid\n",
    "full_grid = pd.DataFrame(\n",
    "    [(lat, lon) for lat in lat_union for lon in lon_union],\n",
    "    columns=['lat', 'lon']\n",
    ")\n",
    "\n",
    "# Add temporal dimension to the grid\n",
    "full_grid = full_grid.assign(key=1)\n",
    "common_time_df = pd.DataFrame({'datetime': common_time, 'key': 1})\n",
    "full_grid = pd.merge(full_grid, common_time_df, on='key').drop('key', axis=1)\n",
    "\n",
    "# Merge each dataset with the full grid\n",
    "def expand_to_full_grid(df, full_grid, value_column):\n",
    "    merged = pd.merge(full_grid, df, on=['datetime', 'lat', 'lon'], how='left')\n",
    "    merged[value_column] = merged[value_column].fillna(np.nan)\n",
    "    return merged\n",
    "\n",
    "alt = expand_to_full_grid(alt, full_grid, 'alt')\n",
    "fc = expand_to_full_grid(fc, full_grid, 'fc')\n",
    "fch4 = expand_to_full_grid(fch4, full_grid, 'fch4')\n",
    "\n",
    "print(f\"ALT shape after merging: {alt.shape}\")\n",
    "print(f\"FC shape after merging: {fc.shape}\")\n",
    "print(f\"FCH4 shape after merging: {fch4.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfaf0672-f18a-4ff4-a3d1-9466e006b693",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.interpolate import griddata\n",
    "\n",
    "# Temporal interpolation for each grid point\n",
    "for df, var in zip([alt, fc, fch4], ['alt', 'fc', 'fch4']):\n",
    "    df[var] = df.groupby(['lat', 'lon'])[var].transform(\n",
    "        lambda x: x.interpolate(method='linear', limit_direction='both')\n",
    "    )\n",
    "\n",
    "# Spatial interpolation for remaining NaNs\n",
    "def spatial_interpolate(df, var, lat_union, lon_union):\n",
    "    for t in tqdm(common_time, desc=f\"Interpolating {var} spatially\"):\n",
    "        \n",
    "        time_slice = df[df['datetime'] == t]\n",
    "        points = time_slice[['lat', 'lon']].dropna().values\n",
    "        print(f\"Number of points: {len(points)}\")\n",
    "        values = time_slice[var].dropna().values\n",
    "        print(f\"Number of values: {len(values)}\")\n",
    "        \n",
    "        if len(points) == 0 or len(values) == 0:\n",
    "            print(f\"Skipping datetime {t}: insufficient data for interpolation.\")\n",
    "            continue\n",
    "\n",
    "        grid_lat, grid_lon = np.meshgrid(lat_union, lon_union, indexing='ij')\n",
    "        print(f\"Lat range in points: {points[:, 0].min()} to {points[:, 0].max()}\")\n",
    "        print(f\"Lon range in points: {points[:, 1].min()} to {points[:, 1].max()}\")\n",
    "        print(f\"Grid lat range: {lat_union[0]} to {lat_union[-1]}\")\n",
    "        print(f\"Grid lon range: {lon_union[0]} to {lon_union[-1]}\")\n",
    "        \n",
    "        interpolated = griddata(points, values, (grid_lat, grid_lon), method='linear')\n",
    "\n",
    "        # Update the dataframe\n",
    "        mask = df['datetime'] == t\n",
    "        interpolated_flatten = interpolated.flatten()\n",
    "        valid_interpolated = ~np.isnan(interpolated_flatten)\n",
    "\n",
    "        if valid_interpolated.sum() > 0:  # Ensure there are valid interpolations\n",
    "            df.loc[mask, var] = df.loc[mask, var].fillna(\n",
    "                pd.Series(interpolated_flatten[valid_interpolated])\n",
    "            )\n",
    "    return df\n",
    "\n",
    "alt = spatial_interpolate(alt, 'alt', lat_union, lon_union)\n",
    "fc = spatial_interpolate(fc, 'fc', lat_union, lon_union)\n",
    "fch4 = spatial_interpolate(fch4, 'fch4', lat_union, lon_union)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63894ec-e2fa-4747-9e16-3249d53269a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate alignment\n",
    "assert alt.shape == fc_df.shape == fch4.shape, \"Datasets are misaligned!\"\n",
    "assert alt[['datetime', 'lat', 'lon']].equals(fc[['datetime', 'lat', 'lon']]), \"Spatial-temporal misalignment!\"\n",
    "assert fc[['datetime', 'lat', 'lon']].equals(fch4[['datetime', 'lat', 'lon']]), \"Spatial-temporal misalignment!\"\n",
    "\n",
    "# Check for remaining NaNs\n",
    "print(f\"Remaining NaNs in ALT: {alt['alt'].isna().sum()}\")\n",
    "print(f\"Remaining NaNs in FC: {fc['fc'].isna().sum()}\")\n",
    "print(f\"Remaining NaNs in FCH4: {fch4['fch4'].isna().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24efbdaa-6a01-4a9f-bc78-6417b88ec5f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca48db9e-faf0-4149-ad44-1ae0a441577a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299b3cb3-17c6-43c8-a9c4-d029ae74fa9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02257d5a-dd62-4dde-8b34-e50592ef777a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9706f3-799a-4f4a-b361-aaed520691b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ccb3454-4244-4ba9-8ff9-c912eb199606",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6823b01-173a-4a3c-a746-a9431572ac64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2369e72b-fb12-47a5-8a03-36f708954829",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31279744-ef18-4b67-be71-d45e93ca62e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88deea84-cd13-47ea-bf56-6002f497b120",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b42c03-3189-4ab7-97f3-5082961d55b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320c7d23-d99a-435d-846f-09a0bfb672a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.interpolate import griddata\n",
    "\n",
    "# Create a grid for interpolation\n",
    "lat_grid, lon_grid = np.meshgrid(\n",
    "    sorted(master_spatial['lat'].unique()),\n",
    "    sorted(master_spatial['lon'].unique())\n",
    ")\n",
    "\n",
    "# Function to perform spatial interpolation for a single time step\n",
    "def spatial_interpolate(df, var, lat_grid, lon_grid):\n",
    "    points = df[['lat', 'lon']].dropna().values\n",
    "    values = df[var].dropna().values\n",
    "    grid_values = griddata(points, values, (lat_grid, lon_grid), method='linear')\n",
    "    return grid_values\n",
    "\n",
    "# Apply spatial interpolation for each time step\n",
    "interpolated_alt = []\n",
    "for t in tqdm(common_time, desc=\"Interpolating ALT spatially\"):\n",
    "    time_slice = alt_df[alt_df['datetime'] == t]\n",
    "    interpolated_slice = spatial_interpolate(time_slice, 'alt', lat_grid, lon_grid)\n",
    "    interpolated_alt.append(interpolated_slice)\n",
    "\n",
    "# Convert back to DataFrame if needed\n",
    "# Similarly for fc_df and fch4_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63416557-c47f-4934-97f9-591dd495f85a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f118c845-1166-43a7-bb04-41290f839fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for remaining NaNs and fill them\n",
    "alt_df['alt'] = alt_df['alt'].fillna(method='ffill').fillna(method='bfill')  # Last resort\n",
    "fc_df['fc'] = fc_df['fc'].fillna(method='ffill').fillna(method='bfill')\n",
    "fch4_df['fch4'] = fch4_df['fch4'].fillna(method='ffill').fillna(method='bfill')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3e9780-2db7-462c-b017-92a0a839cdce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b91cd49-e687-4ef3-8095-f62dec46685f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for NaNs\n",
    "print(f\"Remaining NaNs in ALT: {alt_df['alt'].isna().sum()}\")\n",
    "print(f\"Remaining NaNs in FC: {fc_df['fc'].isna().sum()}\")\n",
    "print(f\"Remaining NaNs in FCH4: {fch4_df['fch4'].isna().sum()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76b64b2-eedc-45dd-9c0e-447f4eaa6748",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f064e817-f4c2-451c-bbae-b7761c3a65fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e37cb68-a6d1-42e9-a639-0eb25d90e6e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aaed4e0-b0f5-4b99-ac49-6853c67e8829",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d54e86-b870-47d1-9a31-8ee61d8bd155",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 10_000\n",
    "\n",
    "def fill_grid_in_chunks(df, full_grid, chunk_size):\n",
    "    grid_chunks = [full_grid[i:i + chunk_size] for i in tqdm(range(0, len(full_grid), chunk_size), desc='Processing grid chunks...')]\n",
    "    filled_chunks = []\n",
    "    for chunk in grid_chunks:\n",
    "        merged_chunk = pd.merge(chunk, df, on=['datetime', 'lat', 'lon'], how='left')\n",
    "        filled_chunks.append(merged_chunk)\n",
    "    return pd.concat(filled_chunks, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb303b2c-4302-498a-ba63-8ee8e28ce302",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_grid2 = pd.DataFrame({\n",
    "    'datetime': np.repeat(common_time, len(lat_union) * len(lon_union)),\n",
    "    'lat': np.tile(np.repeat(lat_union, len(lon_union)), len(common_time)),\n",
    "    'lon': np.tile(lon_union, len(lat_union) * len(common_time))\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71419aa2-591f-490a-be7a-e5bd24bf9f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_grid.shape, full_grid2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13100390-7b09-4585-bf29-c90fa6591f33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f22009-50d9-4f3a-9389-06a4cefdfc5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_df.set_index(['datetime', 'lat', 'lon'], inplace=True)\n",
    "fc_df.set_index(['datetime', 'lat', 'lon'], inplace=True)\n",
    "fch4_df.set_index(['datetime', 'lat', 'lon'], inplace=True)\n",
    "\n",
    "full_grid.set_index(['datetime', 'lat', 'lon'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ca5ef4-e6b6-402a-bd3f-a8bbd446ce67",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_df = fill_grid_in_chunks(alt_df, full_grid, chunk_size)\n",
    "fc_df = fill_grid_in_chunks(fc_df, full_grid, chunk_size)\n",
    "fch4_df = fill_grid_in_chunks(fch4_df, full_grid, chunk_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a6f647-0f0a-4991-a45d-595245724c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"ALT shape after merging: {alt_df.shape}\")\n",
    "print(f\"FC shape after merging: {fc_df.shape}\")\n",
    "print(f\"FCH4 shape after merging: {fch4_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d98ebd5-9374-4b84-9cdd-2a325ea29e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_df.to_parquet('alt_new_df_3.parquet')\n",
    "fc_df.to_parquet('fc_new_df_3.parquet')\n",
    "fch4_df.to_parquet('fch4_new_df_3.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a92e8c2-d31d-44ca-9d64-f68fe3d3c04a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7959c804-354c-46d3-a4db-71fe56d59f39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ea7258-73a5-40fb-b4f1-bfcb9a09f01c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53cf34d9-806f-45b5-92d2-b3215cd07c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate data on the full grid\n",
    "alt_df = alt_df.groupby(['datetime', 'lat', 'lon'], as_index=False)['alt'].min()\n",
    "fc_df = fc_df.groupby(['datetime', 'lat', 'lon'], as_index=False).mean()\n",
    "fch4_df = fch4_df.groupby(['datetime', 'lat', 'lon'], as_index=False).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7916a6a-7ea6-4e1f-90ff-5b94d4277282",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing data\n",
    "alt_df['alt'] = alt_df['alt'].fillna(alt_df['alt'].min())  # Fill with min value\n",
    "fc_df.fillna(fc_df.mean(), inplace=True)  # Fill with column mean\n",
    "fch4_df.fillna(fch4_df.mean(), inplace=True)  # Fill with column mean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c22c7c4-7051-43bc-904e-dbbe0b2c863e",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert alt_df.shape == fc_df.shape == fch4_df.shape, \"Shape mismatch after alignment\"\n",
    "assert (alt_df[['datetime', 'lat', 'lon']] == fc_df[['datetime', 'lat', 'lon']]).all().all(), \"Spatial or temporal misalignment\"\n",
    "assert (alt_df[['datetime', 'lat', 'lon']] == fch4_df[['datetime', 'lat', 'lon']]).all().all(), \"Spatial or temporal misalignment\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cbd8bf9-68c3-43f3-a8f6-003f9991c055",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define tensor dimensions\n",
    "time_steps = len(common_time)\n",
    "lat_steps = len(lat_union)\n",
    "lon_steps = len(lon_union)\n",
    "\n",
    "# Initialize tensors\n",
    "alt_tensor = np.full((time_steps, lat_steps, lon_steps), np.nan, dtype=np.float32)\n",
    "fc_tensor = np.full((time_steps, lat_steps, lon_steps), np.nan, dtype=np.float32)\n",
    "fch4_tensor = np.full((time_steps, lat_steps, lon_steps), np.nan, dtype=np.float32)\n",
    "\n",
    "# Populate tensors\n",
    "for df, tensor, var in zip([alt_df, fc_df, fch4_df], [alt_tensor, fc_tensor, fch4_tensor], ['alt', 'fc', 'fch4']):\n",
    "    for idx, row in df.iterrows():\n",
    "        t_idx = common_time.index(row['datetime'])\n",
    "        lat_idx = lat_union.index(row['lat'])\n",
    "        lon_idx = lon_union.index(row['lon'])\n",
    "        tensor[t_idx, lat_idx, lon_idx] = row[var]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89489e3-fd9e-4b69-b6fa-3117a30bb150",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8382b157-6bba-4553-9f6f-7c3173de3682",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06c69be-2796-4ac1-8587-d39afa7b010f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae983854-5c7c-4482-bc91-aaeb75df0545",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861f751e-570c-4c64-a81a-39517f23d6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_df = alt_df[alt_df['lat'].isin(common_lat) & alt_df['lon'].isin(common_lon)]\n",
    "fc_df = fc_df[fc_df['lat'].isin(common_lat) & fc_df['lon'].isin(common_lon)]\n",
    "fch4_df = fch4_df[fch4_df['lat'].isin(common_lat) & fch4_df['lon'].isin(common_lon)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b69921a-324a-447e-9eac-d699383f33a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_df.to_parquet('alt_new_df.parquet')\n",
    "fc_df.to_parquet('fc_new_df.parquet')\n",
    "fch4_df.to_parquet('fch4_new_df.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec50c52-43e9-4f62-b855-32e966d7ff15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "alt_df = pd.read_parquet('alt_new_df.parquet')\n",
    "fc_df = pd.read_parquet('fc_new_df.parquet')\n",
    "fch4_df = pd.read_parquet('fch4_new_df.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1faff2-9c94-4f8c-83d7-6f55ae13c719",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify unique lat/lon values\n",
    "print(f\"Unique latitudes in ALT: {alt_df['lat'].nunique()}\")\n",
    "print(f\"Unique latitudes in FC: {fc_df['lat'].nunique()}\")\n",
    "print(f\"Unique latitudes in FCH4: {fch4_df['lat'].nunique()}\")\n",
    "\n",
    "print(f\"Unique longitudes in ALT: {alt_df['lon'].nunique()}\")\n",
    "print(f\"Unique longitudes in FC: {fc_df['lon'].nunique()}\")\n",
    "print(f\"Unique longitudes in FCH4: {fch4_df['lon'].nunique()}\")\n",
    "\n",
    "# Check for alignment\n",
    "assert np.array_equal(alt_df['lat'], fc_df['lat']), \"Spatial misalignment between ALT and FC in latitude.\"\n",
    "assert np.array_equal(alt_df['lon'], fc_df['lon']), \"Spatial misalignment between ALT and FC in longitude.\"\n",
    "assert np.array_equal(alt_df['lat'], fch4_df['lat']), \"Spatial misalignment between ALT and FCH4 in latitude.\"\n",
    "assert np.array_equal(alt_df['lon'], fch4_df['lon']), \"Spatial misalignment between ALT and FCH4 in longitude.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc66c5d4-b6e4-48fc-a68c-eec526fcc553",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd032d9f-c3df-4855-add2-579275f2fc32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f10610-fafc-4ca7-909b-7866c17c47df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c361385d-fb54-45c4-9d9c-fb820d82ff06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f26273c-9ffe-4ee9-90b1-1dfb51740f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_sparse = []\n",
    "fc_sparse = []\n",
    "fch4_sparse = []\n",
    "\n",
    "for t in tqdm(range(time_steps), desc='Processing sparse tensors...'):\n",
    "    alt_slice = alt_df[alt_df['datetime'] == pd.unique(alt_df['datetime'])[t]]\n",
    "    fc_slice = fc_df[fc_df['datetime'] == pd.unique(fc_df['datetime'])[t]]\n",
    "    fch4_slice = fch4_df[fch4_df['datetime'] == pd.unique(fch4_df['datetime'])[t]]\n",
    "    \n",
    "    alt_sparse.append(coo_matrix((alt_slice['alt'], (alt_slice['lat_index'], alt_slice['lon_index'])), shape=(lat_steps, lon_steps)))\n",
    "    fc_sparse.append(coo_matrix((fc_slice['fc'], (fc_slice['lat_index'], fc_slice['lon_index'])), shape=(lat_steps, lon_steps)))\n",
    "    fch4_sparse.append(coo_matrix((fch4_slice['fch4'], (fch4_slice['lat_index'], fch4_slice['lon_index'])), shape=(lat_steps, lon_steps)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1944976-69d4-410d-a985-5bd5d2e91c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for t, sparse_tensor in enumerate(alt_sparse):\n",
    "    print(f\"ALT Sparse Tensor - Time {t}: Shape = {sparse_tensor.shape}, Non-Zero Count = {sparse_tensor.nnz}\")\n",
    "for t, sparse_tensor in enumerate(fc_sparse):\n",
    "    print(f\"FC Sparse Tensor - Time {t}: Shape = {sparse_tensor.shape}, Non-Zero Count = {sparse_tensor.nnz}\")\n",
    "for t, sparse_tensor in enumerate(fch4_sparse):\n",
    "    print(f\"FCH4 Sparse Tensor - Time {t}: Shape = {sparse_tensor.shape}, Non-Zero Count = {sparse_tensor.nnz}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd3d154-7a67-4f18-8eec-cd2104b76de1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e91e761-ad0c-43d9-a32c-159f013668f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1136b547-c61a-4269-b5a5-42125b7f5bf7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ccc1a48-0c22-45ef-89f1-6a9d18383804",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1eca76-ad7b-4d53-9672-d6044e17a0c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f69350-c242-4cac-ab4a-150ebca793da",
   "metadata": {},
   "outputs": [],
   "source": [
    "smoothed_alt_sparse = []\n",
    "smoothed_fc_sparse = []\n",
    "smoothed_fch4_sparse = []\n",
    "\n",
    "for alt, fc, fch4 in tqdm(zip(alt_sparse, fc_sparse, fch4_sparse), total=len(alt_sparse), desc=\"Smoothing Sparse Tensors\"):\n",
    "    alt_dense = alt.toarray()\n",
    "    fc_dense = fc.toarray()\n",
    "    fch4_dense = fch4.toarray()\n",
    "    \n",
    "    alt_smoothed = gaussian_filter(alt_dense, sigma=1)\n",
    "    fc_smoothed = gaussian_filter(fc_dense, sigma=1)\n",
    "    fch4_smoothed = gaussian_filter(fch4_dense, sigma=1)\n",
    "\n",
    "    smoothed_alt_sparse.append(coo_matrix(alt_smoothed))\n",
    "    smoothed_fc_sparse.append(coo_matrix(fc_smoothed))\n",
    "    smoothed_fch4_sparse.append(coo_matrix(fch4_smoothed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d25e61-6519-4585-8328-7c4e43acfe78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a588f4e-1e8a-4061-9e2e-0e36b482da8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3ac442-9ecb-4f2c-83ae-077b6753634b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e04c66-45ad-4488-93cf-753238ab223c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Visualize a slice from the ALT tensor (e.g., first time step)\n",
    "plt.imshow(alt_tensor[0, :, :], cmap='viridis')\n",
    "plt.colorbar(label='ALT Value')\n",
    "plt.title('ALT Tensor - Time Step 0')\n",
    "plt.xlabel('Longitude Index')\n",
    "plt.ylabel('Latitude Index')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202e39a5-f550-4a78-8c87-bea3b2df0f39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14fc9856-1c44-48d9-8941-876c6d639009",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aafe8d9-0978-4a04-b6c7-c20a4ab40cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import hstack\n",
    "\n",
    "ensemble_sparse = []\n",
    "for alt, fc, fch4 in zip(smoothed_alt_sparse, smoothed_fc_sparse, smoothed_fch4_sparse):\n",
    "    ensemble_sparse.append(hstack([alt, fc, fch4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e44222d-d611-4aca-b6d1-f3f1253eab24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate ensemble tensor shape\n",
    "print(f\"Ensemble Tensor Shape: {ensemble_tensor.shape}\")\n",
    "assert ensemble_tensor.shape[0] == time_steps, \"Mismatch in time steps.\"\n",
    "assert ensemble_tensor.shape[1] == lat_steps, \"Mismatch in latitude steps.\"\n",
    "assert ensemble_tensor.shape[2] == lon_steps, \"Mismatch in longitude steps.\"\n",
    "assert ensemble_tensor.shape[3] == 3, \"Mismatch in feature count (ALT, FC, FCH4).\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518c39d6-e2a2-491c-a3b7-448cf332a9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "with h5py.File('ensemble_tensor_sparse.h5', 'w') as f:\n",
    "    grp = f.create_group('sparse_tensor')\n",
    "    for t, sparse_slice in enumerate(ensemble_sparse):\n",
    "        grp.create_dataset(f'time_{t}_data', data=sparse_slice.data)\n",
    "        grp.create_dataset(f'time_{t}_row', data=sparse_slice.row)\n",
    "        grp.create_dataset(f'time_{t}_col', data=sparse_slice.col)\n",
    "        grp.attrs['shape'] = sparse_slice.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10549641-a157-4c49-95f2-08029b4448ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bccb7f97-5e8b-493f-9af9-e167645f8778",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b952615d-b082-4f3e-8434-7842fcb420f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstruct dense tensor for a specific time step\n",
    "t = 0  # Example: first time step\n",
    "with h5py.File('ensemble_tensor_sparse.h5', 'r') as f:\n",
    "    data = f[f'sparse_tensor/time_{t}_data'][:]\n",
    "    row = f[f'sparse_tensor/time_{t}_row'][:]\n",
    "    col = f[f'sparse_tensor/time_{t}_col'][:]\n",
    "    shape = f['sparse_tensor'].attrs['shape']\n",
    "\n",
    "    # Reconstruct sparse matrix and convert to dense\n",
    "    sparse_slice = coo_matrix((data, (row, col)), shape=shape)\n",
    "    dense_slice = sparse_slice.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc3fe54-b3ff-46d4-b0ff-61683516b094",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908e2591-c7e7-4ad4-b036-5a674d11e2af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0585de5-9afe-46b5-a0ae-55033c6d61ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1852a2-9fc2-464e-8a37-19f0ddbd3eb0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8cc7512-7503-4bfb-8b58-e32951559697",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374577e9-36d1-403b-ba32-660022735232",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aafccfd-f72a-4b99-8a1b-22f9bd69f317",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777a1368-bbec-488a-80a8-5c48154af000",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a69dbd-7d27-4eb9-b726-98ad5a02081f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ca7a05-9d33-46a7-8b93-f82c0d2ba309",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a27e5ce-a7fa-4ed0-96f1-638b9cb83cb1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4742288b-f17d-4f7e-b8a2-7a53b23b0a61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0b4acf-83e8-4acc-a0b6-dabd8398c382",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define tensor dimensions\n",
    "time_steps = len(common_time)\n",
    "lat_steps = len(lat_mapping)\n",
    "lon_steps = len(lon_mapping)\n",
    "\n",
    "# Initialize tensors\n",
    "alt_tensor = np.full((time_steps, lat_steps, lon_steps), np.nan, dtype=np.float32)\n",
    "fc_tensor = np.full((time_steps, lat_steps, lon_steps), np.nan, dtype=np.float32)\n",
    "fch4_tensor = np.full((time_steps, lat_steps, lon_steps), np.nan, dtype=np.float32)\n",
    "\n",
    "# Populate tensors\n",
    "for df, tensor in zip([alt_df, fc_df, fch4_df], [alt_tensor, fc_tensor, fch4_tensor]):\n",
    "    t_indices = df['datetime_index'].to_numpy(dtype=np.int32)\n",
    "    lat_indices = df['lat_index'].to_numpy(dtype=np.int32)\n",
    "    lon_indices = df['lon_index'].to_numpy(dtype=np.int32)\n",
    "    values = df.iloc[:, -1].to_numpy(dtype=np.float32)  # Assume the last column contains data\n",
    "    tensor[t_indices, lat_indices, lon_indices] = values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9b359b-0596-4ff0-a444-fa5719ab8149",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check tensor shapes\n",
    "print(f\"ALT Tensor: {alt_tensor.shape}\")\n",
    "print(f\"FC Tensor: {fc_tensor.shape}\")\n",
    "print(f\"FCH4 Tensor: {fch4_tensor.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a8a2ab-0a0b-40f8-866c-69d7806284e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_mapping = {dt: idx for idx, dt in enumerate(common_time)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d63183-eba8-41b4-a8bf-f04ec2001144",
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in [alt_df, fc_df, fch4_df]:\n",
    "    df['datetime_index'] = df['datetime'].map(time_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e878da-fa8c-41b7-ad6c-5fe82f65d94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_df = alt_df[alt_df['datetime_index'].notna()]\n",
    "fc_df = fc_df[fc_df['datetime_index'].notna()]\n",
    "fch4_df = fch4_df[fch4_df['datetime_index'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13480769-c2ba-4980-b728-8dcc4f691a44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c38a6c5-a7d5-4f64-82ac-3a3cca134177",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4af3b1-fb01-4ba4-a8f0-475bcd821ba4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160a0c53-0b0e-4e26-8f2d-6b6f040f1235",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5110e0c-3efa-440c-b941-05d02dc0776f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9fa54fa-3f4a-4adf-8256-4fed6e725f43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30991d9c-daa0-43a6-a8b8-45e881fd0503",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae608d2-0104-4b85-8a04-fe328915f84d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a79692-40e5-4c87-b8b7-c81991576d55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b65c6d1-ddce-4a25-8e34-f46fd5ae06fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d06ec2-bfcc-441c-86dc-fd167c32c060",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bea37c7a-2aed-4c02-8352-e81814e8634e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Archived"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6701d673-5d05-450a-8769-5201dc790de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.merge(lat_lon_grid, alt_df, on=['datetime_index', 'lat_index', 'lon_index'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53919306-d143-4a04-9b29-d40dc18fdc9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = merged_df.sort_values(by=['datetime_index', 'lat_index', 'lon_index'])\n",
    "merged_df['alt'] = merged_df['alt'].interpolate(method='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06a7387-8c50-4332-9551-2bd64b57e29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.merge(merged_df, datetime_mapping, on='datetime_index', how='left')\n",
    "merged_df = pd.merge(merged_df, lat_mapping, on='lat_index', how='left')\n",
    "merged_df = pd.merge(merged_df, lon_mapping, on='lon_index', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111a309d-8486-4fcc-a768-c00a5d63f22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = merged_df[['datetime', 'lat', 'lon', 'alt', 'datetime_index', 'lat_index', 'lon_index']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bff947d-ddaf-4c86-95a5-6e460e6d9c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(merged_df.head())\n",
    "print(merged_df.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf2acf39-91be-403b-88df-cfea7ff46a98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b9e87e-df6e-447e-bf29-41b0c9698754",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3f4d2e-0059-4fd0-8faa-f25ef4a338b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8cae02-a93b-43e1-a7ae-246ca54582f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_df['time_index'] = alt_df['datetime'].dt.to_period('M').astype('category').cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa76d2e-7cd9-4ebd-a3ab-53694e21376a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lat_grid = np.linspace(alt_df['lat'].min(), alt_df['lat'].max(), 100)  # Adjust resolution\n",
    "lon_grid = np.linspace(alt_df['lon'].min(), alt_df['lon'].max(), 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb495673-8ace-49ba-8fea-28e3141c4bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_indices = alt_df['time_index'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4239e8-436b-41ef-a406-d10bc86e8249",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_indices = alt_df['time_index'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e10427-a8e5-4233-bd22-7af256b2999f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48de7092-754b-4c56-be07-9be6a2885002",
   "metadata": {},
   "outputs": [],
   "source": [
    "datetime_mapping = alt_df[['time_index', 'datetime']].drop_duplicates()  # Map time_index back to datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "030c9413-62ba-4f17-8803-96a42f3894af",
   "metadata": {},
   "outputs": [],
   "source": [
    "lat_mapping = alt_df[['time_index', 'datetime']].drop_duplicates()  # Map time_index back to datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85924eff-8726-410b-9a4a-09d99553858a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lat_lon_grid = pd.DataFrame(\n",
    "    [(t, lat, lon) for t in time_indices for lat in lat_grid for lon in lon_grid],\n",
    "    columns=['time_index', 'lat', 'lon']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcbaf073-615c-4ded-a736-ceab3de976ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "lat_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06af880b-0c43-4973-85cc-158bf1976677",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46e7861-714e-4ccb-916b-d3a473f49b76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b765f3d-d2e4-40ec-8e2a-bb75fcc68dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.merge(lat_lon_grid, alt_df, on=['time_index', 'lat', 'lon'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730d4105-b56b-4a12-82e4-ac8e7d064760",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(merged_df.head())\n",
    "print(merged_df.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843e7e3a-c57d-4f67-a7f5-ccd08b4dde66",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.merge(merged_df, datetime_mapping, on='time_index', how='left', suffixes=('', '_mapped'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1360e88f-d218-45a6-a6a4-bf0f5a9aa166",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(merged_df.head())\n",
    "print(merged_df.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27c6a30-476b-47c6-9dd8-af4a31b54c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df['datetime'] = merged_df['datetime'].fillna(merged_df['datetime_mapped'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18dc417b-8d0f-4a1b-b01e-9b34f65caac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(merged_df.head())\n",
    "print(merged_df.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74eb6e99-1bca-4289-a232-0007716c034b",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = merged_df.drop(columns=['datetime_mapped'])  # Clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c49a96b-95ef-4eef-bda6-84deda245fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(merged_df.head())\n",
    "print(merged_df.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd25b3a5-394e-4176-8b34-1ebf3ea33bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = merged_df.sort_values(by=['time_index', 'lat', 'lon'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7152bac7-fd2f-469c-b696-fceb2cdc979c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(merged_df.head())\n",
    "print(merged_df.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34dc3454-18cb-4e08-861e-dd8a8e1a88fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df['alt'] = merged_df['alt'].interpolate(method='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102fbe2a-e368-486a-9c74-d178803e25d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(merged_df.head())\n",
    "print(merged_df.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faefc375-b50d-4d2b-b9b2-115c1a1cfff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = merged_df[['time_index','lat','lon','alt']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61fe6400-1ace-4aff-b8c5-b83892b8dc08",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor = merged_df.pivot_table(\n",
    "    index='time_index', columns=['lat', 'lon'], values='alt', aggfunc='min'\n",
    ").fillna(0).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c87f8db6-806a-408d-9376-ec5ad4672283",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tensor_reshaped = tensor.reshape(len(pd.unique(alt_df['time_index'])), len(lat_grid), len(lon_grid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5ac660-3b99-4519-b82f-396746cbf5f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Tensor Shape: {tensor.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8abc07c0-a67c-4344-bf92-dfadce4bd17f",
   "metadata": {},
   "outputs": [],
   "source": [
    "reshaped_tensor = tensor.reshape(701,100,100,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55cf36f-27e1-4ecd-8863-c42905ba218f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9828d350-d7e1-4e40-be0e-8ef0b83e1071",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8c9164-36e6-4c33-821f-b123131f4201",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44374419-9d5b-49e5-88c1-2db7fce6d0ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef00a8f9-1b16-452f-8beb-8bb0d8271b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_df['timestamp'] = alt_df['datetime'].astype('int64')//10**9 #datetime timestamp encoding | conversion to seconds from epoch for DL methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049b4eba-34ce-4d7b-94ae-af33f703c6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_df = alt_df[['timestamp','lat','lon','alt']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83073397-2b0f-4550-8938-1c53ed231b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_df.columns = ['datetime','lat','lon','alt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef5374a-eaee-49b3-997c-98d21b9f6762",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_df.alt = alt_df.alt.astype('float16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f9dbad-f277-40c7-a548-c29bdc54ab42",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#6708 alt_df.datetime.nunique()\n",
    "#1092 alt_df.lat.nunique()\n",
    "#1092 alt_df.lon.nunique()\n",
    "#238860 alt_df.alt.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17daf67-7228-4f46-8cbe-f98d468c542e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#3163257965+55-6708 (gives us the next factoring opportunity to divide dataset into nunique() values of datetime)\n",
    "#so, subtract 6653 from 3163257965\n",
    "#3163251312"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906c04af-2923-4d97-af7b-b37acb580bd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a517fb-f8de-433f-a1da-33bd1dc57ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(alt_df.dtypes)\n",
    "print(alt_df[['lat', 'lon']].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b345eb-fa1a-40bc-a1ce-edaa58a9fdd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_df['lat'] = pd.to_numeric(alt_df['lat'], errors='coerce')\n",
    "alt_df['lon'] = pd.to_numeric(alt_df['lon'], errors='coerce')\n",
    "alt_df = alt_df.dropna(subset=['lat', 'lon'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c380f1-4100-4985-9f4b-61c76776a0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2065486-5210-4d63-8f15-64ea9122addb",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_df = alt_df.sort_values(by=['datetime', 'lat', 'lon'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f9df1c-3190-4650-811a-5c73d95a0400",
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File('alt_df_nonan_norm_sorted.h5', 'w') as f:\n",
    "    f.create_dataset('alt', data=alt_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b76cc62-4967-440f-ab1a-cf7f78b12506",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b8a86a0-4500-4d14-927c-2ad6cd4f999b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b20fdda-067a-4827-baeb-33a4b17bfe2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43af2b5f-07f2-4e57-8b30-fa0752787953",
   "metadata": {},
   "outputs": [],
   "source": [
    "# duplicates = alt_df.duplicated(subset=['lat', 'lon'])\n",
    "# print(\"Number of duplicates:\", duplicates.sum())\n",
    "# #Number of duplicates: 3162257873"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a017c34-4ce3-4ebf-959e-8f5c86320c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# alt_df = alt_df.groupby(['datetime', 'lat', 'lon'])['alt'].min().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d066c53-ebc8-4936-9a26-a64fa58dab37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "\n",
    "dask_df = dd.read_parquet('alt_df_nonan_norm_dtencoded_tonumeric_nonan.parquet')\n",
    "grouped_dask_df = dask_df.groupby(['datetime', 'lat', 'lon'])['alt'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990af439-2ca6-4fd5-9ba7-632c8cd98949",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_df = grouped_dask_df.compute(); del dask_df, grouped_dask_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac8fd09-1280-4290-b8aa-3d2b64e683ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec0b291-2969-4f4b-a316-e20c4b69e5ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062cb805-102c-47df-a112-4b725690fba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#If duplicates exist:\n",
    "temp_df = temp_df.groupby(['lat', 'lon'])['alt'].min().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581e6bb5-b4cf-46af-8191-048aaad98b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_lats = np.linspace(alt_df['lat'].min(), alt_df['lat'].max(), 1092)\n",
    "all_lons = np.linspace(alt_df['lon'].min(), alt_df['lon'].max(), 1092)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86514ad-3ace-4884-97c7-f6a590710ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = grid.reindex(index=all_lats, columns=all_lons)\n",
    "grid = grid.interpolate(method='linear', axis=0).interpolate(method='linear', axis=1).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855fba3b-1009-4392-88d9-ad953c83462e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if temp_df.empty:\n",
    "    spatiotemporal_data[i, :, :] = 0\n",
    "    continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307ac33a-9f2a-430e-a893-b1a68d0c2ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamps = alt_df['datetime'].unique()\n",
    "spatiotemporal_data = np.zeros((len(timestamps), 1092, 1092), dtype=np.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313a3882-d36b-4e6c-b059-ee216ab3cef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_lats = np.linspace(alt_df['lat'].min(), alt_df['lat'].max(), 1092)\n",
    "all_lons = np.linspace(alt_df['lon'].min(), alt_df['lon'].max(), 1092)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728d5609-932b-46b9-bc26-047bf340317b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, timestamp in enumerate(timestamps):\n",
    "    temp_df = alt_df[alt_df['datetime'] == timestamp]\n",
    "    temp_df = temp_df.groupby(['lat', 'lon'])['alt'].min().reset_index()\n",
    "    if temp_df.empty:\n",
    "        spatiotemporal_data[i, :, :] = 0\n",
    "        continue\n",
    "    grid = temp_df.pivot(index='lat', columns='lon', values='alt')\n",
    "    grid = grid.reindex(index=all_lats, columns=all_lons)\n",
    "    grid = grid.interpolate(method='linear', axis=0).interpolate(method='linear', axis=1).fillna(0)\n",
    "    spatiotemporal_data[i, :, :] = grid.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3efc51bf-d5b8-40c9-a63a-4ac3f915fbaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Spatiotemporal data shape:\", spatiotemporal_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077dff2c-eb9a-40c4-9a0b-32a05d8665cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca26178-a997-49a1-8bb5-a79362fbe7c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed29c73-8eab-4ba5-9f59-06d644819045",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f16564-3869-4254-97bf-98de4cbdf7e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed0c418-d632-427a-ad82-1da228f6c37a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2d58d8-854c-4857-b11f-898a964ddaa4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10dc2923-8a8e-4d87-8eb4-c728cf08c9e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d090ef-c70a-4150-8f2f-42d8a15abdbc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "826ef909-2139-4ad5-a6fe-092414d5c9c4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Archived"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb124fdc-5e55-4127-8bff-5ca46523d704",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "alt_df = pd.read_parquet('alt_df_nonan_norm.parquet')\n",
    "#alt_df = pd.read_parquet('/Volumes/JPL/geocryoai/modeling/data/input/alt/alt_df_nonan_norm.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579ce962-b976-41cf-930a-b62134960e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "TIME_STEP_CHUNK = 30\n",
    "LAT_DIM = 1092\n",
    "LON_DIM = 1092\n",
    "OUTPUT_PATH = \"processed_tensor_alt.h5\"\n",
    "\n",
    "with h5py.File(OUTPUT_PATH, 'w') as f:\n",
    "    unique_datetimes = alt_df['datetime'].unique()\n",
    "    total_time_steps = len(unique_datetimes)\n",
    "    f.create_dataset(\n",
    "        'tensor', \n",
    "        shape=(total_time_steps, LAT_DIM, LON_DIM), \n",
    "        dtype='float32', \n",
    "        chunks=(TIME_STEP_CHUNK, LAT_DIM, LON_DIM),\n",
    "        compression='gzip'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc7f513-8018-4551-a4b2-1d6e84ad83da",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunked_datetimes = [\n",
    "    unique_datetimes[i:i + TIME_STEP_CHUNK]\n",
    "    for i in range(0, len(unique_datetimes), TIME_STEP_CHUNK)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd1e8bb-844e-48c5-919d-99ecd5f20a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_idx = 0\n",
    "for chunk in tqdm(chunked_datetimes, desc=\"Processing datetime chunks\"):\n",
    "    chunk_data = alt_df[alt_df['datetime'].isin(chunk)]\n",
    "    grids = []\n",
    "    for dt in tqdm(chunk, desc=\"Processing spatial grids\", leave=False):\n",
    "        grid = chunk_data[chunk_data['datetime'] == dt].pivot_table(\n",
    "            index='lat',\n",
    "            columns='lon',\n",
    "            values='alt',\n",
    "            aggfunc='min'\n",
    "        )\n",
    "        grid = grid.reindex(index=np.linspace(grid.index.min(), grid.index.max(), LAT_DIM),\n",
    "                            columns=np.linspace(grid.columns.min(), grid.columns.max(), LON_DIM),\n",
    "                            method='nearest').fillna(0)\n",
    "        grids.append(grid.values)\n",
    "    tensor_chunk = np.stack(grids, axis=0)\n",
    "    with h5py.File(OUTPUT_PATH, 'a') as f:\n",
    "        f['tensor'][current_idx:current_idx + tensor_chunk.shape[0], :, :] = tensor_chunk\n",
    "        current_idx += tensor_chunk.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45482fde-55c2-4a15-8559-d69f766ecadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(OUTPUT_PATH, 'r') as f:\n",
    "    tensor = f['tensor'][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1552714-0ee7-46c6-8440-e81367ec67da",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor = tensor[..., np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b538662-c5dd-432d-9622-116f7323a3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = tensor.shape[0] // TIME_STEP_CHUNK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f145ed2-3e28-406d-a917-2feeaaf6bf92",
   "metadata": {},
   "outputs": [],
   "source": [
    "reshaped_tensor = tensor[:samples * TIME_STEP_CHUNK].reshape(\n",
    "    samples, TIME_STEP_CHUNK, LAT_DIM, LON_DIM, 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34fde99b-3c57-4324-8e59-733a8e61b73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "reshaped_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855bd22b-3d2f-481e-8aa7-5695f006e4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File('alt_df_nonan_norm_5dtensor.h5', 'w') as f:\n",
    "    f.create_dataset('alt_5dtensor', data=reshaped_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adcca582-6585-427c-87b4-3841dacaa345",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt = reshaped_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79fa80dd-092f-41bf-86c8-74fdd2ab615c",
   "metadata": {},
   "outputs": [],
   "source": [
    "del tensor, samples, reshaped_tensor, current_idx, tensor_chunk, grids, grid, chunk, chunk_data, chunked_datetimes, unique_datetimes, total_time_steps, alt_df, OUTPUT_PATH, TIME_STEP_CHUNK, LAT_DIM, LON_DIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5185daa0-07c7-432f-85bd-f7e914705530",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a619348a-e54a-4267-8aba-4334645e53e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c9d6d8-a321-4d17-b3c8-b129d3436a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "fc_df = pd.read_parquet('fc_df_nonan_norm.parquet')\n",
    "#fc_df = pd.read_parquet('/Volumes/JPL/geocryoai/modeling/data/input/fc/fc_df_nonan_norm.parquet')\n",
    "fc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be017337-e22b-4fdd-8440-08e315d22604",
   "metadata": {},
   "outputs": [],
   "source": [
    "#314 fc_df.datetime.nunique() #--> unique values of datetime\n",
    "#430493 fc_df.lat.nunique() #--> unique values of latitude\n",
    "#466928 fc_df.lon.nunique() #--> unique values of longitude\n",
    "#2229070 fc_df.fc.nunique() #--> unique values of fc "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d98597-908a-44be-8a62-7af9dc895094",
   "metadata": {},
   "outputs": [],
   "source": [
    "fc_df['timestamp'] = fc_df['datetime'].astype('int64')//10**6 #datetime timestamp encoding | conversion to seconds from epoch for DL methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b445d2d1-39b6-4f73-8f10-587558354650",
   "metadata": {},
   "outputs": [],
   "source": [
    "fc_df = fc_df[['timestamp','lat','lon','fc']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e148f82e-985f-4937-8342-22acffa92d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "fc_df.columns = ['datetime','lat','lon','fc']\n",
    "fc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c66e01-b809-4791-84df-abd3d14b98b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "TIME_STEP_CHUNK = 30\n",
    "LAT_DIM = 1092\n",
    "LON_DIM = 1092\n",
    "OUTPUT_PATH = \"processed_tensor_fc.h5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5aeb336-c092-4599-b4c7-bfcd7b0b41a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(OUTPUT_PATH, 'w') as f:\n",
    "    unique_datetimes = fc_df['datetime'].unique()\n",
    "    total_time_steps = len(unique_datetimes)\n",
    "    f.create_dataset(\n",
    "        'tensor', \n",
    "        shape=(total_time_steps, LAT_DIM, LON_DIM), \n",
    "        dtype='float32', \n",
    "        chunks=(TIME_STEP_CHUNK, LAT_DIM, LON_DIM),\n",
    "        compression='gzip'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d55ce7-aa77-46f9-8a44-095e16295757",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunked_datetimes = [\n",
    "    unique_datetimes[i:i + TIME_STEP_CHUNK]\n",
    "    for i in range(0, len(unique_datetimes), TIME_STEP_CHUNK)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64a5357-d582-4342-88ca-1bfc4a224fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_idx = 0\n",
    "for chunk in tqdm(chunked_datetimes, desc=\"Processing datetime chunks\"):\n",
    "    chunk_data = fc_df[fc_df['datetime'].isin(chunk)]\n",
    "    grids = []\n",
    "    for dt in tqdm(chunk, desc=\"Processing spatial grids\", leave=False):\n",
    "        grid = chunk_data[chunk_data['datetime'] == dt].pivot_table(\n",
    "            index='lat',\n",
    "            columns='lon',\n",
    "            values='fc',\n",
    "            aggfunc='sum'\n",
    "        )\n",
    "        grid = grid.reindex(index=np.linspace(grid.index.min(), grid.index.max(), LAT_DIM),\n",
    "                            columns=np.linspace(grid.columns.min(), grid.columns.max(), LON_DIM),\n",
    "                            method='nearest').fillna(0)\n",
    "        grids.append(grid.values)\n",
    "    tensor_chunk = np.stack(grids, axis=0)\n",
    "    with h5py.File(OUTPUT_PATH, 'a') as f:\n",
    "        f['tensor'][current_idx:current_idx + tensor_chunk.shape[0], :, :] = tensor_chunk\n",
    "        current_idx += tensor_chunk.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef9c126-43c4-4ed8-b494-2ce3d225a7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(OUTPUT_PATH, 'r') as f:\n",
    "    tensor = f['tensor'][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cdcb03f-6612-421a-96c4-233d014fa9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor = tensor[..., np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b2c549-124d-4140-887d-0fc512ab5d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = tensor.shape[0] // TIME_STEP_CHUNK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc78fe99-f111-4c0d-a01a-ff6fee479377",
   "metadata": {},
   "outputs": [],
   "source": [
    "reshaped_tensor = tensor[:samples * TIME_STEP_CHUNK].reshape(\n",
    "    samples, TIME_STEP_CHUNK, LAT_DIM, LON_DIM, 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7cadb0a-8fda-41ba-9d7c-7337742bb1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "reshaped_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84afad6c-e6ab-45c9-bc49-a57f5e49e421",
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File('fc_df_nonan_norm_5dtensor.h5', 'w') as f:\n",
    "    f.create_dataset('fc_5dtensor', data=reshaped_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22eb2972-b2e7-4666-be57-0aa3ad77bc9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fc = reshaped_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eec5568-6973-4365-a92c-228e5ed27613",
   "metadata": {},
   "outputs": [],
   "source": [
    "del tensor, samples, reshaped_tensor, current_idx, tensor_chunk, grids, grid, chunk, chunk_data, chunked_datetimes, unique_datetimes, total_time_steps, fc_df, OUTPUT_PATH, TIME_STEP_CHUNK, LAT_DIM, LON_DIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df087ae5-5152-42cc-a6e2-f93c8e9a795c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6394d48-d066-4731-9706-aa5ba61a545b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c287f4-60a0-4433-99e3-3fa5cd8d3b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "fch4_df = pd.read_parquet('fch4_df_nonan_norm.parquet')\n",
    "#fch4_df = pd.read_parquet('/Volumes/JPL/geocryoai/modeling/data/input/fch4/fch4_df_nonan_norm.parquet')\n",
    "fch4_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b79cc3f-1346-401c-9eab-4a6efdaeeb38",
   "metadata": {},
   "outputs": [],
   "source": [
    "#314 fch4_df.datetime.nunique() #--> unique values of datetime\n",
    "#430493 fch4_df.lat.nunique() #--> unique values of latitude\n",
    "#466928 fc_df.lon.nunique() #--> unique values of longitude\n",
    "#4085540 fc_df.fc.nunique() #--> unique values of fc "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd81d79-86f8-416c-94b8-3534fa690ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "fch4_df['timestamp'] = fch4_df['datetime'].astype('int64')//10**6 #datetime timestamp encoding | conversion to seconds from epoch for DL methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0feb6c5b-c384-4336-b965-fac951419751",
   "metadata": {},
   "outputs": [],
   "source": [
    "fch4_df = fch4_df[['timestamp','lat','lon','fch4']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2762a199-b16a-48d0-a02c-23ee40c9c5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "fch4_df.columns = ['datetime','lat','lon','fch4']\n",
    "fch4_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5f09c6-0ce7-4858-816e-c55ad5303440",
   "metadata": {},
   "outputs": [],
   "source": [
    "TIME_STEP_CHUNK = 30\n",
    "LAT_DIM = 1092\n",
    "LON_DIM = 1092\n",
    "OUTPUT_PATH = \"fch4_processed_tensors.h5\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71bdad56-fb06-4f4a-9bed-38e4844253f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(OUTPUT_PATH, 'w') as f:\n",
    "    unique_datetimes = fch4_df['datetime'].unique()\n",
    "    total_time_steps = len(unique_datetimes)\n",
    "    f.create_dataset(\n",
    "        'tensor', \n",
    "        shape=(total_time_steps, LAT_DIM, LON_DIM), \n",
    "        dtype='float32', \n",
    "        chunks=(TIME_STEP_CHUNK, LAT_DIM, LON_DIM), \n",
    "        compression='gzip'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31f6a05-1514-455a-b49e-d3c1722739ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunked_datetimes = [\n",
    "    unique_datetimes[i:i + TIME_STEP_CHUNK]\n",
    "    for i in range(0, len(unique_datetimes), TIME_STEP_CHUNK)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50fbdd0b-11fd-4414-a343-9209c058cac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_idx = 0\n",
    "for chunk in tqdm(chunked_datetimes, desc=\"Processing datetime chunks\"):\n",
    "    chunk_data = fch4_df[fch4_df['datetime'].isin(chunk)]\n",
    "    grids = []\n",
    "    for dt in tqdm(chunk, desc=\"Processing spatial grids\", leave=False):\n",
    "        grid = chunk_data[chunk_data['datetime'] == dt].pivot_table(\n",
    "            index='lat',\n",
    "            columns='lon',\n",
    "            values='fch4',\n",
    "            aggfunc='sum'\n",
    "        )\n",
    "        grid = grid.reindex(index=np.linspace(grid.index.min(), grid.index.max(), LAT_DIM),\n",
    "                            columns=np.linspace(grid.columns.min(), grid.columns.max(), LON_DIM),\n",
    "                            method='nearest').fillna(0)\n",
    "        grids.append(grid.values)\n",
    "    tensor_chunk = np.stack(grids, axis=0)\n",
    "    with h5py.File(OUTPUT_PATH, 'a') as f:\n",
    "        f['tensor'][current_idx:current_idx + tensor_chunk.shape[0], :, :] = tensor_chunk\n",
    "        current_idx += tensor_chunk.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4265113f-c0a8-4075-91d7-237dc8706987",
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(OUTPUT_PATH, 'r') as f:\n",
    "    tensor = f['tensor'][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1654eee-de97-4c26-9250-0395bfbf34ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor = tensor[..., np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9831d87-a5e9-4dc4-a85b-d957e9f9bcf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = tensor.shape[0] // TIME_STEP_CHUNK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9adb2d7a-38cc-4bca-947e-8e43021071e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "reshaped_tensor = tensor[:samples * TIME_STEP_CHUNK].reshape(\n",
    "    samples, TIME_STEP_CHUNK, LAT_DIM, LON_DIM, 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1a5804-65a0-4feb-a8cd-89aa2ee4cddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "reshaped_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f96d87-d0f6-4afb-9fb2-c9ea98a3bcae",
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File('fch4_df_nonan_norm_5dtensor.h5', 'w') as f:\n",
    "    f.create_dataset('fch4_5dtensor', data=reshaped_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2662495-43a1-4f88-9308-ad8ccea494d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fch4 = reshaped_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a0c6bc-3db3-4138-8c93-b909353d14c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "del tensor, samples, reshaped_tensor, current_idx, tensor_chunk, grids, grid, chunk, chunk_data, chunked_datetimes, unique_datetimes, total_time_steps, fch4_df, OUTPUT_PATH, TIME_STEP_CHUNK, LAT_DIM, LON_DIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155cf482-60dd-42bd-8b09-0fa1e4ed3bcc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ebb70c-ade4-4041-9428-fa854f70e35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2eff7d1-0228-4387-a30a-5fdba2926606",
   "metadata": {},
   "outputs": [],
   "source": [
    "fc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4504ddf3-a607-4632-9bc5-a8717c18b540",
   "metadata": {},
   "outputs": [],
   "source": [
    "fch4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d75b13-0dfd-4c77-a895-c8aa04964980",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "910bb25c-6e65-4979-af2c-eed206fbf33f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Archived"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58ca387-9c46-4309-9f8c-780d6c32539c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2decdb-e727-4d55-8282-727e0bbbb6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#alt_df.alt.shape\n",
    "#(3163257965,)\n",
    "#fc_df.fc.shape\n",
    "#(96654894,)\n",
    "#fch4_df.fch4.shape\n",
    "#(96654894,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f801e8a-f0f0-4d19-9b23-a6e1cf63e285",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_df.alt.values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75746dfb-bf77-4315-b6db-b41c8b201282",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(divisorGenerator(3163257965)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80876b0-1112-4803-bee0-f676bbf7721d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# padded_alt_df = np.pad(alt_df.alt.values, (0, 9825), mode='constant')\n",
    "\n",
    "# # Check the new shape of the padded array\n",
    "# print(f\"Padded array shape: {padded_alt_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ae82c9-6358-4871-a504-0d40587dd630",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# reshaped_array = padded_alt_df.reshape(325, 100, 100, 973)\n",
    "\n",
    "# # Check the new shape of the reshaped array\n",
    "# print(f\"New reshaped array shape: {reshaped_array.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc81451-2095-4f9b-b876-4aa119e5b1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_df.alt.values.reshape(-1, 1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c461cf3-6a72-4b1d-b341-42c966355086",
   "metadata": {},
   "outputs": [],
   "source": [
    "#6708 alt_df.datetime.nunique() --> (unique values of datetime)\n",
    "#1092 alt_df.lat.nunique() --> (unique values of lat)\n",
    "#1092 alt_df.lon.nunique() --> (unique values of lon)\n",
    "#238860 alt_df.alt.nunique() --> (unique values of alt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3568b2d-c803-4f7f-8bcd-7651161cfcf4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4b95db-3b30-4ad3-864e-704f3ec9610d",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_elements = 3163257965\n",
    "factors = []\n",
    "\n",
    "for i in range(1, int(total_elements ** 0.5) + 1):\n",
    "    if total_elements % i == 0:\n",
    "        factors.append(i)\n",
    "\n",
    "print(factors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f326ad-366c-4b09-a20e-2706f119fe1d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "\n",
    "# # Total number of elements in the original array\n",
    "# total_elements = 3163257965\n",
    "\n",
    "# # Choose DIM1, DIM2, DIM3 such that their product divides total_elements\n",
    "# DIM1 = 325\n",
    "# DIM2 = 100\n",
    "# DIM3 = 100\n",
    "\n",
    "# # Calculate the product of the first three dimensions\n",
    "# product_of_first_three = DIM1 * DIM2 * DIM3\n",
    "\n",
    "# # Calculate the target size (next multiple of product_of_first_three)\n",
    "# target_size = (total_elements // product_of_first_three) * product_of_first_three\n",
    "# if total_elements % product_of_first_three != 0:\n",
    "#     target_size += product_of_first_three\n",
    "\n",
    "# # Calculate the padding required\n",
    "# padding_needed = target_size - total_elements\n",
    "\n",
    "# # If the original array size is smaller than the target size, pad it\n",
    "# if total_elements < target_size:\n",
    "#     padded_array = np.pad(alt_df.alt.values, (0, padding_needed), mode='constant')\n",
    "#     print(f\"Padded array size: {padded_array.size}\")\n",
    "# else:\n",
    "#     padded_array = alt_df.alt.values\n",
    "#     print(f\"No padding needed\")\n",
    "\n",
    "# # Check the original and target sizes\n",
    "# print(f\"Original size: {total_elements}, Target size: {target_size}\")\n",
    "\n",
    "# # Reshape the padded array into the desired 4D shape with a singleton dimension at the end\n",
    "# reshaped_array = padded_array.reshape(DIM1, DIM2, DIM3, padded_array.size // (DIM1 * DIM2 * DIM3), 1)\n",
    "\n",
    "# # Output the new shape\n",
    "# print(f\"Reshaped array shape: {reshaped_array.shape}\")\n",
    "\n",
    "\n",
    "# # Padded array size: 3165500000\n",
    "# # Original size: 3163257965, Target size: 3165500000\n",
    "# # Reshaped array shape: (325, 100, 100, 974, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1819b5-a155-42c9-a9d9-59eb467467e7",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# import h5py\n",
    "\n",
    "# # Save the reshaped array as an HDF5 file\n",
    "# with h5py.File('alt_df_nonan_norm_5dtensor_padded.h5', 'w') as f:\n",
    "#     f.create_dataset('alt_5dtensor', data=reshaped_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70e3a28-37c8-4036-87ea-7bb3c22183e6",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# alt = reshaped_array; del alt_df, reshaped_arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96686be-7e10-4ed5-a99d-664cd93cd04e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# del padded_alt_df, padded_array, padding_needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c03afe2-b972-42b2-8aa7-822db1b77124",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "total_elements = 3163257965\n",
    "\n",
    "DIM1 = 7\n",
    "DIM2 = 100\n",
    "DIM3 = 100\n",
    "\n",
    "product_of_first_three = DIM1 * DIM2 * DIM3\n",
    "\n",
    "target_size = (total_elements // product_of_first_three) * product_of_first_three\n",
    "\n",
    "if total_elements > target_size:\n",
    "    trimmed_array = alt_df.alt.values[:target_size]  # Trim the array to the target size\n",
    "    print(f\"Trimmed array size: {trimmed_array.size}\")\n",
    "else:\n",
    "    trimmed_array = alt_df.alt.values\n",
    "    print(f\"No trimming needed\")\n",
    "\n",
    "print(f\"Original size: {total_elements}, Target size: {target_size}\")\n",
    "\n",
    "reshaped_array = trimmed_array.reshape(DIM1, DIM2, DIM3, trimmed_array.size // (DIM1 * DIM2 * DIM3), 1)\n",
    "\n",
    "print(f\"Reshaped array shape: {reshaped_array.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba61fbf-1110-4ed9-8438-633f549f7b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "reshaped_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f976fc0-8969-44bf-84c7-4ebc893efd1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "# Save the reshaped array as an HDF5 file\n",
    "with h5py.File('alt_df_nonan_norm_5dtensor_trimmed_2.h5', 'w') as f:\n",
    "    f.create_dataset('alt_5dtensor', data=reshaped_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d885a65b-046f-48b3-a55e-117aadfd01d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt = reshaped_array; del alt_df, reshaped_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06972d5c-3738-4733-83c3-1b9a8bc695bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae8d509-df13-4006-8a5a-40292762d869",
   "metadata": {},
   "outputs": [],
   "source": [
    "fc_df.fc.values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9a991f-219d-4ebc-9cc7-98c5df7aafa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(divisorGenerator(96654894)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0a0de7-5e33-460c-b559-ee0fe33caf77",
   "metadata": {},
   "outputs": [],
   "source": [
    "fc_df.fc.values.reshape(-1, 1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0f0389-47a6-4be4-886d-791e68fe7cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_elements = 96654894\n",
    "\n",
    "factors = []\n",
    "\n",
    "for i in range(1, int(total_elements ** 0.5) + 1):\n",
    "    if total_elements % i == 0:\n",
    "        factors.append(i)\n",
    "\n",
    "print(factors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fdad12d-ba76-4a46-8fc0-2dd5ccc0a126",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "total_elements = 96654894\n",
    "\n",
    "DIM1 = 7\n",
    "DIM2 = 100\n",
    "DIM3 = 100\n",
    "\n",
    "product_of_first_three = DIM1 * DIM2 * DIM3\n",
    "\n",
    "target_size = (total_elements // product_of_first_three) * product_of_first_three\n",
    "\n",
    "if total_elements > target_size:\n",
    "    trimmed_array = fc_df.fc.values[:target_size]\n",
    "    print(f\"Trimmed array size: {trimmed_array.size}\")\n",
    "else:\n",
    "    trimmed_array = fc_df.fc.values\n",
    "    print(f\"No trimming needed\")\n",
    "\n",
    "print(f\"Original size: {total_elements}, Target size: {target_size}\")\n",
    "\n",
    "reshaped_array = trimmed_array.reshape(DIM1, DIM2, DIM3, trimmed_array.size // (DIM1 * DIM2 * DIM3), 1)\n",
    "\n",
    "print(f\"Reshaped array shape: {reshaped_array.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ffd5183-4bd2-4841-8804-fa49ab0eedaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "reshaped_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5512a80c-81be-45a2-afe8-3c25d0b00a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "# Save the reshaped array as an HDF5 file\n",
    "with h5py.File('fc_df_nonan_norm_5dtensor_trimmed_2.h5', 'w') as f:\n",
    "    f.create_dataset('fc_5dtensor', data=reshaped_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f718f751-5b61-4326-ae98-4f1ff3a11c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fc = reshaped_array; del fc_df, reshaped_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab96c744-a88a-404b-a593-7954d235b44e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9f8fc6-29ed-49b4-ab43-11618bf8b2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fch4_df.fch4.values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66bf41c-5c9b-468a-823c-8e7b5ccadf11",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(divisorGenerator(96654894)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61dbb003-6e27-4704-9af6-22d4611d64fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "fch4_df.fch4.values.reshape(-1, 1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7793bb97-9072-48f4-b485-b65617859ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_elements = 96654894\n",
    "\n",
    "factors = []\n",
    "\n",
    "for i in range(1, int(total_elements ** 0.5) + 1):\n",
    "    if total_elements % i == 0:\n",
    "        factors.append(i)\n",
    "\n",
    "print(factors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5bb612-0a9a-4324-ace0-23b83b5a6b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "total_elements = 96654894\n",
    "\n",
    "DIM1 = 7\n",
    "DIM2 = 100\n",
    "DIM3 = 100\n",
    "\n",
    "product_of_first_three = DIM1 * DIM2 * DIM3\n",
    "\n",
    "target_size = (total_elements // product_of_first_three) * product_of_first_three\n",
    "\n",
    "if total_elements > target_size:\n",
    "    trimmed_array = fch4_df.fch4.values[:target_size]\n",
    "    print(f\"Trimmed array size: {trimmed_array.size}\")\n",
    "else:\n",
    "    trimmed_array = fch4_df.fch4.values\n",
    "    print(f\"No trimming needed\")\n",
    "\n",
    "print(f\"Original size: {total_elements}, Target size: {target_size}\")\n",
    "\n",
    "reshaped_array = trimmed_array.reshape(DIM1, DIM2, DIM3, trimmed_array.size // (DIM1 * DIM2 * DIM3), 1)\n",
    "\n",
    "print(f\"Reshaped array shape: {reshaped_array.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3f9c67-575d-42f4-a8b3-8b5a301fc299",
   "metadata": {},
   "outputs": [],
   "source": [
    "reshaped_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d723bba1-7ba0-4d3b-b246-eace7c7fd8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "# Save the reshaped array as an HDF5 file\n",
    "with h5py.File('fch4_df_nonan_norm_5dtensor_trimmed_2.h5', 'w') as f:\n",
    "    f.create_dataset('fch4_5dtensor', data=reshaped_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0596b3b-85b4-47de-87bb-c209986d2d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "fch4 = reshaped_array; del fch4_df, reshaped_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8581456-4aa9-4d66-9899-ca322126de61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fed8b6d-1fe6-4985-8ba8-de8372e101f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c27ecc1-0b79-4393-b5ac-d52e3fb72329",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# # To load it back:\n",
    "# with h5py.File('alt_df_nonan_norm_5dtensor_padded.h5', 'r') as f:\n",
    "#     loaded_array = f['reshaped_array'][:]\n",
    "# print(f\"Loaded array shape: {loaded_array.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a459bc99-9a32-4702-88b2-3153223dfe17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To load it back:\n",
    "with h5py.File('alt_df_nonan_norm_5dtensor_trimmed.h5', 'r') as f:\n",
    "    alt = f['reshaped_array'][:]\n",
    "print(f\"Loaded array shape: {alt.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeafc47c-ecd7-462e-bf24-3b4ceada18de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To load it back:\n",
    "with h5py.File('fc_df_nonan_norm_5dtensor_trimmed.h5', 'r') as f:\n",
    "    fc = f['reshaped_array'][:]\n",
    "print(f\"Loaded array shape: {fc.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962bf9a5-8407-49a8-a30b-9711f8c9787e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To load it back:\n",
    "with h5py.File('fch4_df_nonan_norm_5dtensor_trimmed.h5', 'r') as f:\n",
    "    fch4 = f['reshaped_array'][:]\n",
    "print(f\"Loaded array shape: {fch4.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd79b281-b521-40fc-8772-94697293817f",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a0c6fc-0e1a-4f28-b5c3-1afb26ab9a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "fc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b4b2ceb-ea36-43ce-99f5-eed8e99bc45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fch4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fafa3f7-e7dc-4c17-b6b0-68d2c5e82c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If my tensors are shaped like the following:\n",
    "# alt.shape\n",
    "# (325, 100, 100, 973, 1)\n",
    "# fc.shape\n",
    "# (42, 100, 100, 230, 1)\n",
    "# fch4.shape\n",
    "# (42, 100, 100, 230, 1)\n",
    "\n",
    "# How can we reshape them so that they are identical to the following tensor shape formats?\n",
    "# alt = (X, Y, 100, 100, 1)\n",
    "# fc = (X, Y, 1)\n",
    "# fch4 = (X, Y, 1)\n",
    "# target = (X, 1)\n",
    "# whereby X should be the same number and Y should be the same number...?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a45d99-d0eb-42c8-a915-8d15af8d6303",
   "metadata": {},
   "outputs": [],
   "source": [
    "If data_format='channels_last': 5D tensor with shape: (samples, time, rows, cols, channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868132b8-82da-43c6-8ebf-cfd499a2ca17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "396337f0-be48-470a-90fe-fd9f4c35b79d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ba8230-121c-47c1-a3d7-565eeed7c627",
   "metadata": {},
   "source": [
    "**Remember to reverse normalization of alt, fc, and fch4 values in each coincident column after training and validation to confirm values.** </br>\n",
    "**Normalization was conducted with min-max normalization methodology, e.g., alt_df['alt'] = (alt_df['alt'] - alt_df['alt'].min()) / (alt_df['alt'].max() - alt_df['alt'].min())**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a189e45a-327e-415a-8066-f24c8110b99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5dcf24-9d1b-4a7a-8481-a52b744c40f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File('ensemble_tensor.h5', 'r') as f:\n",
    "    alt = f['alt'][:].astype('float32')\n",
    "    fc = f['fc'][:].astype('float32')\n",
    "    fch4 = f['fch4'][:].astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f346839-8440-486c-9fdf-75c6d866f24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_alt = alt[:180]\n",
    "val_alt = alt[180:200]\n",
    "test_alt = alt[200:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b62785-8b77-4cc8-8ab2-1ed9e32dff5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def downsample(data, target_size=(256, 256)):\n",
    "#     shape = data.shape\n",
    "#     reshaped = tf.image.resize(data.reshape(-1, shape[2], shape[3], shape[4]), target_size)\n",
    "#     return reshaped.numpy().reshape(shape[0], shape[1], target_size[0], target_size[1], shape[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191b1337-5a37-495c-9955-236fc9a42b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_alt = downsample(train_alt)\n",
    "# val_alt = downsample(val_alt)\n",
    "# test_alt = downsample(test_alt)\n",
    "# print(f\"New Shape: {train_alt.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a10aab-5c97-43bf-a68e-ee587567c5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, models\n",
    "\n",
    "def spatial_model(input_shape):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    x = layers.Conv2D(filters=32, kernel_size=(3, 3), activation='relu', padding='same')(inputs)\n",
    "    x = layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = layers.Flatten()(x)\n",
    "    return models.Model(inputs, x)\n",
    "\n",
    "def temporal_model(input_shape):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    x = layers.LSTM(units=64, activation='relu', return_sequences=False)(inputs)\n",
    "    return models.Model(inputs, x)\n",
    "\n",
    "def build_combined_model(spatial_shape, temporal_steps):\n",
    "    spatial_input = layers.Input(shape=spatial_shape)\n",
    "    temporal_input = layers.Input(shape=(temporal_steps, spatial_shape[0]))\n",
    "\n",
    "    spatial_features = layers.TimeDistributed(spatial_model(spatial_shape))(spatial_input)\n",
    "    temporal_output = temporal_model((temporal_steps, spatial_features.shape[-1]))(spatial_features)\n",
    "\n",
    "    outputs = layers.Dense(units=1, activation='linear')(temporal_output)\n",
    "    model = models.Model(inputs=[spatial_input, temporal_input], outputs=outputs)\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3), \n",
    "                  loss='mse', \n",
    "                  metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "# spatial_shape = (1092, 1092, 1)\n",
    "# temporal_steps = 30\n",
    "\n",
    "# model = build_combined_model(spatial_shape, temporal_steps)\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ad915a-afe2-4b8c-8a82-7f4793d98a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "spatial_model((8,8,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4776c4-d0af-4bc7-8bc3-d98e6ba56871",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a837da-322a-42b8-9e63-52a7177a2951",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b02db1-dac0-4193-a756-c58db8a64c45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7b8faf-8c29-4f80-843e-16b65c287790",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(model, to_file='model_architecture.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd52e395-af8d-40cf-adf4-53c464c46717",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(data, batch_size=4):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((data[:, :-1], data[:, -1]))\n",
    "    dataset = dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    return dataset\n",
    "\n",
    "train_dataset = create_dataset(train_alt)\n",
    "val_dataset = create_dataset(val_alt)\n",
    "test_dataset = create_dataset(test_alt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6919f524-1ebb-4b39-a09a-7bc0884f02ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "checkpoint_cb = callbacks.ModelCheckpoint('best_model.h5', save_best_only=True)\n",
    "\n",
    "history = model.fit(train_dataset, \n",
    "                    validation_data=val_dataset, \n",
    "                    epochs=50, \n",
    "                    callbacks=[early_stopping, checkpoint_cb])\n",
    "\n",
    "# Evaluate the model\n",
    "results = model.evaluate(test_dataset)\n",
    "print(f\"Test Loss: {results[0]}, Test MAE: {results[1]}\")\n",
    "\n",
    "# Save the final model\n",
    "model.save('final_spatiotemporal_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7bce0b-9fec-4bc3-89fc-29009e7cfea9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f736289-6c01-4eac-af83-34e6ef2a0578",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661e653d-5845-4955-9448-c826a4ea4ca9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301207fa-948a-47d3-b181-5ee665be814b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6077e3e-bb4d-456e-a208-6eff8a1807a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d23d91f-9746-42fd-a2e7-a006182bdbec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    model = models.Sequential()\n",
    "    \n",
    "    # Conv3D layers for spatiotemporal feature extraction\n",
    "    model.add(layers.Conv3D(\n",
    "        #filters=hp.Int('conv3d_filters', min_value=16, max_value=64, step=16),\n",
    "        filters=64,\n",
    "        kernel_size=(3, 3, 3),\n",
    "        activation='relu',\n",
    "        padding='same',\n",
    "        input_shape=(30, 1092, 1092, 1)))\n",
    "    model.add(layers.MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "    # ConvLSTM2D for temporal-spatial dependencies\n",
    "    model.add(layers.ConvLSTM2D(\n",
    "        #filters=hp.Int('convlstm_filters', min_value=16, max_value=64, step=16),\n",
    "        filters=64,\n",
    "        kernel_size=(3, 3),\n",
    "        activation='relu',\n",
    "        padding='same',\n",
    "        return_sequences=False))\n",
    "\n",
    "    # Fully connected layers for prediction\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(\n",
    "        #units=hp.Int('dense_units', min_value=32, max_value=128, step=32),\n",
    "        units=128,\n",
    "        activation='relu'))\n",
    "    model.add(layers.Dense(1, activation='linear'))\n",
    "\n",
    "    model.compile(\n",
    "        #optimizer=tf.keras.optimizers.Adam(learning_rate=hp.Choice('learning_rate', [1e-3, 1e-4, 1e-5])),\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "        loss='mse',\n",
    "        metrics=['mae']\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e9f10e-e833-4c00-86bc-7533590d36ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4453bca7-5516-429e-bf31-6a7b78b3d45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63340c5-9520-4b4c-afc6-840c81515fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_cb = callbacks.ModelCheckpoint('best_model.keras', save_best_only=True)\n",
    "earlystop_cb = callbacks.EarlyStopping(patience=10, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec18367-3357-4f3d-a80a-ca602c18cc81",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    train_data, train_data[:, 0],\n",
    "    validation_data=(val_data, val_data[:, 0]),\n",
    "    epochs=100,\n",
    "    callbacks=[checkpoint_cb, earlystop_cb]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468df560-a351-4579-8de4-2e5386f8e7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.evaluate(test_data, test_data[:, 0])\n",
    "print(\"Test Loss, Test MAE:\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c5dd74-568a-46cd-abf0-516647c8e2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('final_alt_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de292175-3529-4f1f-81fc-c0f148a51efa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d667937-7eaf-4188-b5a4-436eb3921227",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45132380-eeba-4021-a654-85b5e3dac166",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0f5d78-74d1-46cc-bd17-ea17cf221f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_alt_verify = train_alt[-10:]\n",
    "valid_alt_verify = val_alt[-10:]\n",
    "test_alt_verify = test_alt[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0b6569-7d34-4e48-975a-714e5285aaf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_tuner_with_progress(tuner, train_data, val_data):\n",
    "    progress_bar = tqdm(total=tuner.oracle.max_trials, desc=\"Tuning Progress\")\n",
    "    class TQDMCallback(callbacks.Callback):\n",
    "        def on_trial_begin(self, trial, logs=None):\n",
    "            progress_bar.set_description(f\"Running Trial {trial.trial_id}\")\n",
    "\n",
    "        def on_trial_end(self, trial, logs=None):\n",
    "            progress_bar.update(1)\n",
    "\n",
    "    tuner.search(\n",
    "        train_alt_verify, train_alt_verify[:, 0],\n",
    "        validation_data=(valid_alt_verify, valid_alt_verify[:, 0]),\n",
    "        epochs=50,\n",
    "        callbacks=[TQDMCallback()]\n",
    "    )\n",
    "    progress_bar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718f5a34-1dbf-4e07-af48-b8d6b52562d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tuner_with_progress():\n",
    "    tuner_creation_desc = \"Creating BayesianOptimization Tuner\"\n",
    "    with tqdm(total=1, desc=tuner_creation_desc) as pbar:\n",
    "        tuner = BayesianOptimization(\n",
    "            build_model,\n",
    "            objective='val_loss',\n",
    "            max_trials=2,\n",
    "            directory='tuner_logs',\n",
    "            project_name='alt_model_tuning',\n",
    "            executions_per_trial=1,\n",
    "            overwrite=True\n",
    "        )\n",
    "        pbar.update(1)\n",
    "    return tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57fd022-d92f-4afa-ac50-4d768eba8812",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_parallel_trials = 2\n",
    "tuner = create_tuner_with_progress()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9e9a94-3f6c-4c4c-bff2-02ce0f5d0058",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# #OR TRY:\n",
    "# tuner = BayesianOptimization(\n",
    "#     build_model,\n",
    "#     objective='val_loss',\n",
    "#     max_trials=10,\n",
    "#     directory='tuner_logs',\n",
    "#     project_name='alt_model_tuning',\n",
    "#     executions_per_trial=1,\n",
    "#     overwrite=True\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131de726-6984-4a69-8603-fea3f35ffc8d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# #OR TRY:\n",
    "# from kerastuner.tuners import RandomSearch\n",
    "\n",
    "# def create_tuner_with_progress():\n",
    "#     tuner_creation_desc = \"Creating Random Search Tuner\"\n",
    "#     with tqdm(total=1, desc=tuner_creation_desc) as pbar:\n",
    "#         tuner = RandomSearch(\n",
    "#             build_model,\n",
    "#             objective='val_loss',\n",
    "#             max_trials=2,\n",
    "#             directory='tuner_logs',\n",
    "#             project_name='alt_model_tuning',\n",
    "#             overwrite=True\n",
    "#         )\n",
    "#         pbar.update(1)\n",
    "#     return tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3860fdc0-e4c5-4076-a187-0a2fa6f63e83",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# num_parallel_trials = 2\n",
    "# tuner = create_tuner_with_progress()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf9d854-7a1d-485d-818d-d3eb171f2b40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8016d00-4ebf-496f-8a3a-bae44538e183",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945395c6-6c6f-4ceb-8de6-2f93a777a916",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ddab9f3-101a-48ee-9f69-954cd9e11113",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_cb = callbacks.ModelCheckpoint('best_model.keras', save_best_only=True)\n",
    "earlystop_cb = callbacks.EarlyStopping(patience=10, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79340a0f-6b8a-414b-9c42-4c1256fe97f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_tuner_with_progress(tuner, train_data, val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8820281d-caf7-4ec8-9fba-89b72547482a",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hps = tuner.get_best_hyperparameters(1)[0]\n",
    "model = tuner.hypermodel.build(best_hps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f13972-98cd-47da-a7ce-42ca76cf0c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "build_model([64, 64, 128, 1e-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d6a26d3-a056-49de-b0f7-9ecb637a1f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    train_data, train_data[:, 0],\n",
    "    validation_data=(val_data, val_data[:, 0]),\n",
    "    epochs=100,\n",
    "    callbacks=[checkpoint_cb, earlystop_cb]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660ca575-2218-42ff-b0bf-d27c0ca5aa0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.evaluate(test_data, test_data[:, 0])\n",
    "print(\"Test Loss, Test MAE:\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4237b3-7179-4aec-843f-941d4e8929e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('final_alt_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0566b8ab-aae7-4dd3-b2c4-c029557ae061",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# from tensorflow.keras import layers, models, callbacks\n",
    "# from kerastuner.tuners import BayesianOptimization\n",
    "\n",
    "# # Define modular ConvLSTM networks\n",
    "# def build_alt_model(input_shape):\n",
    "#     \"\"\"Model for active layer thickness (ALT).\"\"\"\n",
    "#     model = models.Sequential([\n",
    "#         layers.ConvLSTM2D(32, (3, 3), activation='relu', return_sequences=True, input_shape=input_shape),\n",
    "#         layers.BatchNormalization(),\n",
    "#         layers.ConvLSTM2D(64, (3, 3), activation='relu', return_sequences=False),\n",
    "#         layers.Flatten(),\n",
    "#         layers.Dense(128, activation='relu'),\n",
    "#         layers.Dropout(0.3),\n",
    "#         layers.Dense(1)  # Final output for ALT predictions\n",
    "#     ])\n",
    "#     return model\n",
    "\n",
    "# def build_fc_model(input_shape):\n",
    "#     \"\"\"Model for carbon dioxide flux (FC).\"\"\"\n",
    "#     model = models.Sequential([\n",
    "#         layers.Conv3D(32, (3, 3, 3), activation='relu', input_shape=input_shape),\n",
    "#         layers.MaxPooling3D((2, 2, 2)),\n",
    "#         layers.Conv3D(64, (3, 3, 3), activation='relu'),\n",
    "#         layers.GlobalAveragePooling3D(),\n",
    "#         layers.Dense(128, activation='relu'),\n",
    "#         layers.Dropout(0.3),\n",
    "#         layers.Dense(1)  # Final output for FC predictions\n",
    "#     ])\n",
    "#     return model\n",
    "\n",
    "# def build_fch4_model(input_shape):\n",
    "#     \"\"\"Model for methane flux (FCH4).\"\"\"\n",
    "#     model = models.Sequential([\n",
    "#         layers.Conv3D(32, (3, 3, 3), activation='relu', input_shape=input_shape),\n",
    "#         layers.MaxPooling3D((2, 2, 2)),\n",
    "#         layers.Conv3D(64, (3, 3, 3), activation='relu'),\n",
    "#         layers.GlobalAveragePooling3D(),\n",
    "#         layers.Dense(128, activation='relu'),\n",
    "#         layers.Dropout(0.3),\n",
    "#         layers.Dense(1)  # Final output for FCH4 predictions\n",
    "#     ])\n",
    "#     return model\n",
    "\n",
    "# # Combine modular networks into an ensemble\n",
    "# def build_ensemble_model(alt_input_shape, fc_input_shape, fch4_input_shape):\n",
    "#     alt_model = build_alt_model(alt_input_shape)\n",
    "#     fc_model = build_fc_model(fc_input_shape)\n",
    "#     fch4_model = build_fch4_model(fch4_input_shape)\n",
    "    \n",
    "#     combined = layers.Concatenate()([alt_model.output, fc_model.output, fch4_model.output])\n",
    "#     x = layers.Dense(128, activation='relu')(combined)\n",
    "#     x = layers.Dropout(0.3)(x)\n",
    "#     output = layers.Dense(1)(x)  # Final combined prediction\n",
    "\n",
    "#     model = models.Model(inputs=[alt_model.input, fc_model.input, fch4_model.input], outputs=output)\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208a0b2c-07a6-4b27-b15a-e6be17675e10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775e400e-feb2-4cc3-a84d-67b440cabf36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa1c55c-dc3d-42aa-8f46-81115ca5179b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File('fc_df_nonan_norm_5dtensor.h5', 'r') as f:\n",
    "    fc = f['fc_5dtensor'][:]\n",
    "print(f\"Loaded fc tensor shape: {fc.shape}\")\n",
    "\n",
    "with h5py.File('fch4_df_nonan_norm_5dtensor.h5', 'r') as f:\n",
    "    fch4 = f['fch4_5dtensor'][:]\n",
    "print(f\"Loaded fch4 tensor shape: {fch4.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572cd57b-fd34-4dc0-adae-a36e020ab253",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3dede61-adec-40ed-9f2c-a7480c8d38ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a1ac0c-99e7-40d4-b2f3-6c7cd2c3d910",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9dd93a-553c-4b71-ac68-ccfc53d2e9c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf1d91a-a81d-4f9b-b9f7-48f7f435cdfd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df2e33a-b109-45a6-955d-793777c34638",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc77ee55-e1b3-42e3-8968-637107161b10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e998464-95e8-49c9-b56a-9418da5a3d18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a43468-d5cb-4da2-b494-f0ec36e96290",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebde40e2-9cb9-498b-8800-62258a649f8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21bcbcbc-554f-440d-a435-994075cd38ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "88376e76-5f56-45d4-b463-3cf1ed596b60",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Dataframes by feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4ed795-9478-4dac-a841-348aec7f1ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Volumes/JPL/alt/alt.parquet','rb') as f:\n",
    "    alt=pd.read_parquet(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3465f33a-08c9-4f88-a03e-3b5f6b5f4933",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Volumes/JPL/fch4/fch4.parquet','rb') as f:\n",
    "    ch4=pd.read_parquet(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53dbc840-c25a-4719-b537-8020f50124e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Volumes/JPL/fc/fc.parquet','rb') as f:\n",
    "    co2=pd.read_parquet(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ecd9d20-77ad-473b-9751-4e292d2b113b",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.merge(alt, ch4, on=['datetime', 'lat', 'lon'], how='outer')\n",
    "del alt, ch4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce89338-a177-4629-9b71-ae64036f9fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.merge(merged_df, co2, on=['datetime', 'lat', 'lon'], how='outer')\n",
    "del co2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89cd6db9-f655-41e1-9240-71b767aa2d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.columns = ['datetime', 'lat', 'lon', 'alt', 'fch4', 'fc']\n",
    "merged_df = merged_df.groupby(['datetime', 'lat', 'lon']).mean().reset_index()\n",
    "merged_df = merged_df.groupby(['datetime', 'lat', 'lon']).sem().reset_index()\n",
    "\n",
    "# Inspect the result\n",
    "print(merged_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9a3c2b-fbfb-4bca-b5f5-7f60191a754a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#alt.groupby(['datetime','lat','lon'])['alt'].mean().plot();\n",
    "#ch4.groupby(['datetime','lat','lon'])['fch4'].mean().plot();\n",
    "#co2.groupby(['datetime','lat','lon'])['fc'].mean().plot();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4addfaf4-ac1b-465f-ad77-52f6510b8bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,6), dpi=200)\n",
    "ax.set_title('Spatial aggregation of Active Layer Thickness (ALT), '+r'$\\mathregular{1 km^{2}}$', fontsize=12)\n",
    "ax.plot(alt.groupby(['lat','lon'])['alt'].mean().values.tolist(), label='Monthly mean, ALT')\n",
    "ax.set_ylabel(r'$\\mathregular{m}$', fontsize=12, labelpad=10)\n",
    "ax.grid(True)\n",
    "ax.legend(loc='best', fontsize=10)\n",
    "ax.ticklabel_format(useOffset=False, style='sci')\n",
    "ax.set_xlabel('Years elapsed, 1800-2100', fontsize=12, labelpad=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5697034-2f8b-4eec-978c-e482d5946a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,6), dpi=200)\n",
    "ax.set_title('Spatial aggregation of Methane '+r'($\\mathregular{CH}_{4}$) flux, 1 $\\mathregular{km^{2}}$', fontsize=12)\n",
    "ax.plot(ch4.groupby(['lat','lon'])['fch4'].mean().values.tolist(), label=\"Monthly mean, \"+r'$\\mathregular{CH_4} {\\phi}$',color='magenta')\n",
    "ax.set_ylabel(r'$\\mathregular{nmol CH_4 mol^{-1} km^{-2} month^{-1}}$', fontsize=12, labelpad=10)\n",
    "ax.grid(True)\n",
    "ax.legend(loc='best', fontsize=10)\n",
    "ax.ticklabel_format(useOffset=False, style='sci')\n",
    "ax.set_xlabel('Years elapsed, 1996-2022', fontsize=12, labelpad=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "558aa2a2-44e9-48e7-9de5-a948d8e3751f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,6), dpi=200)\n",
    "ax.set_title('Spatial aggregation of Carbon Dioxide '+r'($\\mathregular{CO}_{2}$) flux, 1 $\\mathregular{km^{2}}$', fontsize=12)\n",
    "ax.plot(co2.groupby(['lat','lon'])['fc'].mean().values.tolist(), label=\"Monthy mean, \"+r'$\\mathregular{CO_2} {\\phi}$',color='springgreen')\n",
    "ax.set_ylabel(r'$\\mathregular{umol CO_2 mol^{-1} km^{-2} month^{-1}}$', fontsize=12, labelpad=10)\n",
    "ax.grid(True)\n",
    "ax.legend(loc='best', fontsize=10)\n",
    "ax.ticklabel_format(useOffset=False, style='sci')\n",
    "ax.set_xlabel('Years elapsed, 1996-2022', fontsize=12, labelpad=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1052b5-300d-4dc2-a540-fd169cbe3ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ch4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d569a47a-c490-451a-ac0b-76523ae223e7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### ALT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86189a6b-3b54-4076-8368-e9bd178eddf4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caaad6d6-354a-4b48-9291-a24db7369721",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df=df.replace(0,np.nan).dropna()\n",
    "alt=alt[alt.alt!=0]\n",
    "alt['datetime']=pd.to_datetime(alt['datetime'])\n",
    "alt=alt.sort_values(by='datetime',ascending=True)\n",
    "alt=alt.reset_index(drop=True)\n",
    "alt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580c35ac-ff3a-45a7-ae9d-0bee8852d4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#alt.to_parquet('/Volumes/JPL/alt.parquet',engine='pyarrow',compression='snappy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10c77b2-9eb7-4cb1-9b5f-1dd9292b168c",
   "metadata": {},
   "source": [
    "#### Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a946b99a-d9b1-48b8-b31f-81d3097366c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'datetime' is of type datetime64 in the dataframe\n",
    "train = alt[(alt['datetime'] >= '1800-01-01') & (alt['datetime'] <= '2018-12-31')]\n",
    "valid = alt[(alt['datetime'] >= '2019-01-01') & (alt['datetime'] <= '2021-12-31')]\n",
    "test = alt[(alt['datetime'] >= '2022-01-01') & (alt['datetime'] <= '2100-12-31')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825c1e28-ba09-4244-959d-add6f7b96b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# alt[(alt.datetime >='2017-01-01') & (alt.datetime <= '2022-12-31')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f8524e-5055-48eb-8a76-b33b5896f506",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_resampled = alt.set_index('datetime').resample('M').mean().reset_index()  # Resample to monthly mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ed0b8d-c3d8-413b-921c-aec32d76c291",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train = df_resampled[(df_resampled['datetime'] >= '1800-01-01') & (df_resampled['datetime'] <= '1980-12-31')]\n",
    "# valid = df_resampled[(df_resampled['datetime'] >= '1981-01-01') & (df_resampled['datetime'] <= '2020-12-31')]\n",
    "# test = df_resampled[(df_resampled['datetime'] >= '2021-01-01') & (df_resampled['datetime'] <= '2100-12-31')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce162c2-9357-466c-8f47-1676ff8768e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train\n",
    "#plt.plot(train.datetime, train.alt);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1daa5458-4b37-495c-be28-ec861cc45600",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid\n",
    "#plt.plot(valid.datetime, valid.alt);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e13fe57e-8cc6-4748-b733-a168fffcd6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "test\n",
    "#plt.plot(test.datetime, test.alt);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f96caa-c1df-472c-be93-ee29fc1a9f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#training\n",
    "alt[(alt['datetime'] >= '1800-01-01') & (alt['datetime'] <= '2018-12-31')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab8fbd8-dc9f-462e-b672-be3d642fbb6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "902418/1167089"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a826f9-1e5d-4c54-bc8d-aa12379121b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = '1800-01-01'\n",
    "end_date = '2018-12-31'\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(alt['datetime'], alt['alt'], linestyle='-', marker='o', markersize=1, color='blue')\n",
    "\n",
    "# Set the x-axis limit to the specified date range\n",
    "plt.xlim(pd.to_datetime(start_date), pd.to_datetime(end_date))\n",
    "\n",
    "plt.xlabel('Datetime')\n",
    "plt.ylabel('ALT')\n",
    "plt.title('ALT over Time')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353644fe-d05e-47dd-86b0-d07afb9afbaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#validation\n",
    "alt[(alt['datetime'] >= '2019-01-01') & (alt['datetime'] < '2021-12-31')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a761fe-3a8e-4c9a-b55e-e367782e541a",
   "metadata": {},
   "outputs": [],
   "source": [
    "136275/1167089"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405c20ff-adba-443d-ae2d-4d248689a9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = '2019-01-01'\n",
    "end_date = '2021-12-31'\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(alt['datetime'], alt['alt'], linestyle='-', marker='o', markersize=1, color='blue')\n",
    "\n",
    "# Set the x-axis limit to the specified date range\n",
    "plt.xlim(pd.to_datetime(start_date), pd.to_datetime(end_date))\n",
    "\n",
    "plt.xlabel('Datetime')\n",
    "plt.ylabel('ALT')\n",
    "plt.title('ALT over Time')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1822b3-0b26-44aa-85d8-43e246c67204",
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing\n",
    "alt[(alt['datetime'] >= '2022-01-01')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72831cd3-fef3-4574-97ae-024f6a2d0fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "128396/1167089"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290245a7-17c6-4426-b4fb-b222e85a5d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = '2021-01-01'\n",
    "end_date = '2100-12-31'\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(alt['datetime'], alt['alt'], linestyle='-', marker='o', markersize=1, color='blue')\n",
    "\n",
    "# Set the x-axis limit to the specified date range\n",
    "plt.xlim(pd.to_datetime(start_date), pd.to_datetime(end_date))\n",
    "\n",
    "plt.xlabel('Datetime')\n",
    "plt.ylabel('ALT')\n",
    "plt.title('ALT over Time')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae7210b-6451-496c-9175-c8e1923bdd55",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_train=alt[(alt['datetime'] >= '1800-01-01') & (alt['datetime'] <= '2018-12-31')]\n",
    "alt_valid=alt[(alt['datetime'] >= '2019-01-01') & (alt['datetime'] < '2021-12-31')]\n",
    "alt_test=alt[(alt['datetime'] >= '2022-01-01')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6ef31c-b2e1-4fde-87bf-c46445721673",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_train=alt_train[alt_train.alt!=0]\n",
    "alt_train['datetime']=pd.to_datetime(alt_train['datetime'])\n",
    "alt_train=alt_train.sort_values(by='datetime',ascending=True)\n",
    "alt_train=alt_train.reset_index(drop=True)\n",
    "alt_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45a1780-912f-4a9f-8979-2820986c2c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_train.to_parquet('/Volumes/JPL/alt_train.parquet',engine='pyarrow',compression='snappy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33bae4db-52cf-4fba-afd9-403f0d9f5fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_valid=alt_valid[alt_valid.alt!=0]\n",
    "alt_valid['datetime']=pd.to_datetime(alt_valid['datetime'])\n",
    "alt_valid=alt_valid.sort_values(by='datetime',ascending=True)\n",
    "alt_valid=alt_valid.reset_index(drop=True)\n",
    "alt_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcad821f-56c0-4d19-a57e-30b0044a3a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_valid.to_parquet('/Volumes/JPL/alt_valid.parquet',engine='pyarrow',compression='snappy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f71c6f-1616-4439-aff3-fce63711de77",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_test=alt_test[alt_test.alt!=0]\n",
    "alt_test['datetime']=pd.to_datetime(alt_test['datetime'])\n",
    "alt_test=alt_test.sort_values(by='datetime',ascending=True)\n",
    "alt_test=alt_test.reset_index(drop=True)\n",
    "alt_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791aab91-d73e-4027-9ba0-b1aa7e43b234",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_test.to_parquet('/Volumes/JPL/alt_test.parquet',engine='pyarrow',compression='snappy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b2198c-4fbe-416a-9d38-05ed379b8679",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "036dc2a7-567d-42e5-8cd6-a75dacc15333",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### CH4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6c27d1-3556-4b41-a7bc-df77db408e89",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbba2bef-c707-434e-8b2b-242061066705",
   "metadata": {},
   "outputs": [],
   "source": [
    "ch4=ch4[ch4.fch4!=0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4f2731-6eff-4072-a3fb-2efcf203c277",
   "metadata": {},
   "outputs": [],
   "source": [
    "ch4['datetime']=pd.to_datetime(ch4['datetime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba875bb-f85c-4af6-8055-071b0dca04e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ch4=ch4.sort_values(by='datetime',ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786af6e7-b260-42ac-817e-ead55b533d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "ch4=ch4.reset_index(drop=True)\n",
    "ch4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf80b1e-bbb3-4e27-bfbf-2b26db825517",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ch4.to_parquet('/Volumes/JPL/fch4.parquet',engine='pyarrow',compression='snappy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0308c7d8-1317-49b1-b451-5140fe60f5ef",
   "metadata": {},
   "source": [
    "#### Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d701cdb-a455-4434-86ff-5c4f50d049e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#training\n",
    "ch4[ch4['datetime'] < '2012-01-01']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26f8df1-acff-4b24-a4dd-0000497dc061",
   "metadata": {},
   "outputs": [],
   "source": [
    "1472576874/1972395203"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd24c20f-f588-41af-8610-a05c5d668937",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = '1996-06-01'\n",
    "end_date = '2011-12-01'\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(ch4['datetime'], ch4['fch4'], linestyle='-', marker='o', markersize=1, color='blue')\n",
    "\n",
    "# Set the x-axis limit to the specified date range\n",
    "plt.xlim(pd.to_datetime(start_date), pd.to_datetime(end_date))\n",
    "\n",
    "plt.xlabel('Datetime')\n",
    "plt.ylabel('FCH4')\n",
    "plt.title('FCH4 over Time')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba55b11-ac3b-4184-86b6-6b40e4f5efec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#validation\n",
    "ch4[(ch4['datetime'] >= '2012-01-01') & (ch4['datetime'] < '2013-10-01')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d394f4f-940c-44ce-8839-f832b5546487",
   "metadata": {},
   "outputs": [],
   "source": [
    "281618588/1972395203"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaccf9c7-d7a2-48c7-8df0-7243ecc03384",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = '2012-01-01'\n",
    "end_date = '2013-09-01'\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(ch4['datetime'], ch4['fch4'], linestyle='-', marker='o', markersize=1, color='blue')\n",
    "\n",
    "# Set the x-axis limit to the specified date range\n",
    "plt.xlim(pd.to_datetime(start_date), pd.to_datetime(end_date))\n",
    "\n",
    "plt.xlabel('Datetime')\n",
    "plt.ylabel('FCH4')\n",
    "plt.title('FCH4 over Time')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb0247ef-8e68-4f5a-8537-feb415c2c07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing\n",
    "ch4[(ch4['datetime'] >= '2013-10-01')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388fd51e-e7e7-4201-9046-4a0d023265a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "218199741/1972395203"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003342d0-a198-498f-af57-63c66a877b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = '2013-10-01'\n",
    "end_date = '2022-10-01'\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(ch4['datetime'], ch4['fch4'], linestyle='-', marker='o', markersize=1, color='blue')\n",
    "\n",
    "# Set the x-axis limit to the specified date range\n",
    "plt.xlim(pd.to_datetime(start_date), pd.to_datetime(end_date))\n",
    "\n",
    "plt.xlabel('Datetime')\n",
    "plt.ylabel('FCH4')\n",
    "plt.title('FCH4 over Time')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a80cec-d32c-4b19-afea-0c0a3258f6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "ch4_train=ch4[ch4['datetime'] < '2012-01-01']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e064e3f-8825-4b19-a756-24a5b91c4163",
   "metadata": {},
   "outputs": [],
   "source": [
    "ch4_train=ch4_train[ch4_train.fch4!=0]\n",
    "ch4_train['datetime']=pd.to_datetime(ch4_train['datetime'])\n",
    "ch4_train=ch4_train.sort_values(by='datetime',ascending=True)\n",
    "ch4_train=ch4_train.reset_index(drop=True)\n",
    "ch4_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2624bd7-3507-46c7-82f2-f04ee437d1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ch4_train.to_parquet('/Volumes/JPL/fch4_train.parquet',engine='pyarrow',compression='snappy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b827d9-c9de-4124-816e-66fe4a05941c",
   "metadata": {},
   "outputs": [],
   "source": [
    "del ch4_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b108b8-2cfb-4aa8-a189-95b58c58395e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ch4_valid=ch4[(ch4['datetime'] >= '2012-01-01') & (ch4['datetime'] < '2013-10-01')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5960e4-4c70-47de-8dc1-84c79f728965",
   "metadata": {},
   "outputs": [],
   "source": [
    "ch4_valid=ch4_valid[ch4_valid.fch4!=0]\n",
    "ch4_valid['datetime']=pd.to_datetime(ch4_valid['datetime'])\n",
    "ch4_valid=ch4_valid.sort_values(by='datetime',ascending=True)\n",
    "ch4_valid=ch4_valid.reset_index(drop=True)\n",
    "ch4_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4387f5-8e39-4dba-bf60-23f4233f237e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ch4_valid.to_parquet('/Volumes/JPL/fch4_valid.parquet',engine='pyarrow',compression='snappy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc82fd6-c223-439c-98f4-fa1ade57de8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "del ch4_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0868a6cd-4fe5-4203-b73b-bc35f2ca2eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "ch4_test=ch4[(ch4['datetime'] >= '2013-10-01')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3a2b7a-9f30-4959-b25c-4eb53342bee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ch4_test=ch4_test[ch4_test.fch4!=0]\n",
    "ch4_test['datetime']=pd.to_datetime(ch4_test['datetime'])\n",
    "ch4_test=ch4_test.sort_values(by='datetime',ascending=True)\n",
    "ch4_test=ch4_test.reset_index(drop=True)\n",
    "ch4_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0acabb86-8523-4237-a5d7-a8d39c53b588",
   "metadata": {},
   "outputs": [],
   "source": [
    "ch4_test.to_parquet('/Volumes/JPL/fch4_test.parquet',engine='pyarrow',compression='snappy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0aa330-356c-4eed-b474-2fe3d6a3e1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "del ch4_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195fe5b6-c4c9-4566-b533-30f828399b40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "66fd3a33-fa17-4662-8c9c-05ceecb9d927",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### CO2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b3019b-c03e-465b-8344-6c418509b888",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0f9700-5cbc-4bb2-a2e7-fc13e2c6ccab",
   "metadata": {},
   "outputs": [],
   "source": [
    "co2=co2[co2['datetime'] < '2023-01-01']\n",
    "co2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1e1380-bec0-45e2-8ffa-0deac921e38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "co2=co2[co2.fc!=0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ebf2e1a-d491-4a3f-a26b-65fb257811d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "co2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f795151-300e-4008-b66b-b054a6c759f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "co2['datetime']=pd.to_datetime(co2['datetime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8e7eb6-fe03-4355-bf20-e7a5b5821605",
   "metadata": {},
   "outputs": [],
   "source": [
    "co2.to_parquet('/Volumes/JPL/fc.parquet',engine='pyarrow',compression='snappy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b4f1b4-8ce6-4868-8296-682738f02e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "with open('/Volumes/JPL/fc.parquet','rb') as f:\n",
    "    co2=pd.read_parquet(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de76b595-3e16-4c14-b0c0-90349d543691",
   "metadata": {},
   "outputs": [],
   "source": [
    "co2=co2.sort_values(by='datetime',ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003ba6dd-7ad8-4cd5-95d5-4b20db5078ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "co2.to_parquet('/Volumes/JPL/fc2.parquet',engine='pyarrow',compression='snappy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d58870a-dd78-4061-81b3-f81ef3cddb0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Volumes/JPL/fc2.parquet','rb') as f:\n",
    "    co2=pd.read_parquet(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381fcd44-c11f-4fe5-b110-d773ee75f3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "co2=co2.reset_index(drop=True)\n",
    "co2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d457a8-7635-412f-9ea7-9d6f6d1f1934",
   "metadata": {},
   "outputs": [],
   "source": [
    "co2.to_parquet('/Volumes/JPL/fc3.parquet',engine='pyarrow',compression='snappy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591cb4a7-2918-4b43-9e7c-74ef97f83f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Volumes/JPL/newdf_fc3.parquet','rb') as f:\n",
    "    co2=pd.read_parquet(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33834baa-3ea0-4499-bd52-79423f5ea3a5",
   "metadata": {},
   "source": [
    "#### Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bdc5aee-5f26-4ff9-bc87-1d6e077ac86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "co2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c487b9-e60e-4cdf-b0df-244c6fc8ec79",
   "metadata": {},
   "outputs": [],
   "source": [
    "#training\n",
    "co2[co2['datetime'] < '2012-01-01']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c91bc82-6d7f-4e7d-90dd-c72d55c928a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "3147412758/4210946142"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4365b4e7-1f85-4af8-a4bf-ce502fa49794",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = '1996-06-01'\n",
    "end_date = '2012-01-01'\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(co2['datetime'], co2['fch4'], linestyle='-', marker='o', markersize=1, color='blue')\n",
    "\n",
    "# Set the x-axis limit to the specified date range\n",
    "plt.xlim(pd.to_datetime(start_date), pd.to_datetime(end_date))\n",
    "\n",
    "plt.xlabel('Datetime')\n",
    "plt.ylabel('FC')\n",
    "plt.title('FC over Time')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd7d265-9dcf-4772-996f-cf50a1be2c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#validation\n",
    "co2[(co2['datetime'] >= '2012-01-01') & (co2['datetime'] < '2013-10-01')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6538a98-5fae-4d75-92f6-25f9680bee2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "610055462/4210946142"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e87685-3987-4627-9322-3de71bb17901",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = '2012-01-01'\n",
    "end_date = '2013-10-01'\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(co2['datetime'], co2['fc'], linestyle='-', marker='o', markersize=1, color='blue')\n",
    "\n",
    "# Set the x-axis limit to the specified date range\n",
    "plt.xlim(pd.to_datetime(start_date), pd.to_datetime(end_date))\n",
    "\n",
    "plt.xlabel('Datetime')\n",
    "plt.ylabel('FC')\n",
    "plt.title('FC over Time')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f69b5b-8955-4139-afd1-2d9dbfb70542",
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing\n",
    "co2[(co2['datetime'] >= '2013-10-01')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62968d18-68f7-40b8-9ecc-3625493cc016",
   "metadata": {},
   "outputs": [],
   "source": [
    "453477922/4210946142"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e526ac-efeb-4a25-a98c-fac29042b775",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = '2018-08-01'\n",
    "end_date = '2100-12-01'\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(co2['datetime'], co2['fc'], linestyle='-', marker='o', markersize=1, color='blue')\n",
    "\n",
    "# Set the x-axis limit to the specified date range\n",
    "plt.xlim(pd.to_datetime(start_date), pd.to_datetime(end_date))\n",
    "\n",
    "plt.xlabel('Datetime')\n",
    "plt.ylabel('FC')\n",
    "plt.title('FC over Time')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1e39e0-daa9-4268-992f-2232d07ee8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "co2_train=co2[co2['datetime'] < '2012-01-01']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79131655-f783-4faf-8c77-e120e93fbb73",
   "metadata": {},
   "outputs": [],
   "source": [
    "co2_train=co2_train[co2_train.fc!=0]\n",
    "co2_train['datetime']=pd.to_datetime(co2_train['datetime'])\n",
    "co2_train=co2_train.sort_values(by='datetime',ascending=True)\n",
    "co2_train=co2_train.reset_index(drop=True)\n",
    "co2_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c855ba-f29c-4e96-bc89-9557e1732f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "co2_train.to_parquet('/Volumes/JPL/fc_train.parquet',engine='pyarrow',compression='snappy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f074bd-941e-4b86-9450-0c612a00ebac",
   "metadata": {},
   "outputs": [],
   "source": [
    "del co2_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc18735-6c19-4fb1-b12f-edb8ba73ff80",
   "metadata": {},
   "outputs": [],
   "source": [
    "co2_valid=co2[(co2['datetime'] >= '2012-01-01') & (co2['datetime'] < '2013-10-01')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02c9ad1-2a56-4a0c-87aa-c47433779050",
   "metadata": {},
   "outputs": [],
   "source": [
    "co2_valid=co2_valid[co2_valid.fc!=0]\n",
    "co2_valid['datetime']=pd.to_datetime(co2_valid['datetime'])\n",
    "co2_valid=co2_valid.sort_values(by='datetime',ascending=True)\n",
    "co2_valid=co2_valid.reset_index(drop=True)\n",
    "co2_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97febf0c-8387-422e-8405-5ad37d7c1012",
   "metadata": {},
   "outputs": [],
   "source": [
    "co2_valid.to_parquet('/Volumes/JPL/fc_valid.parquet',engine='pyarrow',compression='snappy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a77aa6f-928f-4857-8660-1ec388a18b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "del co2_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8247cfc3-b068-4283-87c8-79e6a0089401",
   "metadata": {},
   "outputs": [],
   "source": [
    "co2_test=co2[(co2['datetime'] >= '2013-10-01')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45fff69-00ac-4b6c-9966-dfa9cfa20823",
   "metadata": {},
   "outputs": [],
   "source": [
    "co2_test=co2_test[co2_test.fc!=0]\n",
    "co2_test['datetime']=pd.to_datetime(co2_test['datetime'])\n",
    "co2_test=co2_test.sort_values(by='datetime',ascending=True)\n",
    "co2_test=co2_test.reset_index(drop=True)\n",
    "co2_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2401eb40-11ad-4912-a46b-741d56b1cd20",
   "metadata": {},
   "outputs": [],
   "source": [
    "co2_test.to_parquet('/Volumes/JPL/fc_test.parquet',engine='pyarrow',compression='snappy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee83fb17-4fb5-4a2d-8eba-fd56299a32be",
   "metadata": {},
   "outputs": [],
   "source": [
    "del co2_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eda47a9-48b2-4f24-bb3a-d5a0c71fdcfc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ff6f4df8-8385-4787-9989-c27d5d1862ed",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Standardize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72294a8-1cd6-486b-87d7-a12cffb53055",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# with open('/Volumes/JPL/alt_train.parquet','rb') as f:\n",
    "#     alt_train=pd.read_parquet(f)\n",
    "# with open('/Volumes/JPL/fc_train.parquet','rb') as f:\n",
    "#     fc_train=pd.read_parquet(f)\n",
    "# with open('/Volumes/JPL/fch4_train.parquet','rb') as f:\n",
    "#     fch4_train=pd.read_parquet(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73173d63-0e46-43e2-962d-229f5af27b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Volumes/JPL/alt_train.parquet','rb') as f:\n",
    "    alt_train=pd.read_parquet(f)\n",
    "with open('/Volumes/JPL/alt_valid.parquet','rb') as f:\n",
    "    alt_valid=pd.read_parquet(f)\n",
    "with open('/Volumes/JPL/alt_test.parquet','rb') as f:\n",
    "    alt_test=pd.read_parquet(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efcb149d-f65e-4011-9b76-b7493932e2ae",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# with open('/Volumes/JPL/alt_valid.parquet','rb') as f:\n",
    "#     alt_valid=pd.read_parquet(f)\n",
    "# with open('/Volumes/JPL/fc_valid.parquet','rb') as f:\n",
    "#     fc_valid=pd.read_parquet(f)\n",
    "# with open('/Volumes/JPL/fch4_valid.parquet','rb') as f:\n",
    "#     fch4_valid=pd.read_parquet(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8ee57b-9f4a-4993-aced-65d14b58350a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Volumes/JPL/fc_train.parquet','rb') as f:\n",
    "    fc_train=pd.read_parquet(f)\n",
    "with open('/Volumes/JPL/fc_valid.parquet','rb') as f:\n",
    "    fc_valid=pd.read_parquet(f)\n",
    "with open('/Volumes/JPL/fc_test.parquet','rb') as f:\n",
    "    fc_test=pd.read_parquet(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be6ef13-1d0a-4568-82fa-4924cfc610fe",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# with open('/Volumes/JPL/alt_test.parquet','rb') as f:\n",
    "#     alt_test=pd.read_parquet(f)\n",
    "# with open('/Volumes/JPL/fc_test.parquet','rb') as f:\n",
    "#     fc_test=pd.read_parquet(f)\n",
    "# with open('/Volumes/JPL/fch4_test.parquet','rb') as f:\n",
    "#     fch4_test=pd.read_parquet(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7178b7f-b87b-48f2-9695-a4ede9fa209b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Volumes/JPL/fch4_train.parquet','rb') as f:\n",
    "    fch4_train=pd.read_parquet(f)\n",
    "with open('/Volumes/JPL/fch4_valid.parquet','rb') as f:\n",
    "    fch4_valid=pd.read_parquet(f)\n",
    "with open('/Volumes/JPL/fch4_test.parquet','rb') as f:\n",
    "    fch4_test=pd.read_parquet(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4056586-5551-45bb-9863-cafd7c02b445",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# tf.convert_to_tensor(X.index.values.astype(np.int64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9f228b-3be9-4f2b-9175-aa7efe7b7228",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# def process_dataframe(df, variable_name):\n",
    "#     # Mask out rows where alt is 0 or NaN\n",
    "#     mask = (df['alt'] != 0) & (~df['alt'].isna())\n",
    "#     df_filtered = df[mask]\n",
    "    \n",
    "#     # Ensure no NaN or 0 values in `alt`\n",
    "#     assert df_filtered['alt'].isna().sum() == 0\n",
    "#     assert (df_filtered['alt'] == 0).sum() == 0\n",
    "    \n",
    "#     # Convert to array: datetime, lat, lon, variable_name\n",
    "#     array = df_filtered[['datetime', 'lat', 'lon', variable_name]].values\n",
    "#     return array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a670e3b0-8f5c-4cb9-b445-5c42748a543e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# process_dataframe(alt_train,'datetime').shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98dc807f-e168-4a12-976e-699fd1dac6f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c91b5b2-77be-4312-a6f3-cdfd7be8e468",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0483605d-7d73-4b4c-bffb-be00c6f07330",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb2ae99-901f-461b-8701-d8c1a154b35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_alt = pd.DataFrame({'alt': alt.round(4)}).replace(0, np.nan).dropna()\n",
    "filtered_df = pd.merge(alt, filtered_alt, left_index=True, right_index=True)\n",
    "aggregated_df = filtered_df.groupby(['datetime', 'lat', 'lon']).agg({'alt_x': 'mean'}).reset_index()\n",
    "aggregated_df.rename(columns={'alt_x': 'alt'}, inplace=True)\n",
    "pivot_df = aggregated_df.pivot_table(index='datetime', columns=['lat', 'lon'], values='alt')\n",
    "pivot_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0d34a2-59db-4a50-96dd-228ca1a798a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot_df_numeric = pivot_df.apply(pd.to_numeric, errors='coerce')\n",
    "pivot_df_flat = pivot_df_numeric.copy()\n",
    "pivot_df_flat.columns = ['_'.join(map(str, col)) for col in pivot_df.columns]\n",
    "interpolated_df_flat = pivot_df_flat.interpolate(method='linear', axis=0).interpolate(method='linear', axis=1)\n",
    "interpolated_df_flat = interpolated_df_flat.ffill().bfill()\n",
    "remaining_nans = interpolated_df_flat.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9631c90-da1d-4b63-8756-e0ab970c43f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "multiindex_tuples = [tuple(col.split('_')) for col in interpolated_df_flat.columns]\n",
    "interpolated_df = interpolated_df_flat.copy()\n",
    "interpolated_df.columns = pd.MultiIndex.from_tuples(multiindex_tuples)\n",
    "#print(interpolated_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77a8ddc-db5e-400a-b619-c4b73e12486e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a full index with all combinations of datetime, lat, and lon\n",
    "full_index = pd.MultiIndex.from_product(\n",
    "    [interpolated_df.index.get_level_values('datetime').unique(),\n",
    "     interpolated_df.columns.get_level_values(level=0).unique(),\n",
    "     interpolated_df.columns.get_level_values(level=1).unique()],\n",
    "    names=['datetime', 'lat', 'lon']\n",
    ")\n",
    "\n",
    "# Reindex the DataFrame to include all possible combinations\n",
    "interpolated_df_reindexed = interpolated_df.reindex(full_index, fill_value=np.nan)\n",
    "\n",
    "# Verify the new shape of the DataFrame\n",
    "print(interpolated_df_reindexed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3ce725-0da4-474f-b429-22c34ac6820c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nan_mask = nan_mask.reindex_like(interpolated_df)\n",
    "# assert nan_mask.shape == interpolated_df.shape, \"Shapes of nan_mask and interpolated_df do not match!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec6ca7a-d3a6-455a-84d5-4b5978e275f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Gaussian noise\n",
    "noise_std = 0.01  # Adjust the standard deviation as needed\n",
    "gaussian_noise = np.random.normal(0, noise_std, interpolated_df_reindexed.shape)\n",
    "\n",
    "# Add Gaussian noise only where NaNs were originally present\n",
    "interpolated_df_reindexed += interpolated_df_reindexed.isna() * gaussian_noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff7f03a-0701-4fa1-8d32-feeb77407dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_datetime = len(interpolated_df.index.get_level_values('datetime').unique())\n",
    "print(f\"Number of unique datetime values: {n_datetime}\")\n",
    "n_lat = len(interpolated_df.columns.get_level_values(level=0).unique())\n",
    "n_lon = len(interpolated_df.columns.get_level_values(level=1).unique())\n",
    "print(f\"Number of unique lat values: {n_lat}\")\n",
    "print(f\"Number of unique lon values: {n_lon}\")\n",
    "n_elements = interpolated_df.size\n",
    "print(f\"Total number of elements in the DataFrame: {n_elements}\")\n",
    "expected_size = n_datetime * n_lat * n_lon\n",
    "print(f\"Expected total size: {expected_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d4740d-92c5-4504-95d4-c3901f4a6ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the reindexed DataFrame to a 4D array\n",
    "array_data = interpolated_df_reindexed.values.reshape(\n",
    "    len(interpolated_df_reindexed.index.get_level_values('datetime').unique()),  # Number of unique datetime values\n",
    "    len(interpolated_df_reindexed.columns.get_level_values(level=0).unique()),   # Number of unique latitude values\n",
    "    len(interpolated_df_reindexed.columns.get_level_values(level=1).unique()),   # Number of unique longitude values\n",
    "    1  # Altitude is a single channel\n",
    ")\n",
    "\n",
    "# Verify the shape of the resulting array\n",
    "print(array_data.shape)  # Expected shape should now match (3795, 363, 367, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56d9d3f-f57f-4f06-8192-68f2d63ecbae",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_timesteps = array_data.shape[0]\n",
    "train_size = int(0.7 * n_timesteps)\n",
    "valid_size = int(0.15 * n_timesteps)\n",
    "test_size = n_timesteps - train_size - valid_size\n",
    "\n",
    "X_train = array_data[:train_size]\n",
    "y_train = array_data[:train_size, :, :, 0]\n",
    "X_valid = array_data[train_size:train_size + valid_size]\n",
    "y_valid = array_data[train_size:train_size + valid_size, :, :, 0]\n",
    "X_test = array_data[train_size + valid_size:]\n",
    "y_test = array_data[train_size + valid_size:, :, :, 0]\n",
    "\n",
    "# Convert to TensorFlow datasets\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train)).batch(32)\n",
    "valid_dataset = tf.data.Dataset.from_tensor_slices((X_valid, y_valid)).batch(32)\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ea08a0-d8c3-41fb-b2ce-b3fd2c17b7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_df['datetime'] = pd.to_datetime(aggregated_df['datetime'])\n",
    "aggregated_df['year'] = aggregated_df['datetime'].dt.year\n",
    "aggregated_df['month'] = aggregated_df['datetime'].dt.month\n",
    "\n",
    "# Assuming 'alt' is your original DataFrame with columns ['datetime', 'lat', 'lon', 'alt']\n",
    "\n",
    "# Convert the 'datetime' column to datetime format\n",
    "aggregated_df['datetime'] = pd.to_datetime(aggregated_df['datetime'])\n",
    "\n",
    "# Split datetime into year and month\n",
    "aggregated_df['year'] = aggregated_df['datetime'].dt.year\n",
    "aggregated_df['month'] = aggregated_df['datetime'].dt.month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d827121-21cd-4b6f-8da4-55f59a67d6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_alt = pd.DataFrame({'alt': aggregated_df['alt'].values.round(4)}).replace(0, np.nan).dropna()\n",
    "filtered_df = pd.merge(aggregated_df, filtered_alt, left_index=True, right_index=True)\n",
    "aggregated_df = filtered_df.groupby(['year', 'month', 'lat', 'lon']).agg({'alt_x': 'mean'}).reset_index()\n",
    "aggregated_df.rename(columns={'alt_x': 'alt'}, inplace=True)\n",
    "\n",
    "# Get unique years, months, latitudes, and longitudes\n",
    "unique_years = np.sort(aggregated_df['year'].unique())\n",
    "unique_months = np.arange(1, 13)  # Since months should always be from 1 to 12\n",
    "unique_lats = np.sort(aggregated_df['lat'].unique())\n",
    "unique_lons = np.sort(aggregated_df['lon'].unique())\n",
    "\n",
    "# Initialize the ndarray with zeros\n",
    "ndarray = np.zeros((len(unique_years), len(unique_months), len(unique_lats), len(unique_lons)))\n",
    "\n",
    "# Create mappings from years, months, latitudes, and longitudes to indices\n",
    "year_to_idx = {year: i for i, year in enumerate(unique_years)}\n",
    "month_to_idx = {month: i for i, month in enumerate(unique_months)}\n",
    "lat_to_idx = {lat: i for i, lat in enumerate(unique_lats)}\n",
    "lon_to_idx = {lon: i for i, lon in enumerate(unique_lons)}\n",
    "\n",
    "# Populate the ndarray with alt values\n",
    "for _, row in aggregated_df.iterrows():\n",
    "    year_idx = year_to_idx[row['year']]\n",
    "    month_idx = month_to_idx[row['month']]\n",
    "    lat_idx = lat_to_idx[row['lat']]\n",
    "    lon_idx = lon_to_idx[row['lon']]\n",
    "    ndarray[year_idx, month_idx - 1, lat_idx, lon_idx] = row['alt']  # month - 1 for 0-indexing\n",
    "\n",
    "# Add an additional axis to the ndarray to represent the 'alt' dimension\n",
    "ndarray = ndarray[..., np.newaxis]\n",
    "\n",
    "print(f\"Final ndarray shape: {ndarray.shape}\")  # Should be (years, 12, lats, lons, 1)\n",
    "\n",
    "# # Filter and aggregate the data\n",
    "# filtered_alt = pd.DataFrame({'alt': aggregated_df['alt'].values.round(4)}).replace(0, np.nan).dropna()\n",
    "# filtered_df = pd.merge(aggregated_df, filtered_alt, left_index=True, right_index=True)\n",
    "# aggregated_df = filtered_df.groupby(['year', 'month', 'lat', 'lon']).agg({'alt_x': 'mean'}).reset_index()\n",
    "# aggregated_df.rename(columns={'alt_x': 'alt'}, inplace=True)\n",
    "\n",
    "# # Get unique years, months, latitudes, and longitudes\n",
    "# unique_years = np.sort(aggregated_df['year'].unique())\n",
    "# unique_months = np.arange(1, 13)  # Since months should always be from 1 to 12\n",
    "# unique_lats = np.sort(aggregated_df['lat'].unique())\n",
    "# unique_lons = np.sort(aggregated_df['lon'].unique())\n",
    "\n",
    "# # Initialize the ndarray with zeros\n",
    "# ndarray = np.zeros((len(unique_years), len(unique_months), len(unique_lats), len(unique_lons)))\n",
    "\n",
    "# # Create mappings from years, months, latitudes, and longitudes to indices\n",
    "# year_to_idx = {year: i for i, year in enumerate(unique_years)}\n",
    "# month_to_idx = {month: i for i, month in enumerate(unique_months)}\n",
    "# lat_to_idx = {lat: i for i, lat in enumerate(unique_lats)}\n",
    "# lon_to_idx = {lon: i for i, lon in enumerate(unique_lons)}\n",
    "\n",
    "# # Populate the ndarray with alt values\n",
    "# for _, row in aggregated_df.iterrows():\n",
    "#     year_idx = year_to_idx[row['year']]\n",
    "#     month_idx = month_to_idx[row['month']]\n",
    "#     lat_idx = lat_to_idx[row['lat']]\n",
    "#     lon_idx = lon_to_idx[row['lon']]\n",
    "#     ndarray[year_idx, month_idx - 1, lat_idx, lon_idx] = row['alt']  # month - 1 for 0-indexing\n",
    "\n",
    "# # Add an additional axis to the ndarray to represent the 'alt' dimension\n",
    "# ndarray = ndarray[..., np.newaxis]\n",
    "\n",
    "# print(f\"Final ndarray shape: {ndarray.shape}\")  # Should be (years, 12, lats, lons, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b3dda6-3438-45af-b31d-8579c4300c4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e3d8ee-8b4e-4586-b8d5-c0d83d5c99f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt['lat_bin'] = alt['lat'].round(4)  # Adjust the rounding based on your grid resolution\n",
    "alt['lon_bin'] = alt['lon'].round(4)\n",
    "\n",
    "alt_pivot = alt.pivot_table(index=['datetime'], columns=['lat_bin', 'lon_bin'], values='alt', fill_value=0)\n",
    "\n",
    "sequence_length = 12  # Adjust based on your time series needs\n",
    "X = []\n",
    "for i in range(len(grid) - sequence_length):\n",
    "    X.append(grid[i:i + sequence_length])\n",
    "X = np.array(X)\n",
    "\n",
    "\n",
    "df = alt.groupby(['datetime', 'lat', 'lon']).agg({'alt': 'mean'}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6d22b7-8a11-4705-94c7-0c75d1e14c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.set_index('datetime').resample('M').mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6687c2c6-20d9-455d-a8e4-c6fddfb8e397",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.replace(0, np.nan).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e26d583-0a0d-4981-8624-d90160174736",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[df.set_index('datetime').resample('M').mean().reset_index().alt.replace(0, np.nan).dropna().index.values.tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3af4bb3-f097-488b-a6e4-771334faa677",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c6c8d5-2262-40f3-bcb5-fbdeb4fb15c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc47a2a4-49c8-4d64-8562-8d05e4b87705",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ca05e7-ecee-40ca-bade-824dbf507a88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4bdeb7-73aa-40c6-9856-d50289e5bb10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bcddd60-4c3e-48ad-a1a2-3239ff25f3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#alt.shape, co2.shape\n",
    "ch4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89b3db8-18cc-4065-b7ee-a8aaa268a5cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a309d64-b096-4f44-814e-da8c5f271cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# alt=co2; del co2\n",
    "# alt.columns=['datetime','lat','lon','alt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5657609f-7b58-4b82-a8b6-19e6116e6c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#unc = alt.groupby(['datetime', 'lat', 'lon']).agg({'alt': 'sem'}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46521572-0b57-40d4-a8c0-a786aea975c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df=alt.groupby(['datetime','lat','lon']).agg({'alt': 'mean'}).reset_index()\n",
    "# df.iloc[df.set_index('datetime').resample('M').mean().reset_index().alt.replace(0,np.nan).dropna().index.values.tolist()].sort_values(by='datetime').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864cba7b-aa79-4426-a9d7-4a2a009f6299",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Shape of ALT: (1167089, 4)\n",
    "# datetime\tlat\tlon\talt\n",
    "# 0\t1800-01-01\t68.623871\t-149.619370\t1.263625e-01\n",
    "# 1\t1800-01-01\t69.399323\t-150.949722\t1.670335e-02\n",
    "# 2\t1800-01-01\t65.165298\t-164.821640\t-1.262362e-01\n",
    "# 3\t1800-01-01\t64.874695\t-147.681366\t1.989791e-02\n",
    "# 4\t1800-01-01\t68.623871\t-149.619370\t1.253314e-01\n",
    "# ...\t...\t...\t...\t...\n",
    "# 1167084\t2100-12-01\t63.815472\t-144.956192\t-5.960464e-08\n",
    "# 1167085\t2100-12-01\t68.714645\t-149.028976\t-5.960464e-08\n",
    "# 1167086\t2100-12-01\t68.929497\t-150.280441\t5.960464e-08\n",
    "# 1167087\t2100-12-01\t65.567215\t-148.925171\t5.960464e-08\n",
    "# 1167088\t2100-12-01\t64.869881\t-147.739990\t5.960464e-08\n",
    "# 1167089 rows  4 columns\n",
    "# Shape of FCH4: (1972395203, 4)\n",
    "# datetime\tlat\tlon\tfch4\n",
    "# 0\t1996-06-01\t69.509682\t-148.587189\t37.477615\n",
    "# 1\t1996-06-01\t69.509682\t-148.587189\t10.290923\n",
    "# 2\t1996-06-01\t69.509682\t-148.587189\t13.020323\n",
    "# 3\t1996-06-01\t69.509682\t-148.587189\t-7.242540\n",
    "# 4\t1996-06-01\t69.509682\t-148.587189\t5.294002\n",
    "# ...\t...\t...\t...\t...\n",
    "# 1972395198\t2022-10-01\t61.268654\t-163.228394\t0.000449\n",
    "# 1972395199\t2022-10-01\t61.268654\t-163.228394\t0.011420\n",
    "# 1972395200\t2022-10-01\t61.268654\t-163.228394\t-0.010974\n",
    "# 1972395201\t2022-10-01\t61.268654\t-163.228394\t-0.248331\n",
    "# 1972395202\t2022-10-01\t61.268654\t-163.228394\t-0.540498\n",
    "# 1972395203 rows  4 columns\n",
    "# Shape of FC: (4210946142, 4)\n",
    "# datetime\tlat\tlon\tfc\n",
    "# 0\t1996-06-01\t69.509682\t-148.587189\t3.251806\n",
    "# 1\t1996-06-01\t69.509682\t-148.587189\t3.854896\n",
    "# 2\t1996-06-01\t69.509682\t-148.587189\t-1.799835\n",
    "# 3\t1996-06-01\t69.509682\t-148.587189\t0.554885\n",
    "# 4\t1996-06-01\t69.509682\t-148.587189\t0.926192\n",
    "# ...\t...\t...\t...\t...\n",
    "# 4210946137\t2022-12-01\t61.251190\t-163.266663\t33.813412\n",
    "# 4210946138\t2022-12-01\t61.251190\t-163.266663\t1.768532\n",
    "# 4210946139\t2022-12-01\t61.251190\t-163.266663\t-6.183669\n",
    "# 4210946140\t2022-12-01\t61.251190\t-163.266663\t-29.775229\n",
    "# 4210946141\t2022-12-01\t61.268654\t-163.228394\t1.443291\n",
    "# 4210946142 rows  4 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af39b1d2-e749-426d-a50e-eb23e5d8d283",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a674345d-5c0e-46d7-8fc3-a16eb300f40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_missing_data(df):\n",
    "    \"\"\"\n",
    "    Handles missing data by masking out rows with NaN values.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: The dataframe to process.\n",
    "    \n",
    "    Returns:\n",
    "    - The dataframe with NaN values masked.\n",
    "    \"\"\"\n",
    "    mask = df.isna().any(axis=1)  # Create a mask for rows with NaN values\n",
    "    df_clean = df.dropna().reset_index(drop=True)  # Drop rows with NaN values\n",
    "    return df_clean, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd67877-c47f-42bf-9bbb-3767f3caedc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(df, value_column):\n",
    "    \"\"\"\n",
    "    Cleans the dataframe by removing rows where the specified value column is 0 or NaN.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: The dataframe to clean.\n",
    "    - value_column: The name of the column containing the values to check for 0 or NaN.\n",
    "    \n",
    "    Returns:\n",
    "    - Cleaned dataframe.\n",
    "    \"\"\"\n",
    "    return df[(df[value_column] != 0) & (~df[value_column].isna())].sort_values(by=['datetime', 'lat', 'lon']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff2fd26-47a1-4505-bbd3-b3f111f99742",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pivot_to_sequences(df, n_months=12):\n",
    "    \"\"\"\n",
    "    Converts the cleaned dataframe into sequences suitable for machine learning models.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: The cleaned dataframe.\n",
    "    - n_months: Number of months to include in each sequence.\n",
    "    \n",
    "    Returns:\n",
    "    - Array of sequences.\n",
    "    \"\"\"\n",
    "    # Pivot the dataframe\n",
    "    pivot_df = df.pivot_table(index='datetime', columns=['lat', 'lon'], values=['alt'])#, 'fc', 'fch4'])\n",
    "    \n",
    "    # Replace 0 and NaN with a mask\n",
    "    pivot_df = pivot_df.replace(0, np.nan)\n",
    "    \n",
    "    sequences = []\n",
    "    unique_dates = pivot_df.index.unique()\n",
    "    \n",
    "    for start in range(len(unique_dates) - n_months + 1):\n",
    "        end = start + n_months\n",
    "        seq = pivot_df.loc[unique_dates[start:end]].values\n",
    "        if seq.shape[0] == n_months:\n",
    "            sequences.append(np.nan_to_num(seq, nan=0))  # Replace NaN with zero or another value\n",
    "    \n",
    "    return np.array(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d90591-72b2-4f28-a19a-2f08a3a944c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt=pivot_to_sequences(alt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078cb92d-b632-48d2-b50e-67277d81194d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(alt[0,0,:]).replace(0,np.nan).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d043983-5bcd-4951-a7e4-b2191c58b461",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt['datetime'] = pd.to_datetime(alt['datetime'])\n",
    "alt = alt.groupby(['datetime', 'lat', 'lon']).agg({'alt': 'mean'}).reset_index()\n",
    "\n",
    "filtered_alt = pd.DataFrame({'alt': alt.alt.values.round(4)}).replace(0, np.nan).dropna()\n",
    "filtered_df = pd.merge(alt, filtered_alt, left_index=True, right_index=True)\n",
    "aggregated_df = filtered_df.groupby(['datetime', 'lat', 'lon']).agg({'alt_x': 'mean'}).reset_index()\n",
    "aggregated_df.rename(columns={'alt_x': 'alt'}, inplace=True)\n",
    "aggregated_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b41e1e-08b3-4eb7-95c7-31853ff801eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid_resolution = 1.0  # km\n",
    "\n",
    "# aggregated_df['lat_bin'] = np.floor(aggregated_df['lat'] / grid_resolution)\n",
    "# aggregated_df['lon_bin'] = np.floor(aggregated_df['lon'] / grid_resolution)\n",
    "\n",
    "# tensor_df = aggregated_df.pivot_table(\n",
    "#     index=['lat_bin', 'lon_bin', 'datetime'],\n",
    "#     values=['alt'],\n",
    "#     aggfunc='mean'\n",
    "# ).fillna(0)  # Replace NaN with 0 (you can choose another strategy here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7cd59fd-81c6-4d3a-8d74-ccd6ae8919c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensor_df.stack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "898236d0-14f8-4097-a710-05ee8357ad44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Converting to 5D tensor (samples, timesteps, height, width, channels)\n",
    "# # Assuming the dataset is sorted by datetime, we can reshape directly\n",
    "# tensor = tensor_df.values.reshape(\n",
    "#     (-1, len(aggregated_df['datetime'].unique()), len(aggregated_df['lat_bin'].unique()), \\\n",
    "#      len(aggregated_df['lon_bin'].unique()), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad49721b-2f2c-4d07-8f6f-57a6a7899b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71284727-f72a-4985-a95a-e4c2d947f58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_df.alt.plot();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299432de-9bcb-454b-b5b8-c7e1ad072985",
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_df=aggregated_df.set_index('datetime').resample('M').mean().reset_index()\n",
    "aggregated_df.alt.plot();\n",
    "aggregated_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f553a46-ac98-4c56-8d30-783a8176a36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SHAPE, ALT\n",
    "# aggregated_df[:int(3565*0.7)]\n",
    "# aggregated_df[int(3565*0.7):(int(3565*0.7)+int(3565*.15))]\n",
    "# aggregated_df[int(3565*0.7)+int(3565*.15):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0358bb77-b489-4ce5-b99c-c7f3f2699606",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SHAPE, FCH4\n",
    "# aggregated_df[:int(317*0.7)]\n",
    "# aggregated_df[(int(317*0.7)):(int(317*0.7)+int(317*.15))]\n",
    "# aggregated_df[int(317*0.7)+int(317*.15):]\n",
    "\n",
    "# aggregated_df[:int(317*0.7)]\n",
    "# aggregated_df[int(317*0.7):(int(317*0.7)+int(317*.15))]\n",
    "# aggregated_df[int(317*0.7)+int(317*.15):];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1129ed9c-435d-4521-940e-72c594f504be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SHAPE, FC\n",
    "#aggregated_df[:int(319*0.7)]\n",
    "#aggregated_df[(int(319*0.7)):(int(319*0.7)+int(319*.15))]\n",
    "#aggregated_df[int(319*0.7)+int(319*.15):]\n",
    "\n",
    "# aggregated_df[:int(319*0.7)]\n",
    "# aggregated_df[int(319*0.7):(int(319*0.7)+int(319*.15))]\n",
    "# aggregated_df[int(319*0.7)+int(319*.15):];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "512d15ce-197e-4bfa-886f-ee07a8533616",
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_df['year'] = aggregated_df['datetime'].dt.year\n",
    "aggregated_df['month'] = aggregated_df['datetime'].dt.month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2ead05-dc6e-4953-8e26-a6084b879288",
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_df=aggregated_df[aggregated_df.isna()!=True]\n",
    "aggregated_df=aggregated_df.reset_index(drop=True)\n",
    "aggregated_df=aggregated_df.sort_values(by='datetime')\n",
    "print(\"Checking for NaN values before processing:\")\n",
    "print(aggregated_df.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2237b723-f0ca-42e3-bacd-694274bc6de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_df = aggregated_df.dropna(subset=['lat', 'lon', 'alt'])\n",
    "aggregated_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832024f6-7e7f-4b6c-bc11-a8948b619067",
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_df=aggregated_df[aggregated_df.isna()!=True]\n",
    "aggregated_df=aggregated_df.reset_index(drop=True)\n",
    "aggregated_df=aggregated_df.sort_values(by='datetime')\n",
    "print(\"Checking for NaN values before processing:\")\n",
    "print(aggregated_df.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2011db-655a-4458-a6f6-42fab468556d",
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20936e0-1bb4-4154-9452-2ab2661e8360",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #SHAPE, ALT\n",
    "# aggregated_df[:int(3241*0.7)]\n",
    "# aggregated_df[int(3241*0.7):(int(3241*0.7)+int(3241*.15))]\n",
    "# aggregated_df[int(3241*0.7)+int(3241*.15):];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e1bf13-26ff-40d9-a2b8-d72d426b62ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SHAPE, FC\n",
    "# aggregated_df[:int(244*0.7)]\n",
    "# aggregated_df[int(244*0.7):(int(244*0.7)+int(244*.15))]\n",
    "# aggregated_df[int(244*0.7)+int(244*.15):];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0aa71a-d861-4ef7-8246-3c132bf7b7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SHAPE, FCH4\n",
    "#aggregated_df[:int(226*0.7)]\n",
    "#aggregated_df[int(226*0.7):(int(226*0.7)+int(226*.15))]\n",
    "#aggregated_df[int(226*0.7)+int(226*.15):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a04fa2d-08e2-4a9e-8640-6f0b92bbb17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#unique_dts = aggregated_df['datetime'].unique()\n",
    "unique_years = np.sort(aggregated_df['year'].unique())\n",
    "unique_months = np.sort(aggregated_df['month'].unique())\n",
    "unique_lats = np.sort(aggregated_df['lat'].unique())\n",
    "unique_lons = np.sort(aggregated_df['lon'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bec033b-f637-415b-ba27-79a4beec49c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for NaN values in the merged dataframe\n",
    "print(aggregated_df.isna().sum())\n",
    "\n",
    "# Fill or handle NaN values appropriately\n",
    "aggregated_df.fillna(0, inplace=True)  # Example: filling NaN with 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5d3805-64b4-4055-9fb3-6f8fc007ca80",
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_df.interpolate(method='linear', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6d2d20-b2c8-40bf-a034-491e7f68f470",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Check normalization\n",
    "min_value = aggregated_df[['alt']].min()\n",
    "if (min_value < 0).any():\n",
    "    print(\"Warning: Negative values found before log transformation!\")\n",
    "\n",
    "# Apply log transformation if necessary and ensure it's done safely\n",
    "aggregated_df[['alt']] = aggregated_df[['alt']].apply(lambda x: np.log1p(x - x.min() + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3ae8ff-1823-4aa2-803c-4379cc4507b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ndarray = np.zeros((len(unique_years), len(unique_months), len(unique_lats), len(unique_lons)))\n",
    "\n",
    "# Create mappings from years, months, latitudes, and longitudes to indices\n",
    "year_to_idx = {year: i for i, year in enumerate(unique_years)}\n",
    "month_to_idx = {month: i for i, month in enumerate(unique_months)}\n",
    "lat_to_idx = {lat: i for i, lat in enumerate(unique_lats)}\n",
    "lon_to_idx = {lon: i for i, lon in enumerate(unique_lons)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc592620-3711-4776-ae86-c0aefe47af81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Populate the ndarray with alt values\n",
    "for _, row in aggregated_df.iterrows():\n",
    "    year_idx = year_to_idx[row['year']]\n",
    "    month_idx = month_to_idx[row['month']]\n",
    "    lat_idx = lat_to_idx[row['lat']]\n",
    "    lon_idx = lon_to_idx[row['lon']]\n",
    "    ndarray[year_idx, month_idx - 1, lat_idx, lon_idx] = row['alt']  # month - 1 for 0-indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa7963a-2948-492d-8580-4e2e76bf995d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ndarray = ndarray[..., np.newaxis]\n",
    "print(f\"Final ndarray shape: {ndarray.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac3e82b-aed4-4e3f-94f7-96f86029ad6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_timesteps = ndarray.shape[0]\n",
    "train_size = int(0.7 * n_timesteps)\n",
    "valid_size = int(0.15 * n_timesteps)\n",
    "test_size = n_timesteps - train_size - valid_size\n",
    "\n",
    "X_train = ndarray[:train_size]\n",
    "y_train = ndarray[:train_size, :, :, :, 0]\n",
    "X_valid = ndarray[train_size:train_size + valid_size]\n",
    "y_valid = ndarray[train_size:train_size + valid_size, :, :, :, 0]\n",
    "X_test = ndarray[train_size + valid_size:]\n",
    "y_test = ndarray[train_size + valid_size:, :, :, :, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd87c6e8-0801-4017-92c3-a8a1c4e82ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, X_valid.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343f6e5d-81b6-46c9-92a6-f79364efd217",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape, y_valid.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba16874-5956-4b1e-a5d0-b4c8d0ecc25c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c0c63f-bba9-4d18-85f9-8993546f6c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Create a scaler for each training, validation, and test dataset\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# FOR ALT\n",
    "# Flatten the data for scaling\n",
    "X_train_flat = X_train.reshape(-1, 1)\n",
    "X_valid_flat = X_valid.reshape(-1, 1)\n",
    "X_test_flat = X_test.reshape(-1, 1)\n",
    "# Fit the scaler on the training data and transform all datasets\n",
    "X_train_scaled = scaler.fit_transform(X_train_flat).reshape(X_train.shape)\n",
    "X_valid_scaled = scaler.transform(X_valid_flat).reshape(X_valid.shape)\n",
    "X_test_scaled = scaler.transform(X_test_flat).reshape(X_test.shape)\n",
    "#uncertainty = np.abs(1/alt.alt.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9b49fe-9d1e-4bc0-b6d1-68d2d258d541",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "valid_dataset = tf.data.Dataset.from_tensor_slices((X_valid, y_valid))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
    "\n",
    "batch_size = 1\n",
    "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "valid_dataset = valid_dataset.shuffle(buffer_size=1024).batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "test_dataset = test_dataset.shuffle(buffer_size=1024).batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "# train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train)).batch(1)\n",
    "# valid_dataset = tf.data.Dataset.from_tensor_slices((X_valid, y_valid)).batch(1)\n",
    "# test_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(1)\n",
    "# # train_dataset = tf.data.Dataset.from_tensor_slices((X_train_scaled, y_train)).batch(32)\n",
    "# # valid_dataset = tf.data.Dataset.from_tensor_slices((X_valid_scaled, y_valid)).batch(32)\n",
    "# # test_dataset = tf.data.Dataset.from_tensor_slices((X_test_scaled, y_test)).batch(32)\n",
    "\n",
    "train_dataset.element_spec[0].shape, valid_dataset.element_spec[0].shape, test_dataset.element_spec[0].shape\n",
    "\n",
    "# Example of iterating through the dataset\n",
    "for batch in train_dataset.take(1):\n",
    "    X_train_batch, y_train_batch = batch\n",
    "    print(X_train_batch.shape, y_train_batch.shape)  # This should be the batched shapes\n",
    "for batch in valid_dataset.take(1):\n",
    "    X_valid_batch, y_valid_batch = batch\n",
    "    print(X_valid_batch.shape, y_valid_batch.shape)  # This should be the batched shapes\n",
    "for batch in test_dataset.take(1):\n",
    "    X_test_batch, y_test_batch = batch\n",
    "    print(X_test_batch.shape, y_test_batch.shape)  # This should be the batched shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee47243a-a65c-4cdd-bfbd-e46c1b4aa514",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133a096d-748b-40b5-8705-8c9c2aa4f76e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2775e3-ee13-4a47-9d73-ad5da33551b0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# from tensorflow.keras import layers, models\n",
    "\n",
    "# # Build the model\n",
    "# def model(hp):\n",
    "#     model = models.Sequential()\n",
    "    \n",
    "#     # ConvLSTM2D layer\n",
    "#     model.add(layers.ConvLSTM2D(\n",
    "#         filters=32,\n",
    "#         kernel_size=(3, 3),\n",
    "#         activation='relu',\n",
    "#         padding='same',\n",
    "#         return_sequences=True,\n",
    "#         #input_shape=(12, 559, 561, 1)\n",
    "#         input_shape=(12, 186, 186, 1)\n",
    "#     ))\n",
    "    \n",
    "#     # Another ConvLSTM2D layer\n",
    "#     model.add(layers.ConvLSTM2D(\n",
    "#         filters=32,\n",
    "#         kernel_size=(3, 3),\n",
    "#         activation='relu',\n",
    "#         padding='same',\n",
    "#         return_sequences=True\n",
    "#     ))\n",
    "    \n",
    "#     model.add(layers.BatchNormalization())\n",
    "        \n",
    "#     model.add(layers.Dropout(0.2))\n",
    "    \n",
    "#     model.add(layers.ConvLSTM2D(\n",
    "#         filters=hp.Int('filters_2', min_value=16, max_value=64, step=16), \n",
    "#         kernel_size=hp.Choice('kernel_size_2', values=[1, 3]),\n",
    "#         padding='same', \n",
    "#         return_sequences=True,\n",
    "#         activation=hp.Choice('activation_function_2', values=['relu','sigmoid', 'exponential', 'gelu', 'elu', 'linear', 'selu', 'swish','leaky_relu']), #'tanh','softmax',\n",
    "#     ))\n",
    "        \n",
    "#     model.add(layers.BatchNormalization())\n",
    "        \n",
    "#     model.add(layers.Dropout(0.2))\n",
    "    \n",
    "#     # Final Conv3D layer to match the output dimensions\n",
    "#     model.add(layers.Conv3D(\n",
    "#         filters=1,  # Single channel output\n",
    "#         kernel_size=(3, 3, 3),\n",
    "#         activation='linear',\n",
    "#         padding='same'\n",
    "#     ))\n",
    "    \n",
    "#     # Compile the model\n",
    "#     model.compile(optimizer='adam', loss='mse')\n",
    "    \n",
    "#     # Summary of the model\n",
    "#     model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7016b6-ce03-414a-ab23-b2b5dfbfb43e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "# # Callbacks setup\n",
    "# callbacks = [\n",
    "#     ModelCheckpoint('model_checkpoint.keras', save_best_only=True),\n",
    "#     EarlyStopping(monitor='val_loss', patience=5)\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc79c26c-3cec-4abc-b356-78eedeeee98a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# # Train the model\n",
    "# history = model.fit(\n",
    "#     train_dataset, \n",
    "#     epochs=5, \n",
    "#     validation_data=valid_dataset, \n",
    "#     callbacks=callbacks\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ef65fd-b210-4edd-bced-8dc81fac173f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# plt.plot(history.history['loss']);\n",
    "# #plt.plot(history.history['val_loss']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0968837a-441c-4f30-a606-15a2dd6bed3a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# plt.plot(history.history['loss']);\n",
    "# #plt.plot(history.history['val_loss']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7905b3b-c6d9-4ac5-a300-1c59e2bf72f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6bad462-1e51-4612-8e1b-c931aad1df80",
   "metadata": {},
   "outputs": [],
   "source": [
    "#aggregated_df[168:]\n",
    "#aggregated_df[168:168+36]\n",
    "aggregated_df[168+36:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a160a32c-40d5-4cc0-b8a1-737a8959d592",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418389b4-a4b6-45d8-9da5-4fa13fab098b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "def build_model(hp):\n",
    "    model = models.Sequential()\n",
    "    \n",
    "    model.add(layers.ConvLSTM2D(\n",
    "        filters=hp.Int('filters_1', min_value=16, max_value=64, step=16), \n",
    "        kernel_size=hp.Choice('kernel_size_1', values=[1, 3]),\n",
    "        input_shape=(None, 559, 561, 1),\n",
    "        #input_shape=(X_train.shape[1], 1, 1, X_train.shape[3]),\n",
    "        padding='same', \n",
    "        return_sequences=True,\n",
    "        activation=hp.Choice('activation_function_1', values=['relu','sigmoid',  'gelu', 'elu', 'linear', 'selu', 'swish','leaky_relu']), #'tanh','softmax',\n",
    "    ))\n",
    "    \n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Dropout(0.2))\n",
    "\n",
    "    model.add(layers.ConvLSTM2D(\n",
    "        filters=hp.Int('filters_2', min_value=16, max_value=64, step=16), \n",
    "        kernel_size=hp.Choice('kernel_size_2', values=[1, 3]),\n",
    "        padding='same', \n",
    "        return_sequences=True,\n",
    "        activation=hp.Choice('activation_function_2', values=['relu','sigmoid', 'gelu', 'elu', 'linear', 'selu', 'swish','leaky_relu']), #'tanh','softmax',\n",
    "    ))\n",
    "    \n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Dropout(0.2))\n",
    "\n",
    "    # Extract the last time step\n",
    "    #model.add(Lambda(lambda x: x[:, -1, :, :, :]))  # Shape becomes [batch_size, height, width, channels]\n",
    "\n",
    "    # Conv2D Layer for output (single frame prediction)\n",
    "    # model.add(Conv2D(\n",
    "    #     filters=1,  # Ensure only one output channel to match y_train's shape\n",
    "    #     kernel_size=(1, 1),  # Use a 2D kernel size\n",
    "    #     activation=hp.Choice('activation_function_3', values=['relu','tanh','softmax','sigmoid','leaky_relu','swish']),\n",
    "    #     padding='same',\n",
    "    #     data_format='channels_last'\n",
    "    # ))\n",
    "\n",
    "    model.add(layers.Conv3D(\n",
    "        filters=1,  # Single channel output\n",
    "        kernel_size=(3, 3, 3),\n",
    "        activation=hp.Choice('activation_function_3', values=['relu','sigmoid', 'gelu', 'elu', 'linear', 'selu', 'swish','leaky_relu']), #'tanh','softmax',\n",
    "        padding='same',\n",
    "        data_format='channels_last'\n",
    "    )\n",
    "             )\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(\n",
    "        optimizer=Adam(clipvalue=1.0),  # Set the clip value\n",
    "        loss='mse',\n",
    "        metrics = ['mae','mse','mape','accuracy']\n",
    "    )\n",
    "\n",
    "    model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915f3231-4c8b-49c7-9ed9-e2228fad288e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#batch_size = 32  # Adjust based on memory and GPU capability\n",
    "#train_dataset = train_dataset.cache().shuffle(1000).batch(batch_size).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "#valid_dataset = valid_dataset.batch(batch_size).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "#test_dataset = test_dataset.batch(batch_size).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "# Define callbacks\n",
    "early_stopping_cb = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "lr_scheduler_cb = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5)\n",
    "checkpoint_cb = ModelCheckpoint('model_checkpoint.keras', save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b373b4-76d5-42e9-a387-f0d3da9dacc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_tuner import BayesianOptimization\n",
    "\n",
    "# Define the tuner\n",
    "tuner = BayesianOptimization(\n",
    "    build_model,\n",
    "    objective='val_loss',\n",
    "    max_trials=10,\n",
    "    executions_per_trial=1,\n",
    "    #directory='/Volumes/JPL/alt_train_new',\n",
    "    directory='/Volumes/JPL/altnew_train_new',\n",
    "    #project_name='alt_train_conv3dlstm_tuning',\n",
    "    project_name='alt_train_conv3dlstm_tuning',\n",
    "    overwrite=False,\n",
    ")\n",
    "\n",
    "tuner.search_space_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25c1c8f-7a77-4de4-868a-faf8e784c249",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "\n",
    "# from tensorflow.keras.mixed_precision import set_global_policy\n",
    "# set_global_policy('float32')\n",
    "\n",
    "#X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1, 1, X_train.shape[3]))\n",
    "#X_valid = X_valid.reshape((X_valid.shape[0], X_valid.shape[1], 1, 1, X_valid.shape[3]))\n",
    "#X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1, 1, X_test.shape[3]))\n",
    "#tuner.search(train_dataset, epochs=10, validation_data=valid_dataset, callbacks=[early_stopping_cb, lr_scheduler_cb])\n",
    "#tuner.search(X_train, y_train, epochs=100, validation_data=(X_valid, y_valid), callbacks=[early_stopping_cb, lr_scheduler_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5740261-4950-419e-9fe7-0923cb962ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.search(train_dataset, epochs=10, validation_data=valid_dataset, callbacks=[early_stopping_cb, lr_scheduler_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8adc7d0f-72a6-4f5f-9eeb-e7f14d11d3af",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = tuner.get_best_models(num_models=1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4860900-7d59-4e7e-8457-021bd9721a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_model(best_model, to_file='/Volumes/JPL/alt_model.png', show_shapes=True, show_layer_names=True, dpi=300)\n",
    "plot_model(best_model, to_file='/Volumes/JPL/co2_model.png', show_shapes=True, show_layer_names=True, dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482979b5-2779-441a-a971-cb97f36f39a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14bd395-d0b9-44e6-bb6f-29b964086912",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.results_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad22c4b6-6c7a-44eb-a4b8-ffcb85f19f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the best model on the test data\n",
    "test_loss = best_model.evaluate(test_dataset)\n",
    "print(f'Test Loss: {test_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9edee5-58da-433e-91dc-ef22878ea56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping_cb = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "lr_scheduler_cb = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5)\n",
    "checkpoint_cb = ModelCheckpoint('model_checkpoint.keras', save_best_only=True)\n",
    "\n",
    "history = best_model.fit(train_dataset, validation_data=valid_dataset, epochs=100, callbacks=[early_stopping_cb, lr_scheduler_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09dad63a-9dbb-4319-9305-80204cf2ebcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#history = best_model.fit(train_dataset, epochs=10, validation_data=valid_dataset, callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7aa723-e88b-41e3-b7ed-0c45676e62ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_test_loss, final_test_mae = best_model.evaluate(train_dataset)\n",
    "print(f\"Final Test Loss: {final_test_loss}, Final Test MAE: {final_test_mae}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92753ea5-994d-4dda-adbc-480aad859765",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.evaluate(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43bb1da8-b513-407b-9e38-f93e7d03dffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = best_model.predict(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374e1a53-3576-4c34-8770-617f5077c985",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If using a scaler, inverse transform the predictions and actual values\n",
    "predictions_original = scaler.inverse_transform(predictions.reshape(-1, 1)).reshape(predictions.shape)\n",
    "actual_values_original = scaler.inverse_transform(y_test.reshape(-1, 1)).reshape(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3861b320-8632-49a0-89a9-3d724c632764",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare first few predictions with actual values\n",
    "for i in range(4):\n",
    "    print(f\"Prediction {i+1}: {predictions_original[i, :, :, 0]}\")\n",
    "    print(f\"Actual Value {i+1}: {actual_values_original[i, :, :, 0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21bac863-d3cc-42f0-b708-9c460ee51491",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_original.shape, actual_values_original.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c815c94-7add-4395-a7ac-e8b0faf7976f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title('Predicted (Original Scale)')\n",
    "plt.imshow(predictions_original[0, 7, :, :, 0], cmap='plasma')\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title('Actual (Original Scale)')\n",
    "plt.imshow(actual_values_original[0, 7, :, :], cmap='plasma')\n",
    "plt.colorbar()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34153069-2e87-4d6e-9da7-85c71df44531",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "def model_with_dropout(hp):\n",
    "    model = Sequential()\n",
    "    \n",
    "    # ConvLSTM2D layer to process the sequence data\n",
    "    model.add(ConvLSTM2D(\n",
    "        filters=hp.Int('filters_1', min_value=16, max_value=64, step=16), \n",
    "        kernel_size=hp.Choice('kernel_size_1', values=[1, 3]),\n",
    "        padding='same', \n",
    "        return_sequences=True,\n",
    "        activation=hp.Choice('activation_function_1', values=['relu','sigmoid', 'exponential', 'gelu', 'elu', 'linear', 'selu', 'swish','leaky_relu']), #'tanh','softmax',\n",
    "        input_shape=(12, 165, 165, 1),\n",
    "    ))\n",
    "    \n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.5))  # Dropout layer for uncertainty\n",
    "    \n",
    "    # Another ConvLSTM2D layer\n",
    "    model.add(ConvLSTM2D(\n",
    "        filters=hp.Int('filters_2', min_value=16, max_value=64, step=16), \n",
    "        kernel_size=hp.Choice('kernel_size_2', values=[1, 3]),\n",
    "        padding='same',\n",
    "        activation=hp.Choice('activation_function_2', values=['relu','sigmoid', 'exponential', 'gelu', 'elu', 'linear', 'selu', 'swish','leaky_relu']), #'tanh','softmax',\n",
    "        return_sequences=True\n",
    "    ))\n",
    "    \n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.5))  # Dropout layer for uncertainty\n",
    "    \n",
    "    # Final ConvLSTM2D layer without return_sequences\n",
    "    model.add(ConvLSTM2D(\n",
    "        filters=hp.Int('filters_3', min_value=16, max_value=64, step=16), \n",
    "        kernel_size=hp.Choice('kernel_size_3', values=[1, 3]),\n",
    "        padding='same', \n",
    "        activation=hp.Choice('activation_function_3', values=['relu','sigmoid', 'exponential', 'gelu', 'elu', 'linear', 'selu', 'swish','leaky_relu']), #'tanh','softmax',\n",
    "        return_sequences=False\n",
    "    ))\n",
    "    \n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.5))  # Dropout layer for uncertainty\n",
    "\n",
    "    model.add(layers.Conv3D(\n",
    "        filters=1,  # Single channel output\n",
    "        kernel_size=(1, 1, 1),\n",
    "        activation=hp.Choice('activation_function_3', values=['relu','sigmoid', 'exponential', 'gelu', 'elu', 'linear', 'selu', 'swish','leaky_relu']), #'tanh','softmax',\n",
    "        padding='same',\n",
    "        data_format='channels_last'\n",
    "    ))\n",
    "    \n",
    "    # # Conv2D to reduce dimensions\n",
    "    # model.add(Conv2D(\n",
    "    #     filters=1, \n",
    "    #     kernel_size=1, \n",
    "    #     activation='sigmoid', \n",
    "    #     padding='same'))\n",
    "    \n",
    "    # model.add(Flatten())\n",
    "    # model.add(Dense(1, activation='linear'))\n",
    "    \n",
    "    model.compile(optimizer=Adam(lr=1e-4), loss='mse', metrics=['mae','mse','accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97962085-856f-40ea-9607-d0c5d27915bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Monte Carlo Dropout\n",
    "def monte_carlo_prediction(model, x_data, n_samples=50):\n",
    "    predictions = [model.predict(x_data) for _ in range(n_samples)]\n",
    "    predictions = np.array(predictions)\n",
    "    \n",
    "    # Mean and variance\n",
    "    mean_prediction = np.mean(predictions, axis=0)\n",
    "    uncertainty = np.std(predictions, axis=0)\n",
    "    \n",
    "    return mean_prediction, uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3031c4c-a06b-4cd2-9790-53249b8d53c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "zip(*[monte_carlo_prediction(model, X_test, n_samples=50) for model in models])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7313e8-a6b9-42d1-bf6e-8117fff6719e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tunerX.oracle.trials.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa16eb6f-4c98-4872-951e-fe488b259332",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble_predictions(models, x_data):\n",
    "    predictions = np.array([model.predict(x_data) for model in models])\n",
    "    mean_prediction = np.mean(predictions, axis=0)\n",
    "    uncertainty = np.std(predictions, axis=0)\n",
    "    \n",
    "    return mean_prediction, uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c93079-1ed9-4989-9993-eabfa9022b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_ensemble_uncertainty(mean_preds, uncertainties):\n",
    "    \"\"\"\n",
    "    Aggregate predictions and uncertainties from ensemble models.\n",
    "    \n",
    "    Parameters:\n",
    "    - mean_preds: Array of mean predictions from ensemble models.\n",
    "    - uncertainties: Array of uncertainties from ensemble models.\n",
    "    \n",
    "    Returns:\n",
    "    - Combined mean prediction and uncertainty.\n",
    "    \"\"\"\n",
    "    combined_mean = np.mean(mean_preds, axis=0)\n",
    "    combined_uncertainty = np.sqrt(np.sum(np.square(uncertainties), axis=0) / len(uncertainties))\n",
    "    return combined_mean, combined_uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab7bc59-777c-4e4e-bca9-f9cb7d57eda5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming models is a list of trained models\n",
    "mean_preds, uncertainties = zip(*[monte_carlo_prediction(model, X_test, n_samples=50) for model in models])\n",
    "combined_mean, combined_uncertainty = aggregate_ensemble_uncertainty(mean_preds, uncertainties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "696342c0-8407-4058-96db-416c0c068cb7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9971b234-7f2b-4906-b4fb-37565ed7eabe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9487b5f4-501c-46cd-a2ad-741ea49f7784",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Function to compute Grad-CAM\n",
    "# def make_gradcam_heatmap(img_array, model, last_conv_layer_name, classifier_layer_names):\n",
    "#     grad_model = tf.keras.models.Model(\n",
    "#         [model.inputs], [model.get_layer(last_conv_layer_name).output, model.output]\n",
    "#     )\n",
    "#     with tf.GradientTape() as tape:\n",
    "#         conv_outputs, predictions = grad_model(img_array)\n",
    "#         loss = predictions[:, tf.argmax(predictions[0])]\n",
    "#     grads = tape.gradient(loss, conv_outputs)\n",
    "#     pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n",
    "#     conv_outputs = conv_outputs[0]\n",
    "#     heatmap = conv_outputs @ pooled_grads[..., tf.newaxis]\n",
    "#     heatmap = tf.squeeze(heatmap)\n",
    "#     heatmap = tf.maximum(heatmap, 0) / tf.math.reduce_max(heatmap)\n",
    "#     return heatmap.numpy()\n",
    "    \n",
    "# # Assume the model is already defined and trained\n",
    "# img = np.random.random((1, 559, 561, 1))  # Example 2D slice of input\n",
    "# last_conv_layer_name = \"conv3d_layer\"  # Replace with your Conv3D layer name\n",
    "# classifier_layer_names = [\"dense\"]  # Replace with your dense layers\n",
    "\n",
    "# heatmap = make_gradcam_heatmap(img, model, last_conv_layer_name, classifier_layer_names)\n",
    "\n",
    "# # Plot the heatmap\n",
    "# plt.matshow(heatmap)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c3a32f-0863-455c-98d6-535f9b575fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# # Collect the layer information\n",
    "# layer_info = []\n",
    "# for layer in model.layers:\n",
    "#     layer_info.append({\n",
    "#         \"Layer Name\": layer.name,\n",
    "#         \"Layer Type\": layer.__class__.__name__,\n",
    "#         \"Input Shape\": layer.input_shape,\n",
    "#         \"Output Shape\": layer.output_shape,\n",
    "#         \"Number of Parameters\": layer.count_params(),\n",
    "#         \"Activation\": layer.activation.__name__ if hasattr(layer, 'activation') else 'N/A'\n",
    "#     })\n",
    "\n",
    "# # Create a DataFrame\n",
    "# df = pd.DataFrame(layer_info)\n",
    "# print(df)\n",
    "\n",
    "# # Optionally, save it to a CSV\n",
    "# df.to_csv(\"model_summary.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39bb307d-9ec3-4a65-970e-05db3bae7588",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55903a3e-ac7d-4927-b937-acc6f73a4a9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae17b24b-d1bf-4af1-a1c2-f99bcbef6ee4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a199d4-cd13-4a79-96db-33fbb2498b21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08667990-224c-42fc-9f31-9199dc104b65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1bfe1de-195e-4193-be0f-701ab262f4e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d10703e-cbd1-4f51-b860-2cf110814a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_steps = 50\n",
    "lats = 128\n",
    "lons = 128\n",
    "features = 4\n",
    "out_feats = 3\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import ConvLSTM2D, Dense, Flatten\n",
    "\n",
    "# Example: ConvLSTM2D Model\n",
    "model = Sequential([\n",
    "    ConvLSTM2D(filters=32, kernel_size=(3, 3), activation='relu', input_shape=(X.shape[1], X.shape[2], X.shape[3], X.shape[4])),\n",
    "    Flatten(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(1, activation='linear')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "model.fit(X, Y, epochs=10, batch_size=32, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4216c668-89a9-429c-8822-aeab29a1969d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497b396d-6a95-4410-8754-9872176af82f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3e4eb2-62f9-4da9-b03a-7bffb3710741",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64fe4c7-3258-45a5-91b6-1263d6be0bcc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbce552f-279f-4d91-a16d-3c5a68863454",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f14de5b-e455-4599-90f9-74e667878015",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e04ac2-7e3e-424b-8179-6a6aab9a1d21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c82229-2c85-4f81-855a-10f1eb2f6d95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e79777-8404-4028-bfcb-8eaf9fb351b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_timesteps = ndarray.shape[0]\n",
    "train_size = int(0.7 * n_timesteps)\n",
    "valid_size = int(0.15 * n_timesteps)\n",
    "test_size = n_timesteps - train_size - valid_size\n",
    "\n",
    "X_train = ndarray[:train_size]\n",
    "y_train = ndarray[:train_size, :, :, :, 0]\n",
    "X_valid = ndarray[train_size:train_size + valid_size]\n",
    "y_valid = ndarray[train_size:train_size + valid_size, :, :, :, 0]\n",
    "X_test = ndarray[train_size + valid_size:]\n",
    "y_test = ndarray[train_size + valid_size:, :, :, :, 0]\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train)).batch(1)\n",
    "valid_dataset = tf.data.Dataset.from_tensor_slices((X_valid, y_valid)).batch(1)\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(1)\n",
    "\n",
    "# X_train = tf.convert_to_tensor(X_train, dtype=tf.float32)\n",
    "# y_train = tf.convert_to_tensor(y_train, dtype=tf.float32)\n",
    "# X_valid = tf.convert_to_tensor(X_valid, dtype=tf.float32)\n",
    "# y_valid = tf.convert_to_tensor(y_valid, dtype=tf.float32)\n",
    "# X_test = tf.convert_to_tensor(X_test, dtype=tf.float32)\n",
    "# y_test = tf.convert_to_tensor(y_test, dtype=tf.float32)\n",
    "\n",
    "# X_train = X_train[..., np.newaxis]  # Shape becomes (batch_size, time_steps, rows, cols, 1)\n",
    "# X_valid = X_valid[..., np.newaxis]\n",
    "# X_test = X_test[..., np.newaxis]\n",
    "# y_train = y_train[..., np.newaxis]\n",
    "# y_valid = y_valid[..., np.newaxis]\n",
    "# y_test = y_test[..., np.newaxis]\n",
    "\n",
    "# X_train = tf.transpose(X_train, perm=[0, 4, 1, 2, 3])  # Shape becomes (batch_size, 1, rows, cols, channels)\n",
    "# X_valid = tf.transpose(X_valid, perm=[0, 4, 1, 2, 3])\n",
    "# X_test = tf.transpose(X_test, perm=[0, 4, 1, 2, 3])\n",
    "\n",
    "# train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))#.batch(32)\n",
    "# valid_dataset = tf.data.Dataset.from_tensor_slices((X_valid, y_valid))#.batch(32)\n",
    "# test_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test))#.batch(32)\n",
    "\n",
    "# train_dataset = train_dataset.batch(32)\n",
    "# valid_dataset = valid_dataset.batch(32)\n",
    "# test_dataset = test_dataset.batch(32)\n",
    "\n",
    "# #X = ndarray[..., :-1]\n",
    "# #y = ndarray[..., -1:]\n",
    "# #X = X.reshape(X.shape[0], X.shape[1], X.shape[2], X.shape[3])\n",
    "# #y = y.reshape(y.shape[0], y.shape[1], y.shape[2], y.shape[3])\n",
    "\n",
    "# #X_tensor = tf.convert_to_tensor(X, dtype=tf.float32)\n",
    "# #y_tensor = tf.convert_to_tensor(y, dtype=tf.float32)\n",
    "# #dataset = tf.data.Dataset.from_tensor_slices((X_tensor, y_tensor))\n",
    "# #batch_size = 32\n",
    "# #dataset = dataset.shuffle(buffer_size=len(X)).batch(batch_size)\n",
    "# #alt_dataset = dataset#.take(train_size)\n",
    "# #alt_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34018ac-2108-47f4-9c7f-348cc3ff8a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_dataset\n",
    "for data in train_dataset.take(1):\n",
    "    print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba280400-65fd-4035-b21a-d928fab38a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#valid_dataset\n",
    "for data in valid_dataset.take(1):\n",
    "   print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f01ee45-bd3a-4656-9c09-9a7dec7d7798",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_dataset\n",
    "for data in test_dataset.take(1):\n",
    "  print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e6bb06-42f0-49af-9266-a963cc344579",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b05b89-f8c4-4337-a883-6cd6845f1f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ndarray.shape)  # Ensure this outputs the correct shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed5f763-cfb9-498e-8b42-cc81577bc655",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# filtered_alt = pd.DataFrame({'alt': alt_valid.alt.values.round(4)}).replace(0, np.nan).dropna()\n",
    "# filtered_df = pd.merge(alt_valid, filtered_alt, left_index=True, right_index=True)\n",
    "# aggregated_df = filtered_df.groupby(['datetime', 'lat', 'lon']).agg({'alt_x': 'mean'}).reset_index()\n",
    "# aggregated_df.rename(columns={'alt_x': 'alt'}, inplace=True)\n",
    "# pivot_df = aggregated_df.pivot_table(index='datetime', columns=['lat', 'lon'], values='alt')\n",
    "# unique_dts = aggregated_df['datetime'].unique()\n",
    "# unique_lats = np.sort(aggregated_df['lat'].unique())\n",
    "# unique_lons = np.sort(aggregated_df['lon'].unique())\n",
    "# ndarray = np.zeros((len(unique_dts), len(unique_lats), len(unique_lons)))\n",
    "# lat_to_idx = {lat: i for i, lat in enumerate(unique_lats)}\n",
    "# lon_to_idx = {lon: i for i, lon in enumerate(unique_lons)}\n",
    "# dt_to_idx = {dt: i for i, dt in enumerate(unique_dts)}\n",
    "# for _, row in aggregated_df.iterrows():\n",
    "#     dt_idx = dt_to_idx[row['datetime']]\n",
    "#     lat_idx = lat_to_idx[row['lat']]\n",
    "#     lon_idx = lon_to_idx[row['lon']]\n",
    "#     ndarray[dt_idx, lat_idx, lon_idx] = row['alt']\n",
    "# ndarray = ndarray[..., np.newaxis]\n",
    "# X = ndarray[..., :-1]\n",
    "# y = ndarray[..., -1:]\n",
    "# X = X.reshape(X.shape[0], X.shape[1], X.shape[2], X.shape[3])\n",
    "# y = y.reshape(y.shape[0], y.shape[1], y.shape[2], y.shape[3])\n",
    "# X_tensor = tf.convert_to_tensor(X, dtype=tf.float32)\n",
    "# y_tensor = tf.convert_to_tensor(y, dtype=tf.float32)\n",
    "# dataset = tf.data.Dataset.from_tensor_slices((X_tensor, y_tensor))\n",
    "# batch_size = 32\n",
    "# dataset = dataset.shuffle(buffer_size=len(X)).batch(batch_size)\n",
    "# valid_dataset = dataset#.take(train_size)\n",
    "# valid_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1a44da-9675-409a-b3aa-ca0aefe522e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ndarray.shape)  # Ensure this outputs the correct shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481f1ffb-ce69-4087-8fe2-b4a437068dac",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# filtered_alt = pd.DataFrame({'alt': alt_test.alt.values.round(4)}).replace(0, np.nan).dropna()\n",
    "# filtered_df = pd.merge(alt_test, filtered_alt, left_index=True, right_index=True)\n",
    "# aggregated_df = filtered_df.groupby(['datetime', 'lat', 'lon']).agg({'alt_x': 'mean'}).reset_index()\n",
    "# aggregated_df.rename(columns={'alt_x': 'alt'}, inplace=True)\n",
    "# pivot_df = aggregated_df.pivot_table(index='datetime', columns=['lat', 'lon'], values='alt')\n",
    "# unique_dts = aggregated_df['datetime'].unique()\n",
    "# unique_lats = np.sort(aggregated_df['lat'].unique())\n",
    "# unique_lons = np.sort(aggregated_df['lon'].unique())\n",
    "# ndarray = np.zeros((len(unique_dts), len(unique_lats), len(unique_lons)))\n",
    "# lat_to_idx = {lat: i for i, lat in enumerate(unique_lats)}\n",
    "# lon_to_idx = {lon: i for i, lon in enumerate(unique_lons)}\n",
    "# dt_to_idx = {dt: i for i, dt in enumerate(unique_dts)}\n",
    "# for _, row in aggregated_df.iterrows():\n",
    "#     dt_idx = dt_to_idx[row['datetime']]\n",
    "#     lat_idx = lat_to_idx[row['lat']]\n",
    "#     lon_idx = lon_to_idx[row['lon']]\n",
    "#     ndarray[dt_idx, lat_idx, lon_idx] = row['alt']\n",
    "# ndarray = ndarray[..., np.newaxis]\n",
    "# X = ndarray[..., :-1]\n",
    "# y = ndarray[..., -1:]\n",
    "# X = X.reshape(X.shape[0], X.shape[1], X.shape[2], X.shape[3])\n",
    "# y = y.reshape(y.shape[0], y.shape[1], y.shape[2], y.shape[3])\n",
    "# X_tensor = tf.convert_to_tensor(X, dtype=tf.float32)\n",
    "# y_tensor = tf.convert_to_tensor(y, dtype=tf.float32)\n",
    "# dataset = tf.data.Dataset.from_tensor_slices((X_tensor, y_tensor))\n",
    "# batch_size = 32\n",
    "# dataset = dataset.shuffle(buffer_size=len(X)).batch(batch_size)\n",
    "# test_dataset = dataset#.take(train_size)\n",
    "# test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ceec39-9104-4eea-9b29-b843c6e24ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ndarray.shape)  # Ensure this outputs the correct shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8accce78-6f3b-472e-9be0-3f703012077f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, X_valid.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b1ca6a-9751-47c8-9b18-cd397a351f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape, y_valid.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b60a8d7-a2b7-46d5-ada0-a5069236ac3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.element_spec[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252943ad-57b0-4a8a-bbb6-465b1faa7e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    #tf.keras.layers.ConvLSTM2D(64, kernel_size=(3, 3), input_shape=(X_train.shape[1], X_train.shape[2], X_train.shape[3], 1), return_sequences=True),\n",
    "    \n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# Train the model using the training and validation datasets\n",
    "history = model.fit(train_dataset, epochs=10, validation_data=valid_dataset)\n",
    "\n",
    "# Evaluate the model on the test dataset\n",
    "test_loss, test_mae = model.evaluate(test_dataset)\n",
    "print(f'Test Loss: {test_loss}, Test MAE: {test_mae}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c59d02c-1604-4a5b-bcfa-113ae35bb6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, ConvLSTM2D, BatchNormalization, Conv2D, Flatten, Dense\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "import tensorflow as tf\n",
    "\n",
    "# Build the model\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# Define the simplified model\n",
    "model = models.Sequential()\n",
    "\n",
    "model.add(layers.ConvLSTM2D(\n",
    "    filters=32,\n",
    "    kernel_size=(3, 3),\n",
    "    padding=\"same\",\n",
    "    return_sequences=False,  # Set this to False if you don't need the time dimension in subsequent layers\n",
    "    input_shape=(1, 363, 367, 1)  # timesteps, height, width, channels\n",
    "))\n",
    "\n",
    "# Flatten the time dimension if it still exists\n",
    "# This step is not necessary if return_sequences=False\n",
    "model.add(layers.Reshape((363, 367, 32)))  # Reshape to (height, width, channels)\n",
    "\n",
    "# Add final Conv2D layer to match y_true's shape\n",
    "model.add(layers.Conv2D(\n",
    "    filters=1,  # Single channel output\n",
    "    kernel_size=(3, 3),\n",
    "    activation=\"linear\",\n",
    "    padding=\"same\"\n",
    "))\n",
    "\n",
    "def custom_loss(y_true, y_pred):\n",
    "    return tf.reduce_mean(tf.square(y_true - y_pred), axis=-1)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=\"adam\", loss=custom_loss)\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train)).batch(4)\n",
    "valid_dataset = tf.data.Dataset.from_tensor_slices((X_valid, y_valid)).batch(4)\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(4)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    epochs=10,  # Reduce epochs for testing\n",
    "    validation_data=valid_dataset\n",
    ")\n",
    "\n",
    "# model = Sequential([\n",
    "#     layers.ConvLSTM2D(filters=32, kernel_size=(3, 3), input_shape=(1, 363, 367, 1), padding='same', return_sequences=True),\n",
    "#     #layers.BatchNormalization(),\n",
    "#     layers.ConvLSTM2D(filters=32, kernel_size=(3, 3), padding='same', return_sequences=True),\n",
    "#     #layers.BatchNormalization(),\n",
    "#     layers.ConvLSTM2D(filters=32, kernel_size=(3, 3), padding='same', return_sequences=False),\n",
    "#     #layers.BatchNormalization(),\n",
    "#     layers.Conv2D(filters=1, kernel_size=(3, 3), activation='linear', padding='same')\n",
    "# ])\n",
    "\n",
    "# Compile the model\n",
    "#model.compile(optimizer='adam', loss='mse')#, metrics=['mae','mse','accuracy',\\\n",
    "                                            #         keras.metrics.RootMeanSquaredError(),\\\n",
    "                                             #        'mean_absolute_percentage_error','cosine_similarity'])\n",
    "\n",
    "# def custom_loss(y_true, y_pred):\n",
    "#     # Ensure that y_pred matches y_true's shape directly\n",
    "#     return tf.reduce_mean(tf.square(y_true - y_pred), axis=-1)\n",
    "\n",
    "# model.compile(optimizer=\"adam\", loss=custom_loss)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff183687-e01c-4d54-bdeb-b075ece06a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train)).batch(8)\n",
    "valid_dataset = tf.data.Dataset.from_tensor_slices((X_valid, y_valid)).batch(8)\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87b7bee-695d-48dc-bbaa-048443ef3225",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train = X_train[..., np.newaxis]  # Ensures that shape is (batch_size, time_steps, height, width, channels)\n",
    "# y_train = y_train[..., np.newaxis]\n",
    "X_train.shape, X_valid.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd35106b-6fb7-43ba-b248-d6ffd43a5324",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape, y_valid.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec53cac5-f251-44fe-86ab-502a85122824",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check and print the shapes of the datasets\n",
    "for X_batch, y_batch in train_dataset.take(1):\n",
    "    print(f'X_train batch shape: {X_batch.shape}')\n",
    "    print(f'y_train batch shape: {y_batch.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5660fd1-6ca4-45a6-9e78-0e8fadb5fcdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True),\n",
    "    tf.keras.callbacks.ModelCheckpoint('best_model.keras', save_best_only=True)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137a5daa-0aaa-4943-b3ef-7f2c00306b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up callbacks for early stopping and model checkpointing\n",
    "# callbacks = [\n",
    "#     tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True),\n",
    "#     tf.keras.callbacks.ModelCheckpoint('best_model.keras', save_best_only=True, monitor='val_loss')\n",
    "# ]\n",
    "# callbacks = [\n",
    "#     ModelCheckpoint('best_model.keras', save_best_only=True, monitor='val_loss', mode='min'),\n",
    "#     EarlyStopping(monitor='val_loss', patience=10, mode='min')\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92acda13-316e-4589-924e-c698e7d1b31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810ca00f-815c-41a7-9c58-8daa66508426",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "\n",
    "# from tensorflow.keras.mixed_precision import set_global_policy\n",
    "# set_global_policy('float32')\n",
    "\n",
    "# train_dataset = train_dataset.batch(8)\n",
    "# valid_dataset = valid_dataset.batch(8)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_dataset, \n",
    "    epochs=50,  # Increase this value if the model needs more training\n",
    "    validation_data=valid_dataset, \n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ed9395-9b5c-4976-b0d9-1b4b71fdec4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test set\n",
    "test_loss, test_mae = model.evaluate(test_dataset)\n",
    "print(f'Test Loss: {test_loss}, Test MAE: {test_mae}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83fe5a0a-7116-41f7-91b4-8c51b34eba65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "predictions = model.predict(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae4b57b-1def-46c4-9cd0-b22ee3f180bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras_tuner as kt\n",
    "\n",
    "def model_builder(hp):\n",
    "    model = models.Sequential()\n",
    "    \n",
    "    # ConvLSTM layer\n",
    "    model.add(layers.ConvLSTM2D(\n",
    "        filters=hp.Int('filters', min_value=32, max_value=128, step=16),\n",
    "        kernel_size=(3, 3), \n",
    "        input_shape=input_shape, \n",
    "        padding='same', \n",
    "        return_sequences=True))\n",
    "    \n",
    "    model.add(layers.BatchNormalization())\n",
    "    \n",
    "    model.add(layers.ConvLSTM2D(\n",
    "        filters=hp.Int('filters', min_value=32, max_value=128, step=16),\n",
    "        kernel_size=(3, 3), \n",
    "        padding='same', \n",
    "        return_sequences=False))\n",
    "    \n",
    "    model.add(layers.BatchNormalization())\n",
    "    \n",
    "    model.add(layers.Conv2D(\n",
    "        filters=hp.Int('filters', min_value=32, max_value=128, step=16),\n",
    "        kernel_size=(3, 3), \n",
    "        padding='same', \n",
    "        activation='relu'))\n",
    "    \n",
    "    model.add(layers.Flatten())\n",
    "    \n",
    "    model.add(layers.Dense(hp.Int('units', min_value=32, max_value=128, step=16), activation='relu'))\n",
    "    \n",
    "    model.add(layers.Dense(1, activation='linear'))\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "tuner = kt.Hyperband(\n",
    "    model_builder,\n",
    "    objective='val_loss',\n",
    "    max_epochs=50,\n",
    "    factor=3,\n",
    "    directory='my_dir',\n",
    "    project_name='conv_lstm_tuning'\n",
    ")\n",
    "\n",
    "tuner.search(train_dataset, epochs=50, validation_data=valid_dataset)\n",
    "\n",
    "# Get the best model and hyperparameters\n",
    "best_model = tuner.get_best_models(num_models=1)[0]\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(f'Best hyperparameters: {best_hps.values}')\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "best_model.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc0dbc4-e855-4489-8952-ca3f068523db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75bea420-a781-4391-8eb6-8369bac7ac47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2988e283-a273-4d31-ac07-49fe4958e63e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6af29f-83ce-4c6f-9a02-204b3ee6dac7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b99b9d5-d6a7-468d-b04c-344006354633",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc10a91-5384-4964-bc40-56dd7f5e9c03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb698c55-d97e-4398-b889-a6bff765b03c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afee16e5-941c-43ec-b1b9-568761189f5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6a23c2-3e26-42f9-8c52-4c3c00113e8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a941701c-5c0e-46b4-969f-25d5f8a48def",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27593549-e61b-4d03-a81c-2340ea51ca58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b2ca14-0628-4a80-ae15-113083ca4121",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbafb4dd-6b28-48b0-a653-214ed7af1d1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97055ca-b2d0-444c-a779-7f8e8ce232c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd94b3f-de1c-4f69-b0aa-20d69f0ff93f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4018ce22-123a-4288-8ede-26c121c1d1cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26544c10-0d3d-4234-a495-599f9e54791d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4b67d2-913d-4e53-805a-1d5821985e1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bad4766-65c2-43dd-828c-e124c81b9351",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_array = pivot_df.values.reshape((len(pivot_df.index), len(pivot_df.columns.levels[0]), len(pivot_df.columns.levels[1]), 1))\n",
    "\n",
    "print(alt_array.shape)  # Expected shape: (n_datetimes, n_lats, n_lons, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1127b5b9-6a51-497d-82aa-b4f56c0a43a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Expand dimensions for LSTM or Conv2DLSTM (if necessary)\n",
    "# Assuming we want to add a time dimension:\n",
    "X = np.expand_dims(alt_array, axis=1)  # Shape becomes (n_datetimes, 1, n_lats, n_lons, 1)\n",
    "\n",
    "# Or stack multiple time steps if needed\n",
    "# Example: Create sequences of length 10 (timesteps)\n",
    "X = np.stack([alt_array[i:i+10] for i in range(len(alt_array)-9)], axis=0)  # Shape becomes (n_sequences, 10, n_lats, n_lons, 1)\n",
    "\n",
    "# Y would be the next time step's alt value (for predictive modeling)\n",
    "Y = alt_array[10:]  # Corresponding target values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb34477e-a3dd-4b73-9f20-a02f640e817a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import ConvLSTM2D, Dense, Flatten\n",
    "\n",
    "# Example: ConvLSTM2D Model\n",
    "model = Sequential([\n",
    "    ConvLSTM2D(filters=32, kernel_size=(3, 3), activation='relu', input_shape=(X.shape[1], X.shape[2], X.shape[3], X.shape[4])),\n",
    "    Flatten(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(1, activation='linear')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "model.fit(X, Y, epochs=10, batch_size=32, validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d97ad87-9796-4f49-b15d-38cd8a65d11c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea55a3c3-2cdd-40d9-8587-b6468e771a4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9ec5c5-0347-4500-b6a2-04848633f85e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee9d77b-8840-4dc8-bab2-e11d5576738f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dts = tf.convert_to_tensor(alt_train.datetime.values.astype(np.float32))\n",
    "lat = tf.convert_to_tensor(alt_train.lat.values.astype(np.float32))\n",
    "lon = tf.convert_to_tensor(alt_train.lon.values.astype(np.float32))\n",
    "alt = tf.convert_to_tensor(alt_train.alt.values.astype(np.float32))\n",
    "\n",
    "inputs = tf.stack([dts, lat, lon], axis=1)\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((inputs, alt))\n",
    "\n",
    "# Define batch size\n",
    "batch_size = 32\n",
    "\n",
    "# Shuffle and batch the dataset\n",
    "dataset = dataset.shuffle(buffer_size=len(alt_train)).batch(batch_size)\n",
    "\n",
    "train_dataset = dataset.take(1104980)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc643d4f-84e4-4a38-a45a-8afbb09f19db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7114af95-67e3-4a29-b9aa-350c366e9888",
   "metadata": {},
   "outputs": [],
   "source": [
    "dts = tf.convert_to_tensor(alt_valid.datetime.values.astype(np.float32))\n",
    "lat = tf.convert_to_tensor(alt_valid.lat.values.astype(np.float32))\n",
    "lon = tf.convert_to_tensor(alt_valid.lon.values.astype(np.float32))\n",
    "alt = tf.convert_to_tensor(alt_valid.alt.values.astype(np.float32))\n",
    "\n",
    "inputs = tf.stack([dts, lat, lon], axis=1)\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((inputs, alt))\n",
    "\n",
    "# Define batch size\n",
    "batch_size = 32\n",
    "\n",
    "# Shuffle and batch the dataset\n",
    "dataset = dataset.shuffle(buffer_size=len(alt_valid)).batch(batch_size)\n",
    "\n",
    "valid_dataset = dataset.take(36708)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9df2e0-0649-4f03-8f80-c7588d1060cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(64, activation='relu', input_shape=(3,)),  # Assuming datetime, lat, lon as inputs\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dense(1)  # Output is alt\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0fb9457-7a46-4404-8452-4fe0eb12b79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model using the training and validation datasets\n",
    "history = model.fit(train_dataset, epochs=10, validation_data=valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255323b8-2bd1-47be-9ccf-af86b8e04edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test dataset\n",
    "test_loss, test_mae = model.evaluate(test_dataset)\n",
    "print(f'Test Loss: {test_loss}, Test MAE: {test_mae}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f72a4f0f-156b-4319-a1ac-c02882bb8a9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28da810f-68b7-4270-8f92-bfc0bd26ebe0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095884f9-89e1-4917-8e5e-c3ce6d89b884",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(hp):\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(ConvLSTM2D(\n",
    "        filters=hp.Int('filters_1', min_value=16, max_value=64, step=16), \n",
    "        kernel_size=hp.Choice('kernel_size_1', values=[1, 1]),\n",
    "        input_shape=(12, 1, 1, 360),\n",
    "        #input_shape=(X_train.shape[1], 1, 1, X_train.shape[3]),\n",
    "        padding='same', \n",
    "        return_sequences=True,\n",
    "        activation=hp.Choice('activation_function_1', values=['relu','tanh','softmax','sigmoid','leaky_relu','swish'])\n",
    "    ))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(ConvLSTM2D(\n",
    "        filters=hp.Int('filters_2', min_value=16, max_value=64, step=16), \n",
    "        kernel_size=hp.Choice('kernel_size_2', values=[1, 1]),\n",
    "        padding='same', \n",
    "        return_sequences=True,\n",
    "        activation=hp.Choice('activation_function_2', values=['relu','tanh','softmax','sigmoid','leaky_relu','swish'])\n",
    "    ))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    # Extract the last time step\n",
    "    model.add(Lambda(lambda x: x[:, -1, :, :, :]))  # Shape becomes [batch_size, height, width, channels]\n",
    "\n",
    "    # Conv2D Layer for output (single frame prediction)\n",
    "    model.add(Conv2D(\n",
    "        filters=1,  # Ensure only one output channel to match y_train's shape\n",
    "        kernel_size=(1, 1),  # Use a 2D kernel size\n",
    "        activation=hp.Choice('activation_function_3', values=['relu','tanh','softmax','sigmoid','leaky_relu','swish']),\n",
    "        padding='same',\n",
    "        data_format='channels_last'\n",
    "    ))\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(\n",
    "            hp.Choice('learning_rate', values=[1e-3, 1e-4, 1e-5])\n",
    "        ),\n",
    "        loss='mse',\n",
    "        metrics=['mae','mse','mape','accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "    \n",
    "batch_size = 32  # Adjust based on memory and GPU capability\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "train_dataset = train_dataset.cache().shuffle(1000).batch(batch_size).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "valid_dataset = tf.data.Dataset.from_tensor_slices((X_valid, y_valid))\n",
    "valid_dataset = valid_dataset.batch(batch_size).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
    "test_dataset = test_dataset.batch(batch_size).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "# Define callbacks\n",
    "early_stopping_cb = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "lr_scheduler_cb = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2)\n",
    "checkpoint_cb = ModelCheckpoint('model_checkpoint.keras', save_best_only=True)\n",
    "\n",
    "from keras_tuner import BayesianOptimization\n",
    "\n",
    "# Define the tuner\n",
    "tuner = BayesianOptimization(\n",
    "    build_model,\n",
    "    objective='val_loss',\n",
    "    max_trials=10,\n",
    "    executions_per_trial=1,\n",
    "    directory='alt_train',\n",
    "    project_name='alt_train_conv3dlstm_tuning',\n",
    "    #directory='ch4_train',\n",
    "    #project_name='ch4_train_conv3dlstm_tuning',\n",
    "    overwrite=True,\n",
    ")\n",
    "\n",
    "tuner.search_space_summary()\n",
    "\n",
    "X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1, 1, X_train.shape[3]))\n",
    "X_valid = X_valid.reshape((X_valid.shape[0], X_valid.shape[1], 1, 1, X_valid.shape[3]))\n",
    "X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1, 1, X_test.shape[3]))\n",
    "\n",
    "#tuner.search(train_dataset, epochs=100, validation_data=valid_dataset, callbacks=[early_stopping_cb, lr_scheduler_cb])\n",
    "tuner.search(X_train, y_train, epochs=100, validation_data=(X_valid, y_valid), callbacks=[early_stopping_cb, lr_scheduler_cb])\n",
    "\n",
    "best_model = tuner.get_best_models(num_models=1)[0]\n",
    "\n",
    "plot_model(best_model, to_file='/Volumes/JPL/alt_model.png', show_shapes=True, show_layer_names=True, dpi=300)\n",
    "\n",
    "best_model.summary()\n",
    "\n",
    "tuner.results_summary()\n",
    "\n",
    "# Evaluate the best model on the test data\n",
    "test_loss = best_model.evaluate(X_test, y_test)\n",
    "print(f'Test Loss: {test_loss}')\n",
    "\n",
    "#history=# Continue training (fine-tuning) the best model with additional data or epochs\n",
    "history = best_model.fit(X_train, y_train, epochs=epochs, validation_data=(X_valid, y_valid), \n",
    "                 epochs2=additional_epochs, validation_data=(X_valid, y_valid))\n",
    "\n",
    "final_test_loss, final_test_mae = best_model.evaluate(train_dataset)\n",
    "print(f\"Final Test Loss: {final_test_loss}, Final Test MAE: {final_test_mae}\")\n",
    "\n",
    "predictions = best_model.predict(test_dataset)\n",
    "\n",
    "# If using a scaler, inverse transform the predictions and actual values\n",
    "predictions_original = scaler_alt.inverse_transform(predictions.reshape(-1, 1)).reshape(predictions.shape)\n",
    "actual_values_original = scaler_alt.inverse_transform(y_test.reshape(-1, 1)).reshape(y_test.shape)\n",
    "\n",
    "# Compare first few predictions with actual values\n",
    "for i in range(5):\n",
    "    print(f\"Prediction {i+1}: {predictions_original[i, :, :, 0]}\")\n",
    "    print(f\"Actual Value {i+1}: {actual_values_original[i, :, :, 0]}\")\n",
    "    \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title('Predicted (Original Scale)')\n",
    "plt.imshow(predictions_original[0, :, :, 0], cmap='viridis')\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title('Actual (Original Scale)')\n",
    "plt.imshow(actual_values_original[0, :, :, 0], cmap='viridis')\n",
    "plt.colorbar()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34129aa9-5e8b-4906-bb91-5a7e46ce3a44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf408c7b-de85-4eec-8b18-607ec9cbc250",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b619c9-3d64-4d81-bd5a-e32db87c37ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8314552-fcc3-42b1-ada5-23f375fb4bb8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94074787-6140-408b-85b4-56bc4312858c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a77ae6-cd3e-4b3a-907f-5dbfa6e29f5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e97314-b57a-4318-a0fa-6c9e7e039780",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7ab1abc6-475e-409f-8484-2bcff8389329",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb1a951-6025-4436-a424-47a9dfbc12c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataframe(df, variable_name):\n",
    "    # Mask out rows where alt is 0 or NaN\n",
    "    mask = (df['alt'] != 0) & (~df['alt'].isna())\n",
    "    df_filtered = df[mask]\n",
    "    \n",
    "    # Ensure no NaN or 0 values in `alt`\n",
    "    assert df_filtered['alt'].isna().sum() == 0\n",
    "    assert (df_filtered['alt'] == 0).sum() == 0\n",
    "    \n",
    "    # Convert to array: datetime, lat, lon, variable_name\n",
    "    array = df_filtered[['datetime', 'lat', 'lon', variable_name]].values\n",
    "    return array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95a8a38-c389-4eb8-91e8-fb2cf5ffd9c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_train_array = process_dataframe(alt_train, 'alt')\n",
    "alt_valid_array = process_dataframe(alt_valid, 'alt')\n",
    "alt_test_array = process_dataframe(alt_test, 'alt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0694733a-caa9-438e-a971-088952d18ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_combined_array = np.vstack([alt_train_array, alt_valid_array, alt_test_array])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7660d3-31e4-4dd6-893c-3180f2e1065e",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_combined_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2186a67-0c06-46db-a4d9-ba3e2a0d69c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = pd.concat([alt_train, alt_valid, alt_test], ignore_index=True)\n",
    "\n",
    "# 2. Mask out rows where alt is 0 or NaN\n",
    "filtered_df = combined_df[(combined_df['alt'] != 0) & (combined_df['alt'].notna())]\n",
    "\n",
    "# 3. Extract year and month from datetime\n",
    "filtered_df['year'] = pd.to_datetime(filtered_df['datetime']).dt.year\n",
    "filtered_df['month'] = pd.to_datetime(filtered_df['datetime']).dt.month\n",
    "\n",
    "# 4. Group by year, month, lat, and lon\n",
    "grouped_df = filtered_df.groupby(['year', 'month', 'lat', 'lon'])\n",
    "\n",
    "# 5. Aggregate the alt values - here we take the mean as an example\n",
    "aggregated_df = grouped_df['alt'].mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24927efe-9e9e-4522-ba44-ae0986508c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_array = aggregated_df.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bfd2f00-3473-4a46-baa0-9e15e4dbbaf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pd.DataFrame(final_array, columns=['year', 'month', 'lat', 'lon', 'alt'])\n",
    "\n",
    "# 1. Create a grid for latitudes and longitudes\n",
    "unique_latitudes = sorted(final_df['lat'].unique())\n",
    "unique_longitudes = sorted(final_df['lon'].unique())\n",
    "\n",
    "# 2. Pivot the DataFrame to organize the data into grid format for each year and month\n",
    "pivot_df = final_df.pivot_table(index=['year', 'month'], columns=['lat', 'lon'], values='alt')\n",
    "\n",
    "# 3. Reindex the pivot table to ensure it includes all latitude-longitude combinations\n",
    "pivot_df = pivot_df.reindex(pd.MultiIndex.from_product([unique_latitudes, unique_longitudes], names=['lat', 'lon']), axis=1)\n",
    "\n",
    "# 4. Fill missing values (you can fill with 0, NaN, or use interpolation)\n",
    "pivot_df = pivot_df.fillna(0)  # or .fillna(np.nan) or .interpolate()\n",
    "\n",
    "# 5. Confirm the shape before reshaping\n",
    "print(\"Shape of pivot_df before reshaping:\", pivot_df.shape)\n",
    "\n",
    "# 6. Reshape the data correctly based on the existing pivot_df shape\n",
    "# Here, len(unique_latitudes) * len(unique_longitudes) should equal 133221\n",
    "rows = len(unique_latitudes)\n",
    "cols = len(unique_longitudes)\n",
    "\n",
    "# Reshape pivot_df values to match (samples, rows, cols, channels=1)\n",
    "data_4d = pivot_df.values.reshape((len(pivot_df), rows, cols, 1))\n",
    "\n",
    "# Since the samples are already in a single timestep, we don't need further reshaping\n",
    "# Final shape should be (samples, 1, rows, cols, channels)\n",
    "conv2dlstm_input = data_4d.reshape((len(pivot_df), 1, rows, cols, 1))\n",
    "\n",
    "# Verify the shape\n",
    "print(\"Final Conv2DLSTM input shape:\", conv2dlstm_input.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79aa0b28-9da2-4e74-a9eb-0c4bc26fead9",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv2dlstm_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114abc4e-6028-46fc-adf1-6ca4c0ed8500",
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot_df_train=pivot_df[:2628]\n",
    "print(\"Shape of pivot_df before reshaping:\", pivot_df_train.shape)\n",
    "rows = len(unique_latitudes)\n",
    "cols = len(unique_longitudes)\n",
    "data_4d_train = pivot_df_train.values.reshape((len(pivot_df_train), rows, cols, 1))\n",
    "conv2dlstm_input_train = data_4d_train.reshape((len(pivot_df_train), 1, rows, cols, 1))\n",
    "print(\"Final Conv2DLSTM input shape:\", conv2dlstm_input_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06053ce-e79a-4eee-b091-54c88ca4905d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot_df_valid=pivot_df[2628:3039]\n",
    "print(\"Shape of pivot_df before reshaping:\", pivot_df_valid.shape)\n",
    "rows = len(unique_latitudes)\n",
    "cols = len(unique_longitudes)\n",
    "data_4d_valid = pivot_df_valid.values.reshape((len(pivot_df_valid), rows, cols, 1))\n",
    "conv2dlstm_input_valid = data_4d_valid.reshape((len(pivot_df_valid), 1, rows, cols, 1))\n",
    "print(\"Final Conv2DLSTM input shape:\", conv2dlstm_input_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb3e1a8-cbd4-4cab-b687-7b420c998608",
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot_df_test=pivot_df[3039:]\n",
    "print(\"Shape of pivot_df before reshaping:\", pivot_df_test.shape)\n",
    "rows = len(unique_latitudes)\n",
    "cols = len(unique_longitudes)\n",
    "data_4d_test = pivot_df_test.values.reshape((len(pivot_df_test), rows, cols, 1))\n",
    "conv2dlstm_input_test = data_4d_test.reshape((len(pivot_df_test), 1, rows, cols, 1))\n",
    "print(\"Final Conv2DLSTM input shape:\", conv2dlstm_input_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ee3cf6-6c18-41bc-b5f6-473e98a312dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = conv2dlstm_input_train\n",
    "y_train = X_train[:, :, :, :, 0]\n",
    "y_train = y_train.reshape((y_train.shape[0], y_train.shape[1], y_train.shape[2], y_train.shape[3], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03bfa2b4-29ab-4f7f-8552-2fcba1a1d80e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid = conv2dlstm_input_valid\n",
    "y_valid = X_valid[:, :, :, :, 0]\n",
    "y_valid = y_valid.reshape((y_valid.shape[0], y_valid.shape[1], y_valid.shape[2], y_valid.shape[3], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6532b82a-f16f-4e15-b54d-857c7e4b0604",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = conv2dlstm_input_test\n",
    "y_test = X_test[:, :, :, :, 0]\n",
    "y_test = y_test.reshape((y_test.shape[0], y_test.shape[1], y_test.shape[2], y_test.shape[3], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f13273-41c6-40a9-b7de-9c55c3dcad48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_train = y_train.reshape((y_train.shape[0], 1, 363, 367, 1))\n",
    "# y_valid = y_valid.reshape((y_valid.shape[0], 1, 363, 367, 1))\n",
    "# y_test = y_test.reshape((y_test.shape[0], 1, 363, 367, 1))\n",
    "\n",
    "# y_train = y_train.reshape(-1, 1)\n",
    "# y_valid = y_valid.reshape(-1, 1)\n",
    "# y_test = y_test.reshape(-1, 1)\n",
    "\n",
    "# # Expanding the labels across the spatial dimensions\n",
    "# y_train_tiled = y_train[:, np.newaxis, np.newaxis, np.newaxis, np.newaxis]\n",
    "# y_train_tiled = np.tile(y_train_tiled, (1, 1, 363, 367, 1))\n",
    "\n",
    "# y_valid_tiled = y_valid[:, np.newaxis, np.newaxis, np.newaxis, np.newaxis]\n",
    "# y_valid_tiled = np.tile(y_valid_tiled, (1, 1, 363, 367, 1))\n",
    "\n",
    "# y_test_tiled = y_test[:, np.newaxis, np.newaxis, np.newaxis, np.newaxis]\n",
    "# y_test_tiled = np.tile(y_test_tiled, (1, 1, 363, 367, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb60baa-21a9-4a17-bfa6-9b70b6ab7d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"X_valid shape:\", X_valid.shape)\n",
    "print(\"y_valid shape:\", y_valid.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_test shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff8f10a-5407-41b3-8ebb-6cc80aabd054",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Assuming y_train, y_valid, y_test are scalar values per image\n",
    "# y_train_reshaped = np.tile(y_train[:, np.newaxis, np.newaxis], (1, 363, 367))\n",
    "# y_train_reshaped = y_train_reshaped.reshape(-1, 1, 363, 367, 1)\n",
    "\n",
    "# y_valid_reshaped = np.tile(y_valid[:, np.newaxis, np.newaxis], (1, 363, 367))\n",
    "# y_valid_reshaped = y_valid_reshaped.reshape(-1, 1, 363, 367, 1)\n",
    "\n",
    "# y_test_reshaped = np.tile(y_test[:, np.newaxis, np.newaxis], (1, 363, 367))\n",
    "# y_test_reshaped = y_test_reshaped.reshape(-1, 1, 363, 367, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4932d7-ace6-4d04-a783-f72dc16e09d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Use a small subset of the data for quick debugging\n",
    "# X_train_small = X_train[:100]\n",
    "# y_train_small = y_train[:100]\n",
    "# X_valid_small = X_valid[:20]\n",
    "# y_valid_small = y_valid[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a840311-fce6-43f3-bb3e-367cecf75472",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc1cca4-349e-4ece-8e6c-29ec51658017",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# import tensorflow_probability as tfp\n",
    "\n",
    "# tfpl = tfp.layers\n",
    "\n",
    "# def build_bnn_model():\n",
    "#     input_layer = tf.keras.layers.Input(shape=(X_train.shape[1:]))  # 5D input shape\n",
    "    \n",
    "#     # Use ConvLSTM2D directly for the 5D input\n",
    "#     x = tf.keras.layers.ConvLSTM2D(filters=32, kernel_size=(3, 3), padding='same', return_sequences=False, activation='relu')(input_layer)\n",
    "    \n",
    "#     # Flatten the output before passing it to the DenseVariational layer\n",
    "#     x = tf.keras.layers.Flatten()(x)\n",
    "    \n",
    "#     # Manually create a variational distribution\n",
    "#     def posterior_mean_field(kernel_size, bias_size=0, dtype=None):\n",
    "#         n = kernel_size + bias_size\n",
    "#         c = np.log(np.expm1(1.))\n",
    "#         return tf.keras.Sequential([\n",
    "#             tfpl.VariableLayer(2 * n, dtype=dtype),\n",
    "#             tfpl.DistributionLambda(lambda t: tfp.distributions.Independent(\n",
    "#                 tfp.distributions.Normal(loc=t[..., :n],\n",
    "#                                          scale=1e-5 + tf.nn.softplus(c + t[..., n:])),\n",
    "#                 reinterpreted_batch_ndims=1)),\n",
    "#         ])\n",
    "\n",
    "#     def prior_trainable(kernel_size, bias_size=0, dtype=None):\n",
    "#         n = kernel_size + bias_size\n",
    "#         return tf.keras.Sequential([\n",
    "#             tfpl.VariableLayer(n, dtype=dtype),\n",
    "#             tfpl.DistributionLambda(lambda t: tfp.distributions.Independent(\n",
    "#                 tfp.distributions.Normal(loc=t, scale=1),\n",
    "#                 reinterpreted_batch_ndims=1)),\n",
    "#         ])\n",
    "    \n",
    "#     output_layer = tfpl.DenseVariational(1, make_posterior_fn=posterior_mean_field, make_prior_fn=prior_trainable)(x)\n",
    "    \n",
    "#     model = tf.keras.Model(inputs=input_layer, outputs=output_layer)\n",
    "    \n",
    "#     model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "    \n",
    "#     return model\n",
    "\n",
    "# # Train the model\n",
    "# bnn_model = build_bnn_model()\n",
    "# bnn_model.fit(X_train, y_train, validation_data=(X_valid, y_valid), epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d5f391-d022-47c2-9c61-58bacdacda31",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# # Predict with the BNN\n",
    "# n_samples = 100  # Number of samples from the posterior distribution\n",
    "# predictions = np.array([bnn_model.predict(X_test, batch_size=32, verbose=0) for _ in range(n_samples)])\n",
    "\n",
    "# # Calculate mean and uncertainty (standard deviation) of predictions\n",
    "# predictions_mean = np.mean(predictions, axis=0)\n",
    "# predictions_std = np.std(predictions, axis=0)  # Uncertainty estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a50fa7c-9b01-4ef7-9ce6-8dd628881ac4",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# from tensorflow.keras import layers\n",
    "\n",
    "# # Define a sampling layer for the VAE\n",
    "# class Sampling(layers.Layer):\n",
    "#     def call(self, inputs):\n",
    "#         z_mean, z_log_var = inputs\n",
    "#         batch = tf.shape(z_mean)[0]\n",
    "#         dim = tf.shape(z_mean)[1]\n",
    "#         epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
    "#         return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "# # Build the VAE model\n",
    "# def build_vae_model(latent_dim=2):\n",
    "#     # Encoder\n",
    "#     input_layer = tf.keras.layers.Input(shape=(X_train.shape[1:]))\n",
    "    \n",
    "#     x = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', strides=2, padding='same')(input_layer)\n",
    "#     x = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', strides=2, padding='same')(x)\n",
    "#     x = tf.keras.layers.Flatten()(x)\n",
    "#     x = tf.keras.layers.Dense(16, activation='relu')(x)\n",
    "    \n",
    "#     z_mean = tf.keras.layers.Dense(latent_dim)(x)\n",
    "#     z_log_var = tf.keras.layers.Dense(latent_dim)(x)\n",
    "#     z = Sampling()([z_mean, z_log_var])\n",
    "    \n",
    "#     encoder = tf.keras.Model(input_layer, [z_mean, z_log_var, z], name=\"encoder\")\n",
    "    \n",
    "#     # Decoder\n",
    "#     latent_inputs = tf.keras.layers.Input(shape=(latent_dim,))\n",
    "#     x = tf.keras.layers.Dense(7 * 7 * 64, activation='relu')(latent_inputs)\n",
    "#     x = tf.keras.layers.Reshape((7, 7, 64))(x)\n",
    "#     x = tf.keras.layers.Conv2DTranspose(64, (3, 3), activation='relu', strides=2, padding='same')(x)\n",
    "#     x = tf.keras.layers.Conv2DTranspose(32, (3, 3), activation='relu', strides=2, padding='same')(x)\n",
    "#     output_layer = tf.keras.layers.Conv2DTranspose(1, (3, 3), activation='sigmoid', padding='same')(x)\n",
    "    \n",
    "#     decoder = tf.keras.Model(latent_inputs, output_layer, name=\"decoder\")\n",
    "    \n",
    "#     # VAE Model\n",
    "#     outputs = decoder(encoder(input_layer)[2])\n",
    "#     vae = tf.keras.Model(input_layer, outputs, name=\"vae\")\n",
    "    \n",
    "#     reconstruction_loss = tf.keras.losses.mse(input_layer, outputs)\n",
    "#     reconstruction_loss *= X_train.shape[1] * X_train.shape[2]\n",
    "    \n",
    "#     kl_loss = 1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var)\n",
    "#     kl_loss = tf.reduce_mean(kl_loss) * -0.5\n",
    "    \n",
    "#     vae_loss = tf.reduce_mean(reconstruction_loss + kl_loss)\n",
    "#     vae.add_loss(vae_loss)\n",
    "#     vae.compile(optimizer='adam')\n",
    "    \n",
    "#     return vae\n",
    "\n",
    "# # Train the VAE model\n",
    "# vae_model = build_vae_model()\n",
    "# vae_model.fit(X_train, y_train, validation_data=(X_valid, y_valid), epochs=10, batch_size=32)\n",
    "\n",
    "# # Predict with the VAE and obtain uncertainty in the latent space\n",
    "# z_mean, z_log_var, _ = vae_model.layers[1](X_test)  # Encoder output\n",
    "# uncertainty = tf.exp(0.5 * z_log_var)  # This gives the uncertainty in the latent space\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64306794-5e1a-425b-97fa-485d0ff046b8",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# def build_mc_dropout_model():\n",
    "#     input_layer = tf.keras.layers.Input(shape=(X_train.shape[1:]))\n",
    "    \n",
    "#     # Example Conv2DLSTM Layer\n",
    "#     x = tf.keras.layers.ConvLSTM2D(filters=32, kernel_size=(3, 3), activation='relu', return_sequences=True)(input_layer)\n",
    "#     x = MCDropout(0.5)(x)\n",
    "    \n",
    "#     x = tf.keras.layers.ConvLSTM2D(filters=32, kernel_size=(3, 3), activation='relu')(x)\n",
    "#     x = MCDropout(0.5)(x)\n",
    "    \n",
    "#     x = tf.keras.layers.Flatten()(x)\n",
    "#     x = tf.keras.layers.Dense(1)(x)  # Output a single value\n",
    "    \n",
    "#     output_layer = tf.keras.layers.Reshape((1,))(x)  # Ensure the output shape is (batch_size, 1)\n",
    "    \n",
    "#     model = tf.keras.Model(inputs=input_layer, outputs=output_layer)\n",
    "    \n",
    "#     model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "    \n",
    "#     return model\n",
    "\n",
    "# # Now train the model\n",
    "# mc_dropout_model = build_mc_dropout_model()\n",
    "# mc_dropout_model.fit(X_train, y_train, validation_data=(X_valid, y_valid), epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fecba98-2034-4cc3-a4bb-bd707beb68f8",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# n_samples = 100  # Number of stochastic forward passes\n",
    "# predictions = np.array([mc_dropout_model.predict(X_test, batch_size=32, verbose=0) for _ in range(n_samples)])\n",
    "\n",
    "# # Calculate mean and uncertainty (standard deviation) of predictions\n",
    "# predictions_mean = np.mean(predictions, axis=0)\n",
    "# predictions_std = np.std(predictions, axis=0)  # This is your uncertainty estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59146320-627b-4b9a-a0cf-4a0d031d4473",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# # Example of combining predictions from the above models\n",
    "# combined_predictions = np.stack([predictions_mc, predictions_bnn, predictions_vae], axis=-1)\n",
    "# combined_mean = np.mean(combined_predictions, axis=-1)\n",
    "# combined_std = np.std(combined_predictions, axis=-1)  # Combined uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ab91d1-f271-4101-b56d-34da40661c3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98b1c0b-fcef-4648-bdc9-2ac30636f556",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from kerastuner.tuners import BayesianOptimization\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "import kerastuner as kt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40269878-9baf-4873-8e30-490cb0de8e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "\n",
    "model.add(layers.ConvLSTM2D(\n",
    "    filters=32,\n",
    "    kernel_size=(1, 1),\n",
    "    activation='relu',\n",
    "    input_shape=(X_train.shape[1], X_train.shape[2], X_train.shape[3], X_train.shape[4]),\n",
    "    return_sequences=False\n",
    "))\n",
    "\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='linear'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f45048-5145-4e5d-837e-91373ae67888",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_simple_model(hp):\n",
    "    model = models.Sequential()\n",
    "    \n",
    "    model.add(layers.ConvLSTM2D(\n",
    "        filters=32,\n",
    "        kernel_size=(3, 3),\n",
    "        activation='relu',\n",
    "        input_shape=(X_train.shape[1], X_train.shape[2], X_train.shape[3], X_train.shape[4]),\n",
    "        return_sequences=False\n",
    "    ))\n",
    "    \n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(64, activation='relu'))\n",
    "    model.add(layers.Dense(1, activation='linear'))\n",
    "    \n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(1e-3),\n",
    "                  loss='mse',\n",
    "                  metrics=['mae'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3348e5c-5224-4755-8841-65757be3e84a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61bfd0b-1b8e-4e73-9b84-31e5cd4dd54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "tuner = BayesianOptimization(\n",
    "    build_model,\n",
    "    objective='val_loss',\n",
    "    max_trials=20,\n",
    "    directory='/Volumes/JPL/alt_train_new',\n",
    "    project_name='conv_lstm_tuning'\n",
    ")\n",
    "\n",
    "# Print the search space summary\n",
    "tuner.search_space_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c971727c-9161-4f1a-b3f2-803de5c2e9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_valid shape:\", X_valid.shape)\n",
    "print(\"X_test shape:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c71eb31-b294-47ae-bcb5-a87feacac456",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb2a7ad-e654-4b99-be8d-513df567b9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.search(\n",
    "    X_train, \n",
    "    y_train,\n",
    "    epochs=10,\n",
    "    validation_data=(X_valid, y_valid)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a85aca0-80da-4181-9e86-81ac30d8642a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the optimal hyperparameters\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(f\"\"\"\n",
    "The optimal number of filters is {best_hps.get('filters')} \n",
    "The optimal number of units in the fully connected layer is {best_hps.get('units')}\n",
    "The optimal learning rate for the optimizer is {best_hps.get('learning_rate')}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53b3bbb-5499-4a6c-8d0a-7079a017733e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model with the best hyperparameters\n",
    "model = tuner.hypermodel.build(best_hps)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, \n",
    "                    epochs=50, \n",
    "                    validation_data=(X_valid, y_valid),\n",
    "                    batch_size=32)\n",
    "\n",
    "# Evaluate on test data\n",
    "test_loss, test_mae = model.evaluate(X_test, y_test)\n",
    "print(f'Test Loss: {test_loss}, Test MAE: {test_mae}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05667f3-1fa9-4ee4-8b0e-eadeb73dff42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_complex_model(hp):\n",
    "    model = models.Sequential()\n",
    "    \n",
    "    # Encoder: ConvLSTM layers\n",
    "    model.add(layers.ConvLSTM2D(\n",
    "        filters=hp.Int('encoder_filters', min_value=32, max_value=128, step=16),\n",
    "        kernel_size=(3, 3),\n",
    "        activation='relu',\n",
    "        input_shape=(X_train.shape[1], X_train.shape[2], X_train.shape[3], X_train.shape[4]),\n",
    "        return_sequences=True\n",
    "    ))\n",
    "    \n",
    "    model.add(layers.ConvLSTM2D(\n",
    "        filters=hp.Int('encoder_filters2', min_value=32, max_value=128, step=16),\n",
    "        kernel_size=(3, 3),\n",
    "        activation='relu',\n",
    "        return_sequences=False\n",
    "    ))\n",
    "    \n",
    "    # Decoder: Conv2DTranspose layers\n",
    "    model.add(layers.Conv2DTranspose(\n",
    "        filters=hp.Int('decoder_filters', min_value=32, max_value=128, step=16),\n",
    "        kernel_size=(3, 3),\n",
    "        activation='relu'\n",
    "    ))\n",
    "    \n",
    "    model.add(layers.Conv2DTranspose(\n",
    "        filters=1,\n",
    "        kernel_size=(3, 3),\n",
    "        activation='linear'\n",
    "    ))\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(\n",
    "                    hp.Choice('learning_rate', [1e-3, 1e-4])),\n",
    "                  loss='mse',\n",
    "                  metrics=['mae'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8af652-7044-4d98-b8eb-754020be445c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_with_uncertainty(hp):\n",
    "    model = models.Sequential()\n",
    "    \n",
    "    model.add(layers.ConvLSTM2D(\n",
    "        filters=hp.Int('filters', min_value=32, max_value=128, step=16),\n",
    "        kernel_size=(3, 3),\n",
    "        activation='relu',\n",
    "        input_shape=(X_train.shape[1], X_train.shape[2], X_train.shape[3], X_train.shape[4]),\n",
    "        return_sequences=True\n",
    "    ))\n",
    "    \n",
    "    model.add(layers.Dropout(0.5))  # Apply dropout\n",
    "    \n",
    "    model.add(layers.ConvLSTM2D(\n",
    "        filters=hp.Int('filters2', min_value=32, max_value=128, step=16),\n",
    "        kernel_size=(3, 3),\n",
    "        activation='relu',\n",
    "        return_sequences=False\n",
    "    ))\n",
    "    \n",
    "    model.add(layers.Dropout(0.5))  # Apply dropout\n",
    "    \n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(hp.Int('units', min_value=32, max_value=256, step=32), activation='relu'))\n",
    "    model.add(layers.Dense(1, activation='linear'))\n",
    "    \n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(hp.Choice('learning_rate', [1e-3, 1e-4])),\n",
    "                  loss='mse',\n",
    "                  metrics=['mae'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daacf5c6-d39f-45ad-9bc2-76c913b1e127",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1098450-9b17-4e3f-a20d-a05afd1c7e83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127cd348-62be-4257-bb06-801750d1ac62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481e1f94-7b0f-43bc-be4a-eacfc933223b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07a613c-794f-4506-a9de-5fb8f636b1d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f6e730-4d09-404d-84e1-4c3788734fb2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37bd7498-4e92-49f5-a460-c746b274fc4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d9f422-e873-4f6a-a41b-4b80bd0546cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2304f7-dd28-42c9-b129-3bdf0477e268",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1518b7-7b59-4ca3-ba3b-d4e6c7ceae29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be78c9b-da36-46c3-886a-5b240f71e863",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2de44d-97a2-4550-8fc6-16d3c808e410",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d9c0f8-1a73-4343-9829-ed510f82702e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import ConvLSTM2D, BatchNormalization, Conv3D, Conv2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import GlobalAveragePooling3D\n",
    "\n",
    "def model(hp):\n",
    "    model = Sequential()\n",
    "    \n",
    "    # ConvLSTM2D layer to process the sequence data\n",
    "    model.add(ConvLSTM2D(\n",
    "        #filters=hp.Int('filters_1', min_value=16, max_value=64, step=16), \n",
    "        filters = hp.Int('filters_1', min_value=8, max_value=32, step=8),\n",
    "        kernel_size=hp.Choice('kernel_size_1', values=[1, 3]),\n",
    "        padding='same', \n",
    "        return_sequences=True,\n",
    "        activation=hp.Choice('activation_function_1', values=['relu','tanh','sigmoid','leaky_relu','swish']),\n",
    "        input_shape=(12, 363, 367, 1)\n",
    "    )\n",
    "             )\n",
    "    \n",
    "    model.add(BatchNormalization())\n",
    "    \n",
    "    # Another ConvLSTM2D layer\n",
    "    model.add(ConvLSTM2D(\n",
    "        #filters=hp.Int('filters_2', min_value=16, max_value=32, step=16), \n",
    "        filters=32,\n",
    "        #kernel_size=hp.Choice('kernel_size_2', values=[1, 3]),\n",
    "        kernel_size=(1,1),\n",
    "        padding='same',\n",
    "        #activation=hp.Choice('activation_function_2', values=['relu','tanh','sigmoid','leaky_relu','swish']),\n",
    "        activation='leaky_relu',\n",
    "        return_sequences=False\n",
    "    )\n",
    "             )\n",
    "    \n",
    "    model.add(BatchNormalization())\n",
    "    \n",
    "    # # # Final ConvLSTM2D layer without return_sequences\n",
    "    # # model.add(ConvLSTM2D(\n",
    "    # #     #filters=hp.Int('filters_3', min_value=16, max_value=32, step=16), \n",
    "    # #     filters=32,\n",
    "    # #     #kernel_size=hp.Choice('kernel_size_3', values=[1, 3]),\n",
    "    # #     kernel_size=(1,1),\n",
    "    # #     padding='same', \n",
    "    # #     #activation=hp.Choice('activation_function_3', values=['relu','tanh','softmax','sigmoid','leaky_relu','swish']),\n",
    "    # #     activation='swish',\n",
    "    # #     return_sequences=False\n",
    "    # # )\n",
    "    # #          )\n",
    "    \n",
    "    # # model.add(BatchNormalization())\n",
    "\n",
    "    # model.add(Lambda(lambda x: x[:, -1, :, :, :]))  # Shape becomes [batch_size, height, width, channels]\n",
    "    \n",
    "    # # Use Conv2D to reduce to the target output shape\n",
    "    # model.add(Conv2D(\n",
    "    #         filters=1,\n",
    "    #         #kernel_size=hp.Choice('kernel_size_4', values=[1, 3]),\n",
    "    #         kernel_size=(1,1),\n",
    "    #         #activation=hp.Choice('activation_function_4', values=['relu','tanh','softmax','sigmoid','leaky_relu','swish']),\n",
    "    #         activation='softmax',\n",
    "    #         padding='same'\n",
    "    #         #data_format='channels_last'\n",
    "    # )\n",
    "    #          )\n",
    "\n",
    "    # model.add(Flatten())\n",
    "\n",
    "    model.add(GlobalAveragePooling2D())\n",
    "    \n",
    "    model.add(Dense(\n",
    "        units=hp.Int('units', min_value=64, max_value=256, step=64),\n",
    "        #units=hp.Int('filters_4', min_value=64, max_value=128, step=16),\n",
    "        #activation=hp.Choice('activation_function_5', values=['relu','tanh','softmax','sigmoid','leaky_relu','swish']),\n",
    "        activation='relu',\n",
    "    )\n",
    "             )\n",
    "    \n",
    "    model.add(Dense(\n",
    "        units=1, \n",
    "        activation='linear'\n",
    "    )\n",
    "             )\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(\n",
    "        optimizer=Adam(\n",
    "            #hp.Choice('learning_rate', values=[1e-3, 1e-4, 1e-5])\n",
    "            hp.Choice('learning_rate', values=[1e-3, 1e-4])\n",
    "        ),\n",
    "        loss='mse', \n",
    "        metrics=['mae','mse','accuracy']\n",
    "    )\n",
    "    \n",
    "    # Display the model's architecture\n",
    "    #model.summary()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254137cd-f716-4630-b2be-7f50481a0c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define callbacks\n",
    "early_stopping_cb = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "lr_scheduler_cb = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5)\n",
    "checkpoint_cb = ModelCheckpoint('model_checkpoint.keras', save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4a1f6b-3b8e-478e-89e7-da82cb47821d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_tuner import BayesianOptimization\n",
    "\n",
    "tuner = BayesianOptimization(\n",
    "    model,\n",
    "    objective='val_loss',\n",
    "    max_trials=5,\n",
    "    executions_per_trial=8,\n",
    "    directory='/Volumes/JPL/alt_train_new',\n",
    "    project_name='alt_train_conv3dlstm_tuning',\n",
    "    #directory='ch4_train',\n",
    "    #project_name='ch4_train_conv3dlstm_tuning',\n",
    "    overwrite=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e05bb7-68f2-4a56-a3ff-1800095360b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_size = y_train.size\n",
    "new_shape = (4, original_size // (4 * 1 * 1), 1, 1)\n",
    "y_train = y_train.reshape(new_shape)\n",
    "y_train.shape\n",
    "\n",
    "# batch_size = 4  # Number of samples per batch\n",
    "# height = 363    # Height of the image\n",
    "# width = 367     # Width of the image\n",
    "# channels = 1    # Number of channels\n",
    "\n",
    "# expected_size = batch_size * height * width * channels\n",
    "\n",
    "# # Check if expected size matches actual size\n",
    "# if expected_size == y_train.size:\n",
    "#     y_train = y_train.reshape((batch_size, height, width, channels))\n",
    "# else:\n",
    "#     print(\"Reshape dimensions do not match the array size. Adjust the dimensions accordingly.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d081c72-d523-40de-a9c8-39543158b30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if y_train.shape[0] == 4 and X_train.shape[0] == 2628:\n",
    "    y_train = np.repeat(y_train, 657, axis=0)  # Assuming 657 repetitions to match 2628 samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd218cef-6a76-412d-ad65-5a8b82253a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01180ab-f5d7-4af8-b525-5bcfbfca9a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.search(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    epochs=5,\n",
    "    batch_size = 4,\n",
    "    validation_data=(X_valid, y_valid),\n",
    "    verbose = 1\n",
    "    #callbacks=[early_stopping_cb, lr_scheduler_cb]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215b63cf-bd44-401c-a6a8-0fe2b3c1647c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.search(\n",
    "    X_train_small,\n",
    "    y_train_small,\n",
    "    epochs=5,\n",
    "    batch_size=4,\n",
    "    validation_data=(X_valid_small, y_valid_small),\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae778cf-0b3b-4e36-a8ae-bd56d9e86a80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b551dfa-c3de-4401-902b-6663ae7283f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Check for NaNs or infinities in the data\n",
    "assert not np.isnan(X_train).any(), \"X_train contains NaN values\"\n",
    "assert not np.isinf(X_train).any(), \"X_train contains infinite values\"\n",
    "assert not np.isnan(y_train).any(), \"y_train contains NaN values\"\n",
    "assert not np.isinf(y_train).any(), \"y_train contains infinite values\"\n",
    "\n",
    "# Repeat for validation data\n",
    "assert not np.isnan(X_valid).any(), \"X_valid contains NaN values\"\n",
    "assert not np.isinf(X_valid).any(), \"X_valid contains infinite values\"\n",
    "assert not np.isnan(y_valid).any(), \"y_valid contains NaN values\"\n",
    "assert not np.isinf(y_valid).any(), \"y_valid contains infinite values\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38bde1c-b1a2-420b-a4b0-60d61ac8cada",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f3e775-c282-41fe-ab92-9d9ea476585a",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt\n",
    "reshaped_df=alt[(alt['alt'] != 0) & (~alt['alt'].isna())].sort_values(by='datetime').reset_index(drop=True)\n",
    "reshaped_df['monthly_diff'] = reshaped_df.groupby(['lat', 'lon'])['alt'].diff()\n",
    "reshaped_df=reshaped_df.replace(0,np.nan).dropna()\n",
    "reshaped_df=reshaped_df.sort_values(by='datetime')\n",
    "reshaped_df=reshaped_df.reset_index(drop=True)\n",
    "reshaped_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15cf7e49-e59a-4c07-abd7-d5f427bcec66",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(np.unique(reshaped_df.datetime)), len(np.unique(reshaped_df.lat)),len(np.unique(reshaped_df.lon)), len(np.unique(reshaped_df.alt)),\\\n",
    "len(np.unique(reshaped_df.monthly_diff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2a9511-e2b9-48c8-8486-811cd3b76439",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(pd.to_datetime(np.unique(reshaped_df.datetime)).strftime('%Y-%m'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b706ae26-81d7-4f34-8d5f-7e36b3d64199",
   "metadata": {},
   "outputs": [],
   "source": [
    "-1.135481e-0-2.092486e-01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902e6c91-456c-4cc0-bdd5-c0b55dd251bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Triple-check for any remaining NaN values\n",
    "assert not reshaped_df.isna().any().any(), \"There are still NaN values in the DataFrame.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57f5298-f5e2-4a78-9628-48892975ab48",
   "metadata": {},
   "outputs": [],
   "source": [
    "reshaped_df['year'] = reshaped_df['datetime'].dt.year\n",
    "reshaped_df['month'] = reshaped_df['datetime'].dt.month\n",
    "\n",
    "unique_years = reshaped_df['year'].unique()\n",
    "unique_months = np.arange(1, 13)  # Since months should always be from 1 to 12\n",
    "unique_lats = np.sort(reshaped_df['lat'].unique())\n",
    "unique_lons = np.sort(reshaped_df['lon'].unique())\n",
    "\n",
    "ndarray = np.full((len(unique_years), len(unique_months), len(unique_lats), len(unique_lons), 1), np.nan)\n",
    "\n",
    "year_to_idx = {year: i for i, year in enumerate(unique_years)}\n",
    "month_to_idx = {month: i for i, month in enumerate(unique_months)}\n",
    "lat_to_idx = {lat: i for i, lat in enumerate(unique_lats)}\n",
    "lon_to_idx = {lon: i for i, lon in enumerate(unique_lons)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3bf9f5-a448-438d-acdb-cd3be1689e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _, row in reshaped_df.iterrows():\n",
    "    year_idx = year_to_idx[row['year']]\n",
    "    month_idx = month_to_idx[row['month']]\n",
    "    lat_idx = lat_to_idx[row['lat']]\n",
    "    lon_idx = lon_to_idx[row['lon']]\n",
    "    ndarray[year_idx, month_idx - 1, lat_idx, lon_idx, 0] = row['monthly_diff']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07bf4501-4c19-491a-8f59-0773626dd760",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert not np.isnan(ndarray).any(), \"There are still NaN values in the ndarray.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8485f105-cdec-401c-a3ee-11e86b1f53b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_indices = np.argwhere(np.isnan(ndarray))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ed9e19-3c1d-4cc7-9f1f-f44fdb6e794c",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_combinations = []\n",
    "for idx in nan_indices:\n",
    "    year = unique_years[idx[0]]\n",
    "    month = unique_months[idx[1]]\n",
    "    lat = unique_lats[idx[2]]\n",
    "    lon = unique_lons[idx[3]]\n",
    "    missing_combinations.append((year, month, lat, lon))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3467b517-e865-448d-afc7-0f7e139636c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Some missing (year, month, lat, lon) combinations:\")\n",
    "for combo in missing_combinations[:10]:  # Only show the first 10 for brevity\n",
    "    print(combo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431cd154-4b76-41d9-8092-27c97dca4fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.ndimage\n",
    "\n",
    "# Define a function to interpolate over missing values in a 4D ndarray\n",
    "def interpolate_nan_values(ndarray):\n",
    "    # Create an array of indices where NaNs are present\n",
    "    nan_mask = np.isnan(ndarray)\n",
    "    \n",
    "    # Use linear interpolation on the masked NaN values\n",
    "    ndarray_interpolated = scipy.ndimage.morphology.distance_transform_edt(~nan_mask, return_distances=False, return_indices=True)\n",
    "    ndarray_interpolated = ndarray[tuple(ndarray_interpolated)]\n",
    "    \n",
    "    return ndarray_interpolated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0573b2-712a-4bc0-9709-7219ddd25d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the interpolation\n",
    "ndarray_interpolated = interpolate_nan_values(ndarray)\n",
    "\n",
    "# Ensure no NaNs are left\n",
    "assert not np.isnan(ndarray_interpolated).any(), \"There are still NaN values in the interpolated ndarray.\"\n",
    "\n",
    "# Print the shape to confirm\n",
    "print(\"Shape of the interpolated ndarray:\", ndarray_interpolated.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ab7913-9baf-4a22-a9ac-f74294932fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for any remaining NaN values\n",
    "print(\"Any NaN values left?:\", np.cumsum(np.isnan(ndarray_interpolated).any()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1232c70b-c298-42b9-bf30-b423821a923e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the interpolation\n",
    "ndarray_interpolated = interpolate_nan_values(ndarray_interpolated)\n",
    "\n",
    "# Ensure no NaNs are left\n",
    "assert not np.isnan(ndarray_interpolated).any(), \"There are still NaN values in the interpolated ndarray.\"\n",
    "\n",
    "# Print the shape to confirm\n",
    "print(\"Shape of the interpolated ndarray:\", ndarray_interpolated.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568d7cf4-9b02-4b7a-85ab-189e5beb84e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_mask = ~np.isnan(ndarray)\n",
    "ndarray_valid = ndarray[valid_mask].reshape(-1, 5)  # Adjust dimensions as needed\n",
    "\n",
    "# Print the shape to confirm\n",
    "print(\"Shape of the valid-only ndarray:\", ndarray_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c5f205-26f1-40f5-a809-33cbc0f23b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the shape to confirm\n",
    "print(\"Shape of the reshaped ndarray:\", ndarray.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655159d8-ab9a-4d44-bc47-32873807c748",
   "metadata": {},
   "outputs": [],
   "source": [
    "ndarray_interpolated[0,0,:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6baccd-d9d5-4faf-b105-d4a02506071d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d7f39f-c8fc-42a8-a204-d9d0b1eb7349",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a344e9-40d2-4c42-b9fb-3a5e44565874",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d91567c-9b0d-4cf7-83d1-6211f228ea04",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_clean = alt[(alt['alt'] != 0) & (~alt['alt'].isna())].sort_values(by='datetime').reset_index(drop=True)\n",
    "alt_clean_pivot = alt_clean.pivot_table(index='datetime', columns=['lat', 'lon'], values='alt')\n",
    "\n",
    "def pivot_to_sequence_with_mask(pivot_df, n_months=12):\n",
    "    # Replace zeros with NaN for consistency in masking\n",
    "    pivot_df_masked = pivot_df.replace(0, np.nan)\n",
    "    \n",
    "    sequences = []\n",
    "    masks = []\n",
    "    unique_dates = pivot_df_masked.index.unique()\n",
    "    \n",
    "    for start in range(len(unique_dates) - n_months + 1):\n",
    "        end = start + n_months\n",
    "        seq = pivot_df_masked.loc[unique_dates[start:end]].values\n",
    "        \n",
    "        if seq.shape[0] == n_months:  # Ensure we have the correct number of months\n",
    "            seq = seq.reshape(1, n_months, *seq.shape[1:], 1)\n",
    "            mask = ~np.isnan(seq)  # Mask is True where the data is valid (not NaN)\n",
    "            sequences.append(np.nan_to_num(seq, nan=0))  # Replace NaN with zero or another value\n",
    "            masks.append(mask)\n",
    "    \n",
    "    if sequences:\n",
    "        sequences = np.concatenate(sequences, axis=0)\n",
    "        masks = np.concatenate(masks, axis=0)\n",
    "        return sequences, masks\n",
    "    else:\n",
    "        raise ValueError(\"No valid sequences found. Check your data and preprocessing steps.\")\n",
    "\n",
    "alt_seq, alt_mask = pivot_to_sequence_with_mask(alt_clean_pivot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19011f08-b6d5-456e-af98-d8c89d6dafa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_df=alt_clean_pivot.stack(level=[0, 1]).reset_index()\n",
    "alt_df.columns = ['datetime', 'lat', 'lon', 'alt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a75122c-ac7e-41aa-82bc-56db8f4f99c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_seq.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59fb113b-10db-4541-90d1-32254fa7813b",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90bd118-4c12-4122-835c-d4dee359f763",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(df):\n",
    "    #return df.dropna().loc[(df.iloc[:, 3] != 0)].sort_values(by='datetime').reset_index(drop=True)\n",
    "    return df[(df[value_column] != 0) & (~df[value_column].isna())].sort_values(by=['datetime', 'lat', 'lon']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4627127a-0a5d-4178-8036-c427362272e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_train_clean = alt_train[(alt_train['alt'] != 0) & (~alt_train['alt'].isna())].sort_values(by='datetime').reset_index(drop=True)\n",
    "alt_valid_clean = alt_valid[(alt_valid['alt'] != 0) & (~alt_valid['alt'].isna())].sort_values(by='datetime').reset_index(drop=True)\n",
    "alt_test_clean = alt_test[(alt_test['alt'] != 0) & (~alt_test['alt'].isna())].sort_values(by='datetime').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732e87be-522d-42f8-bd4f-8224882b02cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_train_clean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f60902-f688-4e03-9205-216e2c09473a",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_train_pivot = alt_train_clean.pivot_table(index='datetime', columns=['lat', 'lon'], values='alt')\n",
    "alt_valid_pivot = alt_valid_clean.pivot_table(index='datetime', columns=['lat', 'lon'], values='alt')\n",
    "alt_test_pivot = alt_test_clean.pivot_table(index='datetime', columns=['lat', 'lon'], values='alt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319d7ac0-36b0-4a20-96f0-7a8663f72c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_train_pivot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5f9963-0ab1-4dee-8eea-f55277bb91c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pivot_to_sequence_with_mask(pivot_df, n_months=12):\n",
    "    # Replace zeros with NaN for consistency in masking\n",
    "    pivot_df_masked = pivot_df.replace(0, np.nan)\n",
    "    \n",
    "    sequences = []\n",
    "    masks = []\n",
    "    unique_dates = pivot_df_masked.index.unique()\n",
    "    \n",
    "    for start in range(len(unique_dates) - n_months + 1):\n",
    "        end = start + n_months\n",
    "        seq = pivot_df_masked.loc[unique_dates[start:end]].values\n",
    "        \n",
    "        if seq.shape[0] == n_months:  # Ensure we have the correct number of months\n",
    "            seq = seq.reshape(1, n_months, *seq.shape[1:], 1)\n",
    "            mask = ~np.isnan(seq)  # Mask is True where the data is valid (not NaN)\n",
    "            sequences.append(np.nan_to_num(seq, nan=0))  # Replace NaN with zero or another value\n",
    "            masks.append(mask)\n",
    "    \n",
    "    if sequences:\n",
    "        sequences = np.concatenate(sequences, axis=0)\n",
    "        masks = np.concatenate(masks, axis=0)\n",
    "        return sequences, masks\n",
    "    else:\n",
    "        raise ValueError(\"No valid sequences found. Check your data and preprocessing steps.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d342389-a72c-440f-a2df-59362219a0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_train_seq, alt_train_mask = pivot_to_sequence_with_mask(alt_train_pivot)\n",
    "alt_valid_seq, alt_valid_mask = pivot_to_sequence_with_mask(alt_valid_pivot)\n",
    "alt_test_seq, alt_test_mask = pivot_to_sequence_with_mask(alt_test_pivot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0bd6c89-764c-4b26-88ef-50d6306e24b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_train_seq.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6e3c9b-4829-43aa-b1d4-081b1aa8b66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(alt_train_seq[-1,4,:,0]);\n",
    "plt.plot(alt_train_seq[-1,7,:,0]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7ff61f-6108-4448-8069-b12ef55c2e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(alt_train_seq[0,4,:,0]);\n",
    "plt.plot(alt_train_seq[0,7,:,0]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ff6f12-7856-447b-8242-015e437b1a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_train_seq.shape, alt_valid_seq.shape, alt_test_seq.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5d51f5-1796-4853-8ad9-1257eba7ac6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_train.lat.nunique(), alt_train.lon.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90820bee-eb2d-47a0-ae18-5d3b2c93ebc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_lats = alt_train_clean['lat'].unique()\n",
    "unique_lons = alt_train_clean['lon'].unique()\n",
    "\n",
    "lat_count = len(unique_lats)\n",
    "lon_count = len(unique_lons)\n",
    "\n",
    "print(f\"Number of unique latitudes: {lat_count}\")\n",
    "print(f\"Number of unique longitudes: {lon_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d23b9e-e1f6-496d-8f85-4fb2c02cbd47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Define the size of the bins\n",
    "lat_bin_size = 0.001  # Adjust this based on the data\n",
    "lon_bin_size = 0.001  # Adjust this based on the data\n",
    "\n",
    "# Create new columns for binned latitude and longitude\n",
    "alt_train_clean['lat_bin'] = (alt_train_clean['lat'] // lat_bin_size) * lat_bin_size\n",
    "alt_train_clean['lon_bin'] = (alt_train_clean['lon'] // lon_bin_size) * lon_bin_size\n",
    "\n",
    "# Now check the number of unique bins\n",
    "unique_lat_bins = alt_train_clean['lat_bin'].unique()\n",
    "unique_lon_bins = alt_train_clean['lon_bin'].unique()\n",
    "\n",
    "lat_count = len(unique_lat_bins)\n",
    "lon_count = len(unique_lon_bins)\n",
    "\n",
    "print(f\"Number of unique lat bins: {lat_count}\")\n",
    "print(f\"Number of unique lon bins: {lon_count}\")\n",
    "print(f\"lat_count * lon_count = {lat_count * lon_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3f08b8-f97c-4fce-962e-af7a0f29dc9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Define the size of the bins\n",
    "lat_bin_size = 0.001  # Adjust this based on the data\n",
    "lon_bin_size = 0.001  # Adjust this based on the data\n",
    "\n",
    "# Create new columns for binned latitude and longitude\n",
    "alt_valid_clean['lat_bin'] = (alt_valid_clean['lat'] // lat_bin_size) * lat_bin_size\n",
    "alt_valid_clean['lon_bin'] = (alt_valid_clean['lon'] // lon_bin_size) * lon_bin_size\n",
    "\n",
    "# Now check the number of unique bins\n",
    "unique_lat_bins = alt_train_clean['lat_bin'].unique()\n",
    "unique_lon_bins = alt_train_clean['lon_bin'].unique()\n",
    "\n",
    "lat_count = len(unique_lat_bins)\n",
    "lon_count = len(unique_lon_bins)\n",
    "\n",
    "print(f\"Number of unique lat bins: {lat_count}\")\n",
    "print(f\"Number of unique lon bins: {lon_count}\")\n",
    "print(f\"lat_count * lon_count = {lat_count * lon_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b750e99-672d-483c-981f-1ae35f6eaffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Define the size of the bins\n",
    "lat_bin_size = 0.001  # Adjust this based on the data\n",
    "lon_bin_size = 0.001  # Adjust this based on the data\n",
    "\n",
    "# Create new columns for binned latitude and longitude\n",
    "alt_test_clean['lat_bin'] = (alt_test_clean['lat'] // lat_bin_size) * lat_bin_size\n",
    "alt_test_clean['lon_bin'] = (alt_test_clean['lon'] // lon_bin_size) * lon_bin_size\n",
    "\n",
    "# Now check the number of unique bins\n",
    "unique_lat_bins = alt_train_clean['lat_bin'].unique()\n",
    "unique_lon_bins = alt_train_clean['lon_bin'].unique()\n",
    "\n",
    "lat_count = len(unique_lat_bins)\n",
    "lon_count = len(unique_lon_bins)\n",
    "\n",
    "print(f\"Number of unique lat bins: {lat_count}\")\n",
    "print(f\"Number of unique lon bins: {lon_count}\")\n",
    "print(f\"lat_count * lon_count = {lat_count * lon_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5aa271-6467-481b-b259-b907ddb09844",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-pivot the data using the new binned latitude and longitude\n",
    "alt_train_pivot = alt_train_clean.pivot_table(index='datetime', columns=['lat_bin', 'lon_bin'], values='alt')\n",
    "alt_valid_pivot = alt_valid_clean.pivot_table(index='datetime', columns=['lat_bin', 'lon_bin'], values='alt')\n",
    "alt_test_pivot = alt_test_clean.pivot_table(index='datetime', columns=['lat_bin', 'lon_bin'], values='alt')\n",
    "\n",
    "# Create sequences with the updated grid\n",
    "alt_train_seq, alt_train_mask = pivot_to_sequence_with_mask(alt_train_pivot, n_months=12)\n",
    "alt_valid_seq, alt_valid_mask = pivot_to_sequence_with_mask(alt_valid_pivot, n_months=12)\n",
    "alt_test_seq, alt_test_mask = pivot_to_sequence_with_mask(alt_test_pivot, n_months=12)\n",
    "\n",
    "# The new shape should reflect the grid size\n",
    "print(alt_train_seq.shape)  # Expected shape: (samples, 12, 72, 103, 1)\n",
    "print(alt_valid_seq.shape)\n",
    "print(alt_test_seq.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46091a8-0475-48bb-af20-f33637be55d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(divisorGenerator(356)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2696ff55-21a7-4f22-adb1-a3e7b95af7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_train_seq = alt_train_seq.reshape((alt_train_seq.shape[0], alt_train_seq.shape[1], 303, 335, alt_train_seq.shape[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef05c23-6b41-484b-87a2-6bac1312ae82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_lat = 303\n",
    "# max_lon = 335\n",
    "\n",
    "# alt_train_seq_padded = pad_sequences(alt_train_seq, max_lat, max_lon)\n",
    "# alt_valid_seq_padded = pad_sequences(alt_valid_seq, max_lat, max_lon)\n",
    "# alt_test_seq_padded = pad_sequences(alt_test_seq, max_lat, max_lon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32defcb-e431-4507-b2fe-32bf8b143673",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_train_seq = alt_train_seq.reshape((alt_train_seq.shape[0], alt_train_seq.shape[1], 303, 335, alt_train_seq.shape[3]))\n",
    "alt_valid_seq = alt_valid_seq.reshape((alt_valid_seq.shape[0], alt_valid_seq.shape[1], 303, 335, alt_valid_seq.shape[3]))\n",
    "alt_test_seq = alt_test_seq.reshape((alt_test_seq.shape[0], alt_test_seq.shape[1], 303, 335, alt_test_seq.shape[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74f1ca9-bf33-4c71-b06c-a0bde5156051",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539eb5a4-a416-4057-a8ab-3105434c7218",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc1426e-801b-4d5f-ac77-ff35c4a5c14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_alt_train = alt_train_seq[:, :, :, :, 0]  # Shape: (samples, 12, 72, 103)\n",
    "y_alt_train = alt_train_seq[:, :, :, :, 1]  # Shape: (samples, 12, 72, 103)\n",
    "\n",
    "X_alt_valid = alt_valid_seq[:, :, :, :, 0]\n",
    "y_alt_valid = alt_valid_seq[:, :, :, :, 1]\n",
    "\n",
    "X_alt_test = alt_test_seq[:, :, :, :, 0]\n",
    "y_alt_test = alt_test_seq[:, :, :, :, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626aefb2-d578-441a-9cff-95377b6c0e2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bba80e9-b0bb-4d44-8256-f2e160b9e212",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecae5dd4-b28b-4ea6-8f0b-14aafede1229",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43495da5-4148-4ca1-8ba6-1497e33c8bae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87ee89b-c3cc-4898-b1e2-560cc1057958",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697182b8-7a23-4ce6-a7ca-ea61e5a9a1a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450d95e7-b666-4ff8-a348-2b4cc5db0544",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_train_seq = alt_train_seq.reshape((alt_train_seq.shape[0], alt_train_seq.shape[1], \\\n",
    "                                       alt_train_seq.shape[2], 1, alt_train_seq.shape[3]))\n",
    "alt_valid_seq = alt_valid_seq.reshape((alt_valid_seq.shape[0], alt_valid_seq.shape[1], \\\n",
    "                                       alt_valid_seq.shape[2], 1, alt_valid_seq.shape[3]))\n",
    "alt_test_seq = alt_test_seq.reshape((alt_test_seq.shape[0], alt_test_seq.shape[1], \\\n",
    "                                     alt_test_seq.shape[2], 1, alt_test_seq.shape[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac04f9f-6006-4f4f-be46-7b0d8a6d657e",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_lat = max([seq.shape[2] for seq in [alt_train_seq, alt_valid_seq, alt_test_seq]])\n",
    "max_lon = max([seq.shape[3] for seq in [alt_train_seq, alt_valid_seq, alt_test_seq]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8eee34a-865d-4d9c-8b08-7f424fb72896",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequences(sequences, max_lat, max_lon):\n",
    "    padded_sequences = []\n",
    "    for seq in sequences:\n",
    "        # Check if the sequence has the expected dimensions (5D: batch, time, lat, lon, feature)\n",
    "        if seq.ndim != 5:\n",
    "            raise ValueError(f\"Expected a 5D sequence, but got {seq.ndim}D sequence.\")\n",
    "        \n",
    "        # Get the current shape\n",
    "        current_lat = seq.shape[2]\n",
    "        current_lon = seq.shape[3]\n",
    "        \n",
    "        # Calculate the padding amounts\n",
    "        lat_diff = max_lat - current_lat\n",
    "        lon_diff = max_lon - current_lon\n",
    "        \n",
    "        # Apply padding\n",
    "        if lat_diff >= 0 and lon_diff >= 0:\n",
    "            padded_seq = np.pad(seq, ((0, 0), (0, 0), (0, lat_diff), (0, lon_diff), (0, 0)), 'constant', constant_values=0)\n",
    "            padded_sequences.append(padded_seq)\n",
    "        else:\n",
    "            raise ValueError(\"Padding dimensions cannot be negative. Check the dimensions of your sequences.\")\n",
    "    \n",
    "    return np.array(padded_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bae7345-8545-497d-8cc4-03312eaf505b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_5d(sequences):\n",
    "    \"\"\"Convert 4D sequences (samples, time_steps, lat, lon) to 5D by adding a feature dimension.\"\"\"\n",
    "    return np.expand_dims(sequences, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4725b6-b2b6-4470-94ee-441026355908",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_train_seq_5d = convert_to_5d(alt_train_seq)\n",
    "alt_valid_seq_5d = convert_to_5d(alt_valid_seq)\n",
    "alt_test_seq_5d = convert_to_5d(alt_test_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f34424-2658-45a1-98e1-6bf486c4d2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# alt_train_mask_seq_5d = convert_to_5d(alt_train_mask)\n",
    "# alt_valid_mask_seq_5d = convert_to_5d(alt_valid_mask)\n",
    "# alt_test_mask_seq_5d = convert_to_5d(alt_test_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa376405-579c-4490-9620-a99f8234f8ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_lat = max([seq.shape[2] for seq in [alt_train_seq_5d, alt_valid_seq_5d, alt_test_seq_5d]])\n",
    "max_lon = max([seq.shape[3] for seq in [alt_train_seq_5d, alt_valid_seq_5d, alt_test_seq_5d]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e8b58f-f71a-4005-a151-116216671627",
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_lat_masked = max([seq.shape[2] for seq in [alt_train_mask_seq_5d, \\\n",
    "#                                                alt_valid_mask_seq_5d, \\\n",
    "#                                                alt_test_mask_seq_5d]])\n",
    "# max_lon_masked = max([seq.shape[3] for seq in [alt_train_mask_seq_5d, \\\n",
    "#                                                alt_valid_mask_seq_5d, \\\n",
    "#                                                alt_test_mask_seq_5d]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab3ffa4-a401-480e-8f06-8003878879cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_train_seq_padded = pad_sequences(alt_train_seq_5d, max_lat, max_lon)\n",
    "alt_valid_seq_padded = pad_sequences(alt_valid_seq_5d, max_lat, max_lon)\n",
    "alt_test_seq_padded = pad_sequences(alt_test_seq_5d, max_lat, max_lon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22ff5f5-4f01-40cd-addc-21d0e40d30ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.DataFrame(alt_train_seq_padded[0,7,:,:,0,0].flatten()).replace(0,np.nan).dropna().plot();\n",
    "alt_train_seq_padded.shape,alt_valid_seq_padded.shape,alt_test_seq_padded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e59dd8e-0b90-495b-a483-4ff10e131724",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_shape(sequences):\n",
    "    \"\"\"Remove unnecessary dimensions from sequences.\"\"\"\n",
    "    return np.squeeze(sequences, axis=-1)\n",
    "\n",
    "alt_train_seq_padded = adjust_shape(alt_train_seq_padded)\n",
    "alt_valid_seq_padded = adjust_shape(alt_valid_seq_padded)\n",
    "alt_test_seq_padded = adjust_shape(alt_test_seq_padded)\n",
    "\n",
    "# Verify the shapes\n",
    "print(alt_train_seq_padded.shape)\n",
    "print(alt_valid_seq_padded.shape)\n",
    "print(alt_test_seq_padded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11aa643d-1c84-4a65-a5dd-522b0bccb4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_X_y(sequences):\n",
    "    # X is the first four dimensions (datetime, lat, lon)\n",
    "    X = sequences[:, :, :, :]\n",
    "    \n",
    "    # y is the last dimension (alt)\n",
    "    y = sequences[:, :, :, :, 0]\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "# Apply the function to each dataset\n",
    "X_alt_train, y_alt_train = split_X_y(alt_train_seq_padded)\n",
    "X_alt_valid, y_alt_valid = split_X_y(alt_valid_seq_padded)\n",
    "X_alt_test, y_alt_test = split_X_y(alt_test_seq_padded)\n",
    "\n",
    "print(X_alt_train.shape, y_alt_train.shape)\n",
    "print(X_alt_valid.shape, y_alt_valid.shape)\n",
    "print(X_alt_test.shape, y_alt_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8b0472-4c6c-46f8-bfd2-4709cf581a72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70e790f-0158-499d-a179-9984c6572c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define callbacks\n",
    "early_stopping_cb = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "lr_scheduler_cb = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2)\n",
    "checkpoint_cb = ModelCheckpoint('model_checkpoint.keras', save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ffc6e5-5ec4-44e9-944f-31484ef95549",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_tuner import BayesianOptimization\n",
    "\n",
    "tuner = BayesianOptimization(\n",
    "    model,\n",
    "    objective='val_loss',\n",
    "    max_trials=10,\n",
    "    executions_per_trial=1,\n",
    "    directory='alt_train2',\n",
    "    project_name='alt_train_conv3dlstm_tuning',\n",
    "    #directory='ch4_train',\n",
    "    #project_name='ch4_train_conv3dlstm_tuning',\n",
    "    overwrite=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f417de77-a841-467e-9195-60b29609a7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1, 1, X_train.shape[3]))\n",
    "X_valid = X_valid.reshape((X_valid.shape[0], X_valid.shape[1], 1, 1, X_valid.shape[3]))\n",
    "X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1, 1, X_test.shape[3]))\n",
    "\n",
    "#tuner.search(train_dataset, epochs=100, validation_data=valid_dataset, callbacks=[early_stopping_cb, lr_scheduler_cb])\n",
    "tuner.search(X_train, y_train, epochs=100, validation_data=(X_valid, y_valid), callbacks=[early_stopping_cb, lr_scheduler_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4805abd-2158-47a2-8cfb-ffc00fbb2402",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.search(\n",
    "    X_alt_train,\n",
    "    y_alt_train,\n",
    "    epochs=10,\n",
    "    batch_size = 16,\n",
    "    validation_data=(X_alt_valid, y_alt_valid),\n",
    "    callbacks=[early_stopping_cb, lr_scheduler_cb]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8a5b75-367c-439f-ac83-7ae91a337116",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb36631-f1d2-4287-83eb-48b056bf7219",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c175ba37-d2f8-403a-82d4-354746559d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fc_train_clean = clean_data(fc_train)\n",
    "fc_valid_clean = clean_data(fc_valid)\n",
    "fc_test_clean = clean_data(fc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8dbe17e-b2aa-4513-93d0-b29e7e35b28c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fc_train_pivot = fc_train_clean.pivot_table(index='datetime', columns=['lat', 'lon'], values='fc')\n",
    "fc_valid_pivot = fc_valid_clean.pivot_table(index='datetime', columns=['lat', 'lon'], values='fc')\n",
    "fc_test_pivot = fc_test_clean.pivot_table(index='datetime', columns=['lat', 'lon'], values='fc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228fda58-0dc1-4219-b863-7fb94f889117",
   "metadata": {},
   "outputs": [],
   "source": [
    "fch4_train_clean = clean_data(fch4_train)\n",
    "fch4_valid_clean = clean_data(fch4_valid)\n",
    "fch4_test_clean = clean_data(fch4_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ee7123-933a-42e8-a836-d2f7877eb780",
   "metadata": {},
   "outputs": [],
   "source": [
    "fch4_train_pivot = fch4_train_clean.pivot_table(index='datetime', columns=['lat', 'lon'], values='fch4')\n",
    "fch4_valid_pivot = fch4_valid_clean.pivot_table(index='datetime', columns=['lat', 'lon'], values='fch4')\n",
    "fch4_test_pivot = fch4_test_clean.pivot_table(index='datetime', columns=['lat', 'lon'], values='fch4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28a8d34-5e3a-40bf-b010-5ab5a903626a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea82e1a3-e535-4025-8dff-53743b0a728f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939d0c78-e380-426d-91b4-8b854461a848",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def create_sequences(df, n_months=12):\n",
    "    df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "    df.sort_values(by=['datetime', 'lat', 'lon'], inplace=True)\n",
    "    \n",
    "    unique_dates = sorted(df['datetime'].unique())\n",
    "    unique_lats = sorted(df['lat'].unique())\n",
    "    unique_lons = sorted(df['lon'].unique())\n",
    "    \n",
    "    sequences = []\n",
    "    \n",
    "    for start in range(len(unique_dates) - n_months + 1):\n",
    "        end = start + n_months\n",
    "        seq = df[(df['datetime'].isin(unique_dates[start:end]))]\n",
    "        \n",
    "        # Pivot the table and fill missing values\n",
    "        seq_pivot = seq.pivot_table(index=['datetime'], columns=['lat', 'lon'], values=df.columns[3])\n",
    "        seq_pivot = seq_pivot.reindex(index=unique_dates[start:end], columns=pd.MultiIndex.from_product([unique_lats, unique_lons]), fill_value=np.nan)\n",
    "        \n",
    "        # Reshape into 5D array\n",
    "        seq_reshaped = seq_pivot.values.reshape(1, n_months, len(unique_lats), len(unique_lons), 1)\n",
    "        sequences.append(seq_reshaped)\n",
    "    \n",
    "    if len(sequences) > 0:\n",
    "        return np.concatenate(sequences, axis=0)\n",
    "    else:\n",
    "        raise ValueError(\"No valid sequences found. Check your data and preprocessing steps.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69024bb5-a22d-452b-8809-fc7f3c7a6856",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# # Check for missing lat/lon combinations\n",
    "# lat_lon_combinations = alt_train_clean.groupby(['lat', 'lon']).size()\n",
    "# missing_combinations = lat_lon_combinations[lat_lon_combinations < 12]\n",
    "# print(f\"Number of missing combinations: {missing_combinations.count()}\")\n",
    "\n",
    "# # Check the number of unique datetime entries\n",
    "# unique_dates_count = alt_train_clean['datetime'].nunique()\n",
    "# print(f\"Number of unique datetime entries: {unique_dates_count}\")\n",
    "\n",
    "# # Check the overall data distribution by datetime\n",
    "# datetime_distribution = alt_train_clean['datetime'].value_counts()\n",
    "# print(datetime_distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9734109b-fb02-4d5a-bdf7-1cc27b7ad65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_train_seq = create_sequences(alt_train_clean)\n",
    "alt_valid_seq = create_sequences(alt_valid_clean)\n",
    "alt_test_seq = create_sequences(alt_test_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1334c139-5df4-46fc-b05c-5436eb2c1021",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "# Save sequences to an HDF5 file\n",
    "with h5py.File('/Volumes/JPL/alt_train_valid_test_sequences.h5', 'w') as hf:\n",
    "    hf.create_dataset('alt_train', data=alt_train_seq)\n",
    "    hf.create_dataset('alt_valid', data=alt_valid_seq)\n",
    "    hf.create_dataset('alt_test', data=alt_test_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00332b4a-f08e-4ca2-b8af-d3f6367e53e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('/Volumes/JPL/alt_train_seq.npy',alt_train_seq)\n",
    "np.save('/Volumes/JPL/alt_valid_seq.npy',alt_valid_seq)\n",
    "np.save('/Volumes/JPL/alt_test_seq.npy',alt_test_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ad25c6-c849-4c10-8050-2b62dd7c0f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fc_train_seq = create_sequences(fc_train_clean)\n",
    "fc_valid_seq = create_sequences(fc_valid_clean)\n",
    "fc_test_seq = create_sequences(fc_test_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f4b81a-2ecd-4eaa-95af-ddfa4dabf596",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('/Volumes/JPL/fc_train_seq.npy',fc_train_seq)\n",
    "np.save('/Volumes/JPL/fc_valid_seq.npy',fc_valid_seq)\n",
    "np.save('/Volumes/JPL/fc_test_seq.npy',fc_test_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ffac13-74ba-4e51-b5a4-b1c8b1daeb94",
   "metadata": {},
   "outputs": [],
   "source": [
    "fch4_train_seq = create_sequences(fch4_train_clean)\n",
    "fch4_valid_seq = create_sequences(fch4_valid_clean)\n",
    "fch4_test_seq = create_sequences(fch4_test_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f589697b-3adc-48ea-ac99-a3f7c3eb9e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('/Volumes/JPL/fc_train_seq.npy',fc_train_seq)\n",
    "np.save('/Volumes/JPL/fc_valid_seq.npy',fc_valid_seq)\n",
    "np.save('/Volumes/JPL/fc_test_seq.npy',fc_test_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5c3071-5566-4b86-86cb-85095111dedf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322d2546-df79-46df-804e-56a1a52ed732",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_train_sequences = numpy.load('/Volumes/JPL/alt_train_sequences.parquet')\n",
    "alt_train_targets = numpy.load('/Volumes/JPL/alt_train_targets.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d51425-7665-48ce-b62e-7c08c512bf8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f66e7c9-6b19-4e3e-9656-e5d31c8621c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1e1e84-98f7-454f-a273-9b28fc4b5247",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16830c8a-8bad-4fe5-8653-5ce527f42d8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7831c3bf-e1ff-48f0-9a6f-311439a0c0a5",
   "metadata": {},
   "source": [
    "#### Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df92115d-cdac-4c30-a469-650d8e0dcdea",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_train.shape, fc_train.shape, fch4_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7282e58-055d-48d3-a329-63fcc2a7d708",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_valid.shape, fc_valid.shape, fch4_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8244632d-7119-4ae1-a844-cee017c4e0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_test.shape, fc_test.shape, fch4_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9dc403-252b-447a-b2bb-0135f780adcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#alt_train=alt_train.interpolate(method='linear', limit_direction='forward').dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a80892-53e1-4f68-9d63-d5b6caae0a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_train.to_parquet('/Volumes/JPL/merged_train_altfc.parquet',engine='pyarrow',compression='snappy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29822da-1cca-4f31-8b8a-3dc778bd8529",
   "metadata": {},
   "outputs": [],
   "source": [
    "del merged_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb115e2-42f8-4e80-bba1-3f4a4fc94c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "del alt_train, fc_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732a9bc1-fe58-4d1e-9f21-210e71ce95b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Volumes/JPL/merged_train_altfc.parquet','rb') as f:\n",
    "    merged_train=pd.read_parquet(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85347acd-91a2-4105-94ff-bb7fd85dc83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_train = pd.merge(merged_train, fch4_train, on=['datetime', 'lat', 'lon'], how='outer')\n",
    "merged_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e02286-37ee-42b4-88d6-4c9f37007f7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569218b0-d2a0-4d1a-83fe-f27d11ddfb22",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_train.to_parquet('/Volumes/JPL/merged_train.parquet',engine='pyarrow',compression='snappy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6420935c-d180-4b39-a18b-f55d84d4db4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201b07dd-f435-4726-a986-d8b6c12a59e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d7c787-fee4-403a-b245-c11ec27901f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Volumes/JPL/merged_train.parquet','rb') as f:\n",
    "    merged_train=pd.read_parquet(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e72cc1-fc6d-4619-8392-5ab789f77a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_train = pd.merge(merged_train, fch4_train, on=['datetime', 'lat', 'lon'], how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42c3257-3a53-446e-9150-cd3b9c97300f",
   "metadata": {},
   "outputs": [],
   "source": [
    "del merged_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0d639a-5924-4012-ac87-73e2a5ec6f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Volumes/JPL/merged_train.parquet','rb') as f:\n",
    "    merged_train=pd.read_parquet(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37303995-ca65-4a35-8029-a3a83b623e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_valid = pd.merge(alt_valid, fc_valid, on=['datetime', 'lat', 'lon'], how='outer')\n",
    "merged_valid = pd.merge(merged_valid, fch4_valid, on=['datetime', 'lat', 'lon'], how='outer')\n",
    "merged_valid.fillna(0, inplace=True)\n",
    "\n",
    "merged_test = pd.merge(alt_test, fc_test, on=['datetime', 'lat', 'lon'], how='outer')\n",
    "merged_test = pd.merge(merged_test, fch4_test, on=['datetime', 'lat', 'lon'], how='outer')\n",
    "merged_test.fillna(0, inplace=True)\n",
    "\n",
    "merged_valid.to_parquet('/Volumes/JPL/merged_valid.parquet',engine='pyarrow',compression='snappy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709683e5-9744-4673-95c8-43cbbe7f29c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_test.to_parquet('/Volumes/JPL/merged_test.parquet',engine='pyarrow',compression='snappy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5020141-4960-44d4-93c9-4064a05f3ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_train = pd.merge(alt_train, fc_train, on=['datetime', 'lat', 'lon'], how='outer')\n",
    "merged_train = pd.merge(merged_train, fch4_train, on=['datetime', 'lat', 'lon'], how='outer')\n",
    "merged_train.fillna(0, inplace=True)\n",
    "merged_train.to_parquet('/Volumes/JPL/merged_train.parquet',engine='pyarrow',compression='snappy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066e06bb-5c07-4c01-9b00-62162e13c57a",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_valid = pd.merge(alt_valid, fc_valid, on=['datetime', 'lat', 'lon'], how='outer')\n",
    "merged_valid = pd.merge(merged_valid, fch4_valid, on=['datetime', 'lat', 'lon'], how='outer')\n",
    "merged_valid.fillna(0, inplace=True)\n",
    "merged_valid.to_parquet('/Volumes/JPL/merged_valid.parquet',engine='pyarrow',compression='snappy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d52a3a6-4851-4b58-a7dc-a0490ab471c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_test = pd.merge(alt_test, fc_test, on=['datetime', 'lat', 'lon'], how='outer')\n",
    "merged_test = pd.merge(merged_test, fch4_test, on=['datetime', 'lat', 'lon'], how='outer')\n",
    "merged_test.fillna(0, inplace=True)\n",
    "merged_test.to_parquet('/Volumes/JPL/merged_test.parquet',engine='pyarrow',compression='snappy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79597fcf-45be-4138-addb-b89462e80b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open('/Volumes/JPL/merged_valid.parquet','rb') as f:\n",
    "    merged_valid=pd.read_parquet(f)\n",
    "with open('/Volumes/JPL/merged_test.parquet','rb') as f:\n",
    "    merged_test=pd.read_parquet(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9cfbf9-2b79-4e2e-94c4-f7a28122e191",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Individual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6287bfa-329a-4c5f-bd01-fce4873b4485",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences_with_masking(df, sequence_length=12):\n",
    "    sequences = []\n",
    "    targets = []\n",
    "    unique_times = df['datetime'].unique()\n",
    "    unique_lat = df['lat'].unique()\n",
    "    unique_lon = df['lon'].unique()\n",
    "    \n",
    "    lat_to_idx = {lat: idx for idx, lat in enumerate(unique_lat)}\n",
    "    lon_to_idx = {lon: idx for idx, lon in enumerate(unique_lon)}\n",
    "    \n",
    "    for i in range(len(unique_times) - sequence_length):\n",
    "        time_slice = unique_times[i:i+sequence_length]\n",
    "        sequence_data = df[df['datetime'].isin(time_slice)].sort_values(by=['datetime', 'lat', 'lon'])\n",
    "        \n",
    "        grid_sequence = np.full((sequence_length, len(unique_lat), len(unique_lon)), np.nan)\n",
    "        \n",
    "        for j, time in enumerate(time_slice):\n",
    "            time_data = sequence_data[sequence_data['datetime'] == time]\n",
    "            for _, row in time_data.iterrows():\n",
    "                if row['alt'] != 0:  # Ignore zero values\n",
    "                    lat_idx = lat_to_idx[row['lat']]\n",
    "                    lon_idx = lon_to_idx[row['lon']]\n",
    "                    grid_sequence[j, lat_idx, lon_idx] = row['alt']\n",
    "        \n",
    "        # Mask out NaN values and keep only valid data\n",
    "        mask = ~np.isnan(grid_sequence)\n",
    "        if np.sum(mask) > 0:  # Ensure that there is valid data\n",
    "            sequences.append(grid_sequence[mask])\n",
    "        \n",
    "        target_time = unique_times[i + sequence_length]\n",
    "        target_data = df[df['datetime'] == target_time].sort_values(by=['lat', 'lon'])\n",
    "        target_grid = np.full((len(unique_lat), len(unique_lon)), np.nan)\n",
    "        for _, row in target_data.iterrows():\n",
    "            if row['alt'] != 0:  # Ignore zero values\n",
    "                lat_idx = lat_to_idx[row['lat']]\n",
    "                lon_idx = lon_to_idx[row['lon']]\n",
    "                target_grid[lat_idx, lon_idx] = row['alt']\n",
    "        \n",
    "        target_mask = ~np.isnan(target_grid)\n",
    "        if np.sum(target_mask) > 0:\n",
    "            targets.append(target_grid[target_mask])\n",
    "    \n",
    "    return np.array(sequences), np.array(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4791cf64-0dba-4f9a-977f-63ef7e15cae3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeeda48b-fb68-4c11-8aee-44959f70c218",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_train_sequences, alt_train_targets = create_sequences(alt_train)\n",
    "alt_train_sequences=alt_train_sequences.reshape(3121, 12, 361, 365,1)\n",
    "alt_train_targets=alt_train_targets.reshape(3121, 361, 365, 1)\n",
    "alt_train_sequences.shape, alt_train_targets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3832a20f-cbf7-4a8e-9d1c-83f7203fb11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_train_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1790ae65-4f2f-4e81-8744-c9d4f5e317f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242bec9c-c397-499a-9ada-7b7918259cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('/Volumes/JPL/alt_train_sequences.parquet',alt_train_sequences)\n",
    "np.save('/Volumes/JPL/alt_train_targets.parquet',alt_train_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fede2dd-4aa1-4aba-81bf-c812d9045a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_train_sequences = numpy.load('/Volumes/JPL/alt_train_sequences.parquet')\n",
    "alt_train_targets = numpy.load('/Volumes/JPL/alt_train_targets.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961c0607-50b9-428f-be5f-db1273cd6b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_valid_sequences, alt_valid_targets = create_sequences(alt_valid)\n",
    "alt_valid_sequences=alt_valid_sequences.reshape(73, 12, 96, 96, 1)\n",
    "alt_valid_targets=alt_valid_targets.reshape(73, 96, 96, 1)\n",
    "alt_valid_sequences.shape, alt_valid_targets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034c63d3-126f-4530-81df-dbf2bcff3114",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_valid_sequences.to_parquet('/Volumes/JPL/alt_valid_sequences.parquet',engine='pyarrow',compression='snappy')\n",
    "alt_valid_targets.to_parquet('/Volumes/JPL/alt_valid_targets.parquet',engine='pyarrow',compression='snappy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f67fde-2d54-465c-87cb-9e789b733bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_test_sequences, alt_test_targets = create_sequences(alt_test)\n",
    "alt_test_sequences=alt_test_sequences.reshape(566, 12, 41, 41, 1)\n",
    "alt_test_targets=alt_test_targets.reshape(566, 41, 41, 1)\n",
    "alt_test_sequences.shape, alt_test_targets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87811beb-0a1c-4c66-83e9-bd09e0fa658c",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_test_sequences.to_parquet('/Volumes/JPL/alt_test_sequences.parquet',engine='pyarrow',compression='snappy')\n",
    "alt_test_targets.to_parquet('/Volumes/JPL/alt_test_targets.parquet',engine='pyarrow',compression='snappy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96436432-5b2a-4ec5-97f1-8f49da1be39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_train_sequences = numpy.load('/Volumes/JPL/alt_train_sequences.parquet')\n",
    "alt_train_targets = numpy.load('/Volumes/JPL/alt_train_targets.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73cbd1e2-6567-4285-9ec7-e3df43322b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_valid_sequences = numpy.load('/Volumes/JPL/alt_valid_sequences.parquet')\n",
    "alt_valid_targets = numpy.load('/Volumes/JPL/alt_valid_targets.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac36a4e6-b155-4d6c-92af-9ddb3520c24d",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_test_sequences = numpy.load('/Volumes/JPL/alt_test_sequences.parquet')\n",
    "alt_test_targets = numpy.load('/Volumes/JPL/alt_test_targets.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e57a3c2-92f8-4a4f-a1a5-14fd9d37dd57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import ConvLSTM2D, BatchNormalization, Conv3D, Conv2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "def model(hp):\n",
    "    model = Sequential()\n",
    "    \n",
    "    # ConvLSTM2D layer to process the sequence data\n",
    "    model.add(ConvLSTM2D(\n",
    "        filters=hp.Int('filters_1', min_value=16, max_value=64, step=16), \n",
    "        kernel_size=hp.Choice('kernel_size_1', values=[1, 3]),\n",
    "        padding='same', \n",
    "        return_sequences=True,\n",
    "        activation=hp.Choice('activation_function_1', values=['relu','tanh','softmax','sigmoid','leaky_relu','swish']),\n",
    "        input_shape=(12, 361, 365, 1)\n",
    "    )\n",
    "             )\n",
    "    \n",
    "    model.add(BatchNormalization())\n",
    "    \n",
    "    # Another ConvLSTM2D layer\n",
    "    model.add(ConvLSTM2D(\n",
    "        filters=hp.Int('filters_2', min_value=16, max_value=64, step=16), \n",
    "        kernel_size=hp.Choice('kernel_size_2', values=[1, 3]),\n",
    "        padding='same',\n",
    "        activation=hp.Choice('activation_function_2', values=['relu','tanh','softmax','sigmoid','leaky_relu','swish']),\n",
    "        return_sequences=True\n",
    "    )\n",
    "             )\n",
    "    \n",
    "    model.add(BatchNormalization())\n",
    "    \n",
    "    # Final ConvLSTM2D layer without return_sequences\n",
    "    model.add(ConvLSTM2D(\n",
    "        filters=hp.Int('filters_3', min_value=16, max_value=64, step=16), \n",
    "        kernel_size=hp.Choice('kernel_size_3', values=[1, 3]),\n",
    "        padding='same', \n",
    "        activation=hp.Choice('activation_function_3', values=['relu','tanh','softmax','sigmoid','leaky_relu','swish']),\n",
    "        return_sequences=False\n",
    "    )\n",
    "             )\n",
    "    \n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    #model.add(Lambda(lambda x: x[:, -1, :, :, :]))  # Shape becomes [batch_size, height, width, channels]\n",
    "    \n",
    "    # Use Conv2D to reduce to the target output shape\n",
    "    model.add(Conv2D(\n",
    "            filters=1,\n",
    "            kernel_size=hp.Choice('kernel_size_4', values=[1, 3]),\n",
    "            activation=hp.Choice('activation_function_4', values=['relu','tanh','softmax','sigmoid','leaky_relu','swish']),\n",
    "            padding='same'\n",
    "            #data_format='channels_last'\n",
    "    )\n",
    "             )\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(\n",
    "        optimizer=Adam(\n",
    "            hp.Choice('learning_rate', values=[1e-3, 1e-4, 1e-5])\n",
    "        ),\n",
    "        loss='mse', \n",
    "        metrics=['mae','mse','mape','accuracy']\n",
    "    )\n",
    "    \n",
    "    # Display the model's architecture\n",
    "    #model.summary()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942c5b46-c83d-438d-887a-fbd5ffd95d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define callbacks\n",
    "early_stopping_cb = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "lr_scheduler_cb = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2)\n",
    "checkpoint_cb = ModelCheckpoint('model_checkpoint.keras', save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f18c699-51ef-4b82-a2bc-732c385d10d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_tuner import BayesianOptimization\n",
    "\n",
    "tuner = BayesianOptimization(\n",
    "    model,\n",
    "    objective='val_loss',\n",
    "    max_trials=10,\n",
    "    executions_per_trial=1,\n",
    "    directory='alt_train2',\n",
    "    project_name='alt_train_conv3dlstm_tuning',\n",
    "    #directory='ch4_train',\n",
    "    #project_name='ch4_train_conv3dlstm_tuning',\n",
    "    overwrite=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde83d6e-3789-452d-b05d-04471914269a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1, 1, X_train.shape[3]))\n",
    "X_valid = X_valid.reshape((X_valid.shape[0], X_valid.shape[1], 1, 1, X_valid.shape[3]))\n",
    "X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1, 1, X_test.shape[3]))\n",
    "\n",
    "#tuner.search(train_dataset, epochs=100, validation_data=valid_dataset, callbacks=[early_stopping_cb, lr_scheduler_cb])\n",
    "tuner.search(X_train, y_train, epochs=100, validation_data=(X_valid, y_valid), callbacks=[early_stopping_cb, lr_scheduler_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1534fda2-5ac6-4ae1-937b-45e691f043f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.search(\n",
    "    alt_train_sequences,\n",
    "    alt_train_targets,\n",
    "    epochs=10,\n",
    "    batch_size = 16,\n",
    "    validation_data=(alt_valid_sequences, alt_valid_targets),\n",
    "    callbacks=[early_stopping_cb, lr_scheduler_cb]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55cb9c2-bfbd-44a5-aaa8-44310c6a523c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fc_train_sequences, fc_train_targets = create_sequences(fc_train)\n",
    "fc_valid_sequences, fc_valid_targets = create_sequences(fc_valid)\n",
    "fc_test_sequences, fc_test_targets = create_sequences(fc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4df7132-1094-4ce1-b8b0-b6f0d2922f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fch4_train_sequences, fch4_train_targets = create_sequences(fch4_train)\n",
    "fch4_valid_sequences, fch4_valid_targets = create_sequences(fch4_valid)\n",
    "fch4_test_sequences, fch4_test_targets = create_sequences(fch4_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee2696d-a32f-4d97-b300-7fa63d50766a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dimensions based on your data\n",
    "num_lat = len(alt_train['lat'].unique())\n",
    "num_lon = len(alt_train['lon'].unique())\n",
    "\n",
    "# Reshape the sequences for each feature\n",
    "alt_train_sequences = alt_train_sequences.reshape(alt_train_sequences.shape[0], 12, num_lat, num_lon, 1)\n",
    "alt_valid_sequences = alt_valid_sequences.reshape(alt_valid_sequences.shape[0], 12, num_lat, num_lon, 1)\n",
    "alt_test_sequences = alt_test_sequences.reshape(alt_test_sequences.shape[0], 12, num_lat, num_lon, 1)\n",
    "\n",
    "fc_train_sequences = fc_train_sequences.reshape(fc_train_sequences.shape[0], 12, num_lat, num_lon, 1)\n",
    "fc_valid_sequences = fc_valid_sequences.reshape(fc_valid_sequences.shape[0], 12, num_lat, num_lon, 1)\n",
    "fc_test_sequences = fc_test_sequences.reshape(fc_test_sequences.shape[0], 12, num_lat, num_lon, 1)\n",
    "\n",
    "fch4_train_sequences = fch4_train_sequences.reshape(fch4_train_sequences.shape[0], 12, num_lat, num_lon, 1)\n",
    "fch4_valid_sequences = fch4_valid_sequences.reshape(fch4_valid_sequences.shape[0], 12, num_lat, num_lon, 1)\n",
    "fch4_test_sequences = fch4_test_sequences.reshape(fch4_test_sequences.shape[0], 12, num_lat, num_lon, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84ab696-a179-4053-a6b6-7bd9feb833e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788f16d1-b8f5-4813-bee3-0747e27c62c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095a425f-c451-426d-aecc-918e6ba66003",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.interpolate import interp1d\n",
    "from sklearn.utils import resample\n",
    "\n",
    "alt_train['datetime'] = pd.to_datetime(alt_train['datetime'])\n",
    "alt_train = alt_train.sort_values(by=['lat', 'lon', 'datetime'])\n",
    "alt_valid['datetime'] = pd.to_datetime(alt_valid['datetime'])\n",
    "alt_valid = alt_valid.sort_values(by=['lat', 'lon', 'datetime'])\n",
    "alt_test['datetime'] = pd.to_datetime(alt_test['datetime'])\n",
    "alt_test = alt_test.sort_values(by=['lat', 'lon', 'datetime'])\n",
    "\n",
    "alt_train=alt_train.reset_index(drop=True)\n",
    "alt_valid=alt_valid.reset_index(drop=True)\n",
    "alt_test=alt_train.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa7e90f-107b-4f0a-ad03-0976f63e2251",
   "metadata": {},
   "outputs": [],
   "source": [
    "#alt_train=alt_train.groupby(['datetime', 'lat', 'lon']).agg({'alt': 'mean'}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f477e7cb-aff5-4d48-a1aa-ff8ae22d93cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "\n",
    "def bootstrap_uncertainty(group, n_iterations=1000):\n",
    "    bootstrapped_means = []\n",
    "    for _ in range(n_iterations):\n",
    "        bootstrapped_sample = resample(group['alt'].dropna())\n",
    "        if len(bootstrapped_sample) > 1:\n",
    "            interpolated = bootstrapped_sample.interpolate()\n",
    "            bootstrapped_means.append(interpolated.mean())\n",
    "        else:\n",
    "            bootstrapped_means.append(bootstrapped_sample.iloc[0])\n",
    "    \n",
    "    # Calculate the standard deviation of the bootstrapped means\n",
    "    return np.std(bootstrapped_means)\n",
    "\n",
    "def create_sequences(data, sequence_length=12):\n",
    "    sequences = []\n",
    "    for i in range(len(data) - sequence_length + 1):\n",
    "        sequences.append(data[i:i + sequence_length])\n",
    "    return np.array(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86efd126-e292-4032-949d-d7ff5277c3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_train['bootstrap_uncertainty'] = alt_train.groupby(['lat', 'lon']).apply(bootstrap_uncertainty).\\\n",
    "reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191bf736-4f7c-4d18-85e4-7a7ffeb922d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_valid['bootstrap_uncertainty'] = alt_valid.groupby(['lat', 'lon']).apply(bootstrap_uncertainty).\\\n",
    "reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd0587f-1443-45d0-9e67-ec34062d8fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_test['bootstrap_uncertainty'] = alt_test.groupby(['lat', 'lon']).apply(bootstrap_uncertainty).\\\n",
    "reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d79445-d611-4541-8674-66332f7b766d",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_train_df = alt_train[['datetime', 'lat', 'lon', 'alt', 'bootstrap_uncertainty']].copy()\n",
    "alt_train_pivot_df = alt_train_df.pivot_table(index='datetime', columns=['lat', 'lon'], \n",
    "                                                    values=['alt', 'bootstrap_uncertainty'])\n",
    "alt_train_sequences = create_sequences(alt_train_pivot_df['alt'].values, sequence_length=12)\n",
    "alt_train_uncertainty_sequences = create_sequences(alt_train_pivot_df['bootstrap_uncertainty'].values, \n",
    "                                                   sequence_length=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e61d7b-8e21-4a04-b9e5-ebd0dc9e826b",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_valid_df = alt_valid[['datetime', 'lat', 'lon', 'alt', 'bootstrap_uncertainty']].copy()\n",
    "alt_valid_pivot_df = alt_valid_df.pivot_table(index='datetime', columns=['lat', 'lon'], \n",
    "                                                    values=['alt', 'bootstrap_uncertainty'])\n",
    "alt_valid_sequences = create_sequences(alt_valid_pivot_df['alt'].values, sequence_length=12)\n",
    "alt_valid_uncertainty_sequences = create_sequences(alt_valid_pivot_df['bootstrap_uncertainty'].values, \n",
    "                                                   sequence_length=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91318ea-6319-4ac1-b79d-468b91312586",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_test_df = alt_train[['datetime', 'lat', 'lon', 'alt', 'bootstrap_uncertainty']].copy()\n",
    "alt_test_pivot_df = alt_test_df.pivot_table(index='datetime', columns=['lat', 'lon'], \n",
    "                                                  values=['alt', 'bootstrap_uncertainty'])\n",
    "alt_test_sequences = create_sequences(alt_test_pivot_df['alt'].values, sequence_length=12)\n",
    "alt_test_uncertainty_sequences = create_sequences(alt_test_pivot_df['bootstrap_uncertainty'].values, \n",
    "                                                  sequence_length=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8f54ca-1d77-464b-ae6a-b2baad562950",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_train_lat_unique = np.sort(alt_train['lat'].unique())\n",
    "alt_train_lon_unique = np.sort(alt_train['lon'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab28a21-af6d-474c-97a0-97e6e7e5f80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_train_sequences.shape, alt_valid_sequences.shape, alt_test_sequences.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3e6189-3f0f-40fd-af7a-ade63c0bca43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b19f36-4882-4bc5-a920-f37f5e40872d",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_train_array = alt_train_sequences.reshape(alt_train_sequences.shape[0], 12, len(alt_train_lat_unique), \n",
    "                                                    len(alt_train_lon_unique), 1)\n",
    "alt_train_uncertainty_array = alt_train_uncertainty_sequences.reshape(alt_train_uncertainty_sequences.shape[0], 12, \n",
    "                                                            len(alt_train_lat_unique), len(alt_train_lon_unique), 1)\n",
    "alt_train_combined_input = np.concatenate([alt_train_array, alt_train_uncertainty_array], axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113b0305-e5d4-4823-b6f9-fd6682d67088",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_array = sequences.reshape(sequences.shape[0], 12, len(lat_unique), len(lon_unique), 1)\n",
    "uncertainty_array = uncertainty_sequences.reshape(uncertainty_sequences.shape[0], 12, len(lat_unique), len(lon_unique), 1)\n",
    "\n",
    "# If needed, you can concatenate both arrays to feed into a model\n",
    "combined_input = np.concatenate([final_array, uncertainty_array], axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2fd4e60-c394-4a55-b63b-1283e661b7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_array = sequences.reshape(sequences.shape[0], 12, len(lat_unique), len(lon_unique), 1)\n",
    "uncertainty_array = uncertainty_sequences.reshape(uncertainty_sequences.shape[0], 12, len(lat_unique), len(lon_unique), 1)\n",
    "\n",
    "# If needed, you can concatenate both arrays to feed into a model\n",
    "combined_input = np.concatenate([final_array, uncertainty_array], axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcbfdf6b-d910-4d34-99e0-e2b351c4f1de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af151c0a-7b4a-4609-bd4a-7af426a1913c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af50e1bc-f85f-4504-99e2-a138248c4e60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff53814-4e17-42b0-a3f6-148c93ebf601",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5773300-0e70-4302-a472-df11f26dd133",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81a4ae6-2054-4e44-9d15-a51487f93b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grouped = alt_train.groupby(['lat', 'lon', 'datetime']).apply(interpolate_with_uncertainty).reset_index(drop=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc43dd8-77c5-492e-ae0d-ff07cf78ce55",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f336a9ae-f2f5-4003-ae17-60d7c3382031",
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot_df = df_grouped.pivot_table(index='datetime', columns=['lat', 'lon'], values=['interpolated_alt', 'uncertainty'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05bf8fb-c1fb-4162-b00c-8c0f662dcd80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(data, sequence_length=12):\n",
    "    sequences = []\n",
    "    for i in range(len(data) - sequence_length + 1):\n",
    "        sequences.append(data[i:i + sequence_length])\n",
    "    return np.array(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545b67c1-3e7a-448f-a772-71153a760980",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = create_sequences(pivot_df['interpolated_alt'].values, sequence_length=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb25b05-a2a3-48c5-8f1e-770e1d4bc1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "uncertainty_sequences = create_sequences(pivot_df['uncertainty'].values, sequence_length=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e376ab-5697-491d-be45-d5796eea3398",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_array = sequences.reshape(sequences.shape[0], 12, len(lat_unique), len(lon_unique), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7fb65c-1872-4a09-bfcb-5c4691f05569",
   "metadata": {},
   "outputs": [],
   "source": [
    "uncertainty_array = uncertainty_sequences.reshape(uncertainty_sequences.shape[0], 12, len(lat_unique), len(lon_unique), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "590cc3e4-2fba-42e5-b163-9aa854bdfb71",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(final_array.shape)  # Shape of the interpolated alt data\n",
    "print(uncertainty_array.shape)  # Shape of the associated uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6359a8d5-608c-4df4-a4c1-1f2c70bc0685",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_input = np.concatenate([final_array, uncertainty_array], axis=-1)\n",
    "\n",
    "print(combined_input.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5d56aa-193d-4e3d-acea-720816b24914",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import ConvLSTM2D, BatchNormalization, Conv3D, Flatten, Dense, Dropout\n",
    "\n",
    "# Assuming your input shape is (samples, 12, lat, lon, 1) for alt data\n",
    "# If including uncertainty as a second channel, the shape would be (samples, 12, lat, lon, 2)\n",
    "\n",
    "# Define the model\n",
    "model = Sequential()\n",
    "\n",
    "# ConvLSTM2D layer: This layer captures the spatiotemporal patterns in the data\n",
    "model.add(ConvLSTM2D(filters=64, \n",
    "                     kernel_size=(3, 3), \n",
    "                     input_shape=(12, len(lat_unique), len(lon_unique), 1),  # Use (12, lat, lon, 2) if including uncertainty\n",
    "                     padding='same', \n",
    "                     return_sequences=True))\n",
    "model.add(BatchNormalization())  # Normalize after each ConvLSTM2D layer to stabilize training\n",
    "\n",
    "# Adding more ConvLSTM2D layers for deeper modeling of the temporal-spatial data\n",
    "model.add(ConvLSTM2D(filters=64, \n",
    "                     kernel_size=(3, 3), \n",
    "                     padding='same', \n",
    "                     return_sequences=True))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "# Final ConvLSTM2D layer before flattening the outputs\n",
    "model.add(ConvLSTM2D(filters=64, \n",
    "                     kernel_size=(3, 3), \n",
    "                     padding='same', \n",
    "                     return_sequences=False))  # return_sequences=False to get a 3D output (lat, lon, filters)\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "# Optionally, apply a 3D convolution to extract features across the depth of the ConvLSTM output\n",
    "model.add(Conv3D(filters=64, \n",
    "                 kernel_size=(3, 3, 3), \n",
    "                 activation='relu', \n",
    "                 padding='same'))\n",
    "\n",
    "# Flatten the 3D output to feed into dense layers\n",
    "model.add(Flatten())\n",
    "\n",
    "# Fully connected layers to learn from the high-level features\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.5))  # Dropout for regularization\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='linear'))  # Final output layer (regression output for alt value)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Print model summary to inspect architecture\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32fd5a1-2b82-4685-84b7-3b82f3ec2359",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "history = model.fit(combined_input, y_train, \n",
    "                    validation_data=(X_valid, y_valid), \n",
    "                    epochs=100, \n",
    "                    batch_size=32, \n",
    "                    callbacks=[early_stopping_cb, lr_scheduler_cb])\n",
    "\n",
    "# Evaluate on test set\n",
    "test_loss = model.evaluate(X_test, y_test)\n",
    "print(f'Test Loss: {test_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c80c1847-e93c-4558-817a-3b00539d543c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3153f2c-bcd0-4fe6-a833-1086d0e3e963",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_train=alt_train.interpolate(method='linear', limit_direction='forward').dropna()\n",
    "alt_train_pivot = alt_train.pivot_table(index='datetime', columns=['lat', 'lon'], values='alt')\n",
    "alt_train_pivot = alt_train_pivot.fillna(0)\n",
    "alt_valid=alt_valid.interpolate(method='linear', limit_direction='forward').dropna()\n",
    "alt_valid_pivot = alt_valid.pivot_table(index='datetime', columns=['lat', 'lon'], values='alt')\n",
    "alt_valid_pivot = alt_valid_pivot.fillna(0)\n",
    "alt_test=alt_test.interpolate(method='linear', limit_direction='forward').dropna()\n",
    "alt_test_pivot = alt_test.pivot_table(index='datetime', columns=['lat', 'lon'], values='alt')\n",
    "alt_test_pivot = alt_test_pivot.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360f78bd-49e2-4458-9f4f-0370700d3e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "common_coords = set(alt_train_pivot.columns).intersection(set(alt_valid_pivot.columns)).intersection(set(alt_test_pivot.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b2aff3-ca69-493e-8e0b-db4ed206e84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_train_pivot = alt_train_pivot[list(common_coords)]\n",
    "spatial_coords = np.array(alt_train_pivot.columns.values.tolist())\n",
    "alt_train_data = alt_train_pivot.values\n",
    "alt_train_data = alt_train_data.reshape((alt_train_data.shape[0], alt_train_data.shape[1], 1))\n",
    "spatial_coords_repeated = np.repeat(spatial_coords[np.newaxis, :, :], alt_train_data.shape[0], axis=0)\n",
    "alt_train_with_coords = np.concatenate([alt_train_data, spatial_coords_repeated], axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3febd72b-aadf-48eb-b684-ce1f451b1b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_train_with_coords.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5892e20a-fb3c-4e67-8ce2-d0ff580ef57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_valid_pivot = alt_valid_pivot[list(common_coords)]\n",
    "spatial_coords = np.array(alt_valid_pivot.columns.values.tolist())\n",
    "alt_valid_data = alt_valid_pivot.values\n",
    "alt_valid_data = alt_valid_data.reshape((alt_valid_data.shape[0], alt_valid_data.shape[1], 1))\n",
    "spatial_coords_repeated = np.repeat(spatial_coords[np.newaxis, :, :], alt_valid_data.shape[0], axis=0)\n",
    "alt_valid_with_coords = np.concatenate([alt_valid_data, spatial_coords_repeated], axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ae6687-21c0-4b53-a705-473d295a4236",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_valid_with_coords.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a83b1a3-434e-4c42-b06b-e08d85e2085d",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_test_pivot = alt_test_pivot[list(common_coords)]\n",
    "spatial_coords = np.array(alt_test_pivot.columns.values.tolist())\n",
    "alt_test_data = alt_test_pivot.values\n",
    "alt_test_data = alt_test_data.reshape((alt_test_data.shape[0], alt_test_data.shape[1], 1))\n",
    "spatial_coords_repeated = np.repeat(spatial_coords[np.newaxis, :, :], alt_test_data.shape[0], axis=0)\n",
    "alt_test_with_coords = np.concatenate([alt_test_data, spatial_coords_repeated], axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880ad9fc-ab4f-4760-837a-d038c7e08dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_test_with_coords.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52d2ff1-c359-4d19-a5c1-3453bc90aa2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences_with_spatial(data, sequence_length):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - sequence_length):\n",
    "        X.append(data[i:i + sequence_length])  # Include both temporal and spatial data\n",
    "        y.append(data[i + sequence_length, :, 0])  # Predict only the temporal data ('alt')\n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b48fe3-7b04-4344-9276-586bef6ce91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = 12\n",
    "X_train, y_train = create_sequences_with_spatial(alt_train_with_coords, sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b1e2d31-c69b-4b8e-9d37-101159c518f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = 12\n",
    "X_valid, y_valid = create_sequences_with_spatial(alt_valid_with_coords, sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95c86fe-ad4a-4494-a956-034c11194e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = 12\n",
    "X_test, y_test = create_sequences_with_spatial(alt_test_with_coords, sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d131a6-c16c-477f-b43a-87b08ba9337d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"X_train shape:\", X_train.shape)  # Expected: (num_samples, sequence_length, num_locations, 3)\n",
    "print(\"y_train shape:\", y_train.shape)  # Expected: (num_samples, num_locations)\n",
    "print(\"X_valid shape:\", X_valid.shape)\n",
    "print(\"y_valid shape:\", y_valid.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_test shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aedada9-0527-46fa-a5d9-ea8eb62aa28a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.reshape(3121,12,41,3)[:,:,:,1].shape #lat\n",
    "X_train.reshape(3121,12,41,3)[:,:,:,2].shape #lon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e583b4-8f21-481b-a00f-4addf061465f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.reshape(3121,12,368,368,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2b5f75-c959-44c6-bacc-1711e5bbfc34",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_array = np.zeros((3121, 12, 368, 368)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056b1907-f075-40b2-8506-e558e2d68318",
   "metadata": {},
   "outputs": [],
   "source": [
    "lat_values = X_train[:,:,:,1]\n",
    "lon_values = X_train[:,:,:,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a759ec-32a5-42c0-8c02-289dd8a9e892",
   "metadata": {},
   "outputs": [],
   "source": [
    "lat_index_mapping = {}  # Dictionary to map latitude values to grid indices\n",
    "lon_index_mapping = {}  # Dictionary to map longitude values to grid indices\n",
    "\n",
    "# Example of filling in the array\n",
    "for i in range(3121):\n",
    "    for j in range(12):\n",
    "        for k in range(alt_train_data.shape[2]):  # Loop through the spatial locations\n",
    "            lat_index = lat_index_mapping[lat_values[i, j, k]]  # Map the latitude to grid index\n",
    "            lon_index = lon_index_mapping[lon_values[i, j, k]]  # Map the longitude to grid index\n",
    "            final_array[i, j, lat_index, lon_index] = alt_train_data[i, j, k, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2937496d-e6ec-4f23-ba0d-bc173fa2fab1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f343be7-4590-4320-8628-0b596de94647",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ffc7c8-3b93-40b3-b932-f488c91056d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2f612b-52e9-4542-8e8f-b065554ff56a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16331d21-d089-4eed-8b2a-49be0fe3d05f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6587c91-79f1-4f3b-ab52-2dcc361d4854",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(alt_train_data[0,:,:]);\n",
    "# plt.plot(alt_train_data[-1,:,:]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59d5c7b-9b9a-4b3c-a248-7a05d7fed85f",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_train_data.reshape((alt_train_data.shape[0], 184alt_train_data.shape[1], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff44ac1d-fa4b-4f3e-9f63-7accafd97e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = pd.concat([alt_train, alt_valid, alt_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17941fe8-c51c-4ad3-9b6d-607e55735546",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# alt_train['lat_grid'] = alt_train['lat'].round(3)  # or use a more sophisticated spatial binning technique\n",
    "# alt_train['lon_grid'] = alt_train['lon'].round(3)\n",
    "# alt_train_agg = alt_train.groupby(['datetime', 'lat_grid', 'lon_grid']).mean().reset_index()\n",
    "# alt_train_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543d947f-6bcb-402d-a5e1-c56b8a87b2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df=combined_df.interpolate(method='linear', limit_direction='forward').dropna()\n",
    "combined_df_pivot = combined_df.pivot_table(index='datetime', columns=['lat', 'lon'], values='alt')\n",
    "combined_df_pivot = combined_df_pivot.fillna(0)\n",
    "combined_df_data = combined_df_pivot.values\n",
    "combined_df_data = combined_df_data.reshape((combined_df_data.shape[0], combined_df_data.shape[1], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9dc5a9a-17d3-45f6-9501-daf04433cbc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = ch4.interpolate(method='linear', limit_direction='forward').dropna()\n",
    "combined_df_pivot = combined_df.pivot_table(index='datetime', columns=['lat', 'lon'], values='fch4')\n",
    "combined_df_pivot = combined_df_pivot.fillna(0)\n",
    "combined_df_data = combined_df_pivot.values\n",
    "combined_df_data = combined_df_data.reshape((combined_df_data.shape[0], combined_df_data.shape[1], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3b8454-18c7-4fd7-871f-66b8d599efcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ch4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9dae5a6-bbf8-41b4-99b5-d703193cdd18",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt['lat_bin'] = alt['lat'].round(3)  # Adjust the rounding based on your grid resolution\n",
    "alt['lon_bin'] = alt['lon'].round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4394fe3-6bda-44d0-9b41-92ea121ad67a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ch4['lat_bin'] = ch4['lat'].round(3)  # Adjust the rounding based on your grid resolution\n",
    "ch4['lon_bin'] = ch4['lon'].round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b94da742-84b9-4081-84a2-e381d15f0ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt=alt.sort_values(by='datetime',ascending=True)\n",
    "alt=alt.reset_index(drop=True)\n",
    "alt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861813f0-97d0-434b-9d98-234b484d8618",
   "metadata": {},
   "outputs": [],
   "source": [
    "ch4=ch4.sort_values(by='datetime',ascending=True)\n",
    "ch4=ch4.reset_index(drop=True)\n",
    "ch4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa26f69a-28f1-4224-8e9b-27e00b8b759f",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_pivot = alt.pivot_table(index=['datetime'], columns=['lat_bin', 'lon_bin'], values='alt', fill_value=0)\n",
    "alt_pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b968ffe3-3c03-4fc4-b6d5-eb9bd56e1a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ch4_pivot = ch4.pivot_table(index=['datetime'], columns=['lat_bin', 'lon_bin'], values='fch4', fill_value=0)\n",
    "ch4_pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96ed38d-f675-4574-8e4e-71565268b212",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = alt_pivot.values.reshape(alt_pivot.shape[0], 1, *alt_pivot.shape[1:])\n",
    "grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6657754-7ff1-48e8-8534-178f99a7b019",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = ch4_pivot.values.reshape(ch4_pivot.shape[0], 1, *ch4_pivot.shape[1:])\n",
    "grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b4cc66-86e8-4c27-92c0-9f613d618cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d47f676-73f4-4231-a8e8-47f85266bcac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85a1b4d-10f4-4d07-bbb4-5927d0ac85ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(data, sequence_length):\n",
    "    X = []\n",
    "    y = []\n",
    "    for i in range(len(data) - sequence_length):\n",
    "        X.append(data[i:i + sequence_length])\n",
    "        y.append(data[i + sequence_length])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Assuming grid has the shape (time_steps, rows, columns, 1)\n",
    "X, y = create_sequences(grid, sequence_length=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9b6620-a1f6-4cd2-aade-2a27a18719a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17ce9af-65ba-4bad-aee6-8af341a851b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Volumes/JPL/alt_valid.parquet','rb') as f:\n",
    "    alt_valid=pd.read_parquet(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d0e180-1c57-46a3-a013-0e917cb016bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.7 * len(X))\n",
    "valid_size = int(0.15 * len(X))\n",
    "test_size = len(X) - train_size - valid_size\n",
    "\n",
    "X_train = X[:train_size]\n",
    "y_train = y[:train_size]\n",
    "\n",
    "X_valid = X[train_size:train_size + valid_size]\n",
    "y_valid = y[train_size:train_size + valid_size]\n",
    "\n",
    "X_test = X[train_size + valid_size:]\n",
    "y_test = y[train_size + valid_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8072c14-2141-4f4c-9b49-35260e1a42e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.reshape((y_train.shape[0], 1, y_train.shape[1], y_train.shape[2], 1))\n",
    "y_valid = y_valid.reshape((y_valid.shape[0], 1, y_valid.shape[1], y_valid.shape[2], 1))\n",
    "y_test = y_test.reshape((y_test.shape[0], 1, y_test.shape[1], y_test.shape[2], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1119fe4-3768-419a-9d80-1e4174384dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_alt=alt[alt.datetime<='2017-06-19']\n",
    "# valid_alt=alt[(alt.datetime >= '2017-07-01') & (alt.datetime <= '2021-12-01')]\n",
    "# test_alt=alt[alt.datetime>='2022-01-01']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc23f81-17c9-4160-9500-6d59c5335581",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_alt.shape, valid_alt.shape, test_alt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8a7803-9594-4447-aae4-991b22fc7f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_alt['lat_bin'] = train_alt['lat'].round(3)\n",
    "# train_alt['lon_bin'] = train_alt['lon'].round(3)\n",
    "# valid_alt['lat_bin'] = valid_alt['lat'].round(3)\n",
    "# valid_alt['lon_bin'] = valid_alt['lon'].round(3)\n",
    "# test_alt['lat_bin'] = test_alt['lat'].round(3)\n",
    "# test_alt['lon_bin'] = test_alt['lon'].round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06730846-2a23-4b2b-b8c0-5d0d9f5d502c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_alt_pivot = train_alt.pivot_table(index=['datetime'], columns=['lat_bin', 'lon_bin'], values='alt', fill_value=0)\n",
    "# valid_alt_pivot = valid_alt.pivot_table(index=['datetime'], columns=['lat_bin', 'lon_bin'], values='alt', fill_value=0)\n",
    "# test_alt_pivot = test_alt.pivot_table(index=['datetime'], columns=['lat_bin', 'lon_bin'], values='alt', fill_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ccab11-fbaa-4c3a-a127-a4f40fc5ba1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_alt_grid = train_alt_pivot.values.reshape(train_alt_pivot.shape[0], 1, *train_alt_pivot.shape[1:])\n",
    "# valid_alt_grid = valid_alt_pivot.values.reshape(valid_alt_pivot.shape[0], 1, *valid_alt_pivot.shape[1:])\n",
    "# test_alt_grid = test_alt_pivot.values.reshape(test_alt_pivot.shape[0], 1, *test_alt_pivot.shape[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beaac9fd-08a5-4ab7-824c-8c4bf7f0ee56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(data, sequence_length):\n",
    "    X = []\n",
    "    y = []\n",
    "    for i in range(len(data) - sequence_length):\n",
    "        X.append(data[i:i + sequence_length])\n",
    "        y.append(data[i + sequence_length])\n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e01609f-051f-48d8-9345-e6cf535d5e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_alt_X, train_alt_y = create_sequences(train_alt_grid, sequence_length=12)\n",
    "valid_alt_X, valid_alt_y = create_sequences(valid_alt_grid, sequence_length=12)\n",
    "test_alt_X, test_alt_y = create_sequences(test_alt_grid, sequence_length=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c24dfbe-8042-452d-8b03-4e229abfb0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_alt_X.shape, train_alt_y.shape, valid_alt_X.shape, valid_alt_y.shape, test_alt_X.shape, test_alt_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784386da-2d1c-437d-96e5-2391ef4fed76",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_alt_y = train_alt_y.reshape((train_alt_y.shape[0], 1, train_alt_y.shape[1], train_alt_y.shape[2]))\n",
    "valid_alt_y = valid_alt_y.reshape((valid_alt_y.shape[0], 1, valid_alt_y.shape[1], valid_alt_y.shape[2]))\n",
    "test_alt_y = test_alt_y.reshape((test_alt_y.shape[0], 1, test_alt_y.shape[1], test_alt_y.shape[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864d8ae9-7cef-4670-8d90-78f196637d65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92145aa9-319b-46e4-99d3-f8a6ba95839d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_alt_X.shape, train_alt_y.shape, valid_alt_X.shape, valid_alt_y.shape, test_alt_X.shape, test_alt_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1aaeaf-6bfc-4bdb-b33f-f5ed6664aceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "X_train_sc = scaler.fit_transform(X_train.reshape(-1, X_train.shape[-1])).reshape(X_train.shape)\n",
    "X_valid_sc = scaler.transform(X_valid.reshape(-1, X_valid.shape[-1])).reshape(X_valid.shape)\n",
    "X_test_sc = scaler.transform(X_test.reshape(-1, X_test.shape[-1])).reshape(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31ff80a-4847-41e1-862a-e69bbb9d1318",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6dea6f-40f4-4cad-8d2b-ddb5ec1eed2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae19e9cc-3471-4ee8-9993-9868c7ec1b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_sc = X_train_sc.reshape((X_train_sc.shape[0], X_train_sc.shape[1], 1, 1, X_train_sc.shape[3]))\n",
    "valid_alt_X = valid_alt_X.reshape((valid_alt_X.shape[0], valid_alt_X.shape[1], 1, 1, valid_alt_X.shape[3]))\n",
    "test_alt_X = test_alt_X.reshape((test_alt_X.shape[0], test_alt_X.shape[1], 1, 1, test_alt_X.shape[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53e7bfe-c481-47e5-bc03-f129f0fb55de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "X_sc = scaler.fit_transform(X.reshape(-1, X.shape[-1])).reshape(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75d140f-4d25-45d7-9873-6644550d3197",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_sc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38c127d-2826-4b45-bcf8-efd04cd6209b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(hp):\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(ConvLSTM2D(\n",
    "        filters=hp.Int('filters_1', min_value=16, max_value=64, step=16), \n",
    "        kernel_size=hp.Choice('kernel_size_1', values=[1, 1]),\n",
    "        input_shape=(12, 1, 1, 360),\n",
    "        #input_shape=(X_train.shape[1], 1, 1, X_train.shape[3]),\n",
    "        padding='same', \n",
    "        return_sequences=True,\n",
    "        activation=hp.Choice('activation_function_1', values=['relu','tanh','softmax','sigmoid','leaky_relu','swish'])\n",
    "    ))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(ConvLSTM2D(\n",
    "        filters=hp.Int('filters_2', min_value=16, max_value=64, step=16), \n",
    "        kernel_size=hp.Choice('kernel_size_2', values=[1, 1]),\n",
    "        padding='same', \n",
    "        return_sequences=True,\n",
    "        activation=hp.Choice('activation_function_2', values=['relu','tanh','softmax','sigmoid','leaky_relu','swish'])\n",
    "    ))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    # Extract the last time step\n",
    "    model.add(Lambda(lambda x: x[:, -1, :, :, :]))  # Shape becomes [batch_size, height, width, channels]\n",
    "\n",
    "    # Conv2D Layer for output (single frame prediction)\n",
    "    model.add(Conv2D(\n",
    "        filters=1,  # Ensure only one output channel to match y_train's shape\n",
    "        kernel_size=(1, 1),  # Use a 2D kernel size\n",
    "        activation=hp.Choice('activation_function_3', values=['relu','tanh','softmax','sigmoid','leaky_relu','swish']),\n",
    "        padding='same',\n",
    "        data_format='channels_last'\n",
    "    ))\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(\n",
    "            hp.Choice('learning_rate', values=[1e-3, 1e-4, 1e-5])\n",
    "        ),\n",
    "        loss='mse',\n",
    "        metrics=['mae','mse','mape','accuracy']\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cbc967c-c78c-4252-9d3c-5f1ceb7be2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)  # Check the shape of the training data\n",
    "print(X_valid.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c68c84-3a34-4587-ae1b-409cc0afd662",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32  # Adjust based on memory and GPU capability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec0dd74-458c-470b-a51c-78dbbac95492",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "train_dataset = train_dataset.cache().shuffle(1000).batch(batch_size).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "valid_dataset = tf.data.Dataset.from_tensor_slices((X_valid, y_valid))\n",
    "valid_dataset = valid_dataset.batch(batch_size).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
    "test_dataset = test_dataset.batch(batch_size).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f4bb0c-ba1f-4121-b80a-03da68045435",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define callbacks\n",
    "early_stopping_cb = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "lr_scheduler_cb = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2)\n",
    "checkpoint_cb = ModelCheckpoint('model_checkpoint.keras', save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e771cd76-1f5e-44cb-8dc7-df1c62532098",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_tuner import BayesianOptimization\n",
    "\n",
    "# Define the tuner\n",
    "tuner = BayesianOptimization(\n",
    "    build_model,\n",
    "    objective='val_loss',\n",
    "    max_trials=10,\n",
    "    executions_per_trial=1,\n",
    "    directory='alt_train',\n",
    "    project_name='alt_train_conv3dlstm_tuning',\n",
    "    #directory='ch4_train',\n",
    "    #project_name='ch4_train_conv3dlstm_tuning',\n",
    "    overwrite=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da4727c-aa3d-4ff1-a1be-72d84ced935a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)  # Check the shape of the training data\n",
    "print(X_valid.shape)  # Check the shape of the validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b019ade-1d7d-403b-b8ad-319dc1060a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1, 1, X_train.shape[3]))\n",
    "X_valid = X_valid.reshape((X_valid.shape[0], X_valid.shape[1], 1, 1, X_valid.shape[3]))\n",
    "X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1, 1, X_test.shape[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5498c2-022e-4578-9278-259ff28fd1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)  # Check the shape of the training data\n",
    "print(X_valid.shape)  # Check the shape of the validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b87424-ccbd-41ef-a95f-11009247959d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tuner.search(train_dataset, epochs=100, validation_data=valid_dataset, callbacks=[early_stopping_cb, lr_scheduler_cb])\n",
    "tuner.search(X_train, y_train, epochs=100, validation_data=(X_valid, y_valid), callbacks=[early_stopping_cb, lr_scheduler_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e04dad-2cbd-4b47-8655-6a830d09e42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = tuner.get_best_models(num_models=1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2867157a-a874-46e4-b868-fcd3daf94558",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(best_model, to_file='/Volumes/JPL/alt_model.png', show_shapes=True, show_layer_names=True, dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21647e6-d5d8-4cf2-89cd-2e2cc01fce9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d372e1de-dd81-4da1-8649-3ee7885c22fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.results_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38046df8-eae2-46ac-9c2e-a4efea8dd785",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the best model on the test data\n",
    "test_loss = best_model.evaluate(X_test, y_test)\n",
    "print(f'Test Loss: {test_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22cad3b4-b37e-4a2b-8d7d-81574eee269f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#history=# Continue training (fine-tuning) the best model with additional data or epochs\n",
    "history = best_model.fit(X_train, y_train, epochs=epochs, validation_data=(X_valid, y_valid), \n",
    "                 epochs2=additional_epochs, validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b78cf4b9-3d9d-407e-8840-b80f95a47281",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_test_loss, final_test_mae = best_model.evaluate(train_dataset)\n",
    "print(f\"Final Test Loss: {final_test_loss}, Final Test MAE: {final_test_mae}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2d1e0e-d359-442a-8346-aa83e8b5d72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = best_model.predict(test_dataset)\n",
    "\n",
    "# If using a scaler, inverse transform the predictions and actual values\n",
    "predictions_original = scaler_alt.inverse_transform(predictions.reshape(-1, 1)).reshape(predictions.shape)\n",
    "actual_values_original = scaler_alt.inverse_transform(y_test.reshape(-1, 1)).reshape(y_test.shape)\n",
    "\n",
    "# Compare first few predictions with actual values\n",
    "for i in range(5):\n",
    "    print(f\"Prediction {i+1}: {predictions_original[i, :, :, 0]}\")\n",
    "    print(f\"Actual Value {i+1}: {actual_values_original[i, :, :, 0]}\")\n",
    "    \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title('Predicted (Original Scale)')\n",
    "plt.imshow(predictions_original[0, :, :, 0], cmap='viridis')\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title('Actual (Original Scale)')\n",
    "plt.imshow(actual_values_original[0, :, :, 0], cmap='viridis')\n",
    "plt.colorbar()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5234d3e6-f9dd-4615-b953-cdeb91c3489f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train\n",
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37cbbfc1-7b95-4129-8d0d-8082120f8508",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05888b2f-6430-4bd8-be48-52922d335927",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grouped = ch4.groupby(['datetime', 'lat', 'lon']).agg({'fch4': ['mean', 'std', 'min', 'max', 'count']}).reset_index()\n",
    "df_grouped.columns = ['datetime', 'lat', 'lon', 'fch4_mean', 'fch4_std', 'fch4_min', 'fch4_max', 'fch4_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17811b40-455a-4253-94f9-a28562dbebde",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grouped = co2.groupby(['datetime', 'lat', 'lon']).agg({'fc': ['mean', 'std', 'min', 'max', 'count']}).reset_index()\n",
    "df_grouped.columns = ['datetime', 'lat', 'lon', 'fc_mean', 'fc_std', 'fc_min', 'fc_max', 'fc_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56669993-c2d2-451b-ae91-8b3eb850c786",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pivoted = ch4.pivot_table(index=['lat', 'lon'], columns='datetime', values='fch4').reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12ca0ca-4921-4ed7-b334-405b5b5131f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pivoted = co2.pivot_table(index=['lat', 'lon'], columns='datetime', values='fc').reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3338888b-f03d-4854-87eb-765885538182",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pivoted.interpolate(method='linear', limit_direction='both', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba13cf9-58c6-4b66-a78f-e79035f95ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pivoted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6115bf81-147a-4b2f-91f8-3a10bef8bb1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pivoted.iloc[:,174:223]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8853a040-b484-47dd-8b95-67110a9fa102",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df=df_pivoted.iloc[:,:174]\n",
    "valid_df=pd.concat([df_pivoted.iloc[:,:2],df_pivoted.iloc[:,174:223]],axis=1)\n",
    "test_df=pd.concat([df_pivoted.iloc[:,:2],df_pivoted.iloc[:,223:]],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c211ef-99d9-4732-b548-9327d47f1d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "lat_lon_columns = ['lat', 'lon']\n",
    "time_series_columns = train_df.columns.difference(lat_lon_columns)\n",
    "data = train_df[time_series_columns].values\n",
    "sequence_length = 12  # for example, using 12 months to predict the next month\n",
    "X_train = []\n",
    "y_train = []\n",
    "\n",
    "for i in range(len(data) - sequence_length):\n",
    "    X_train.append(data[i:i+sequence_length])\n",
    "    y_train.append(data[i+sequence_length])\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "lat_lon_values = train_df[lat_lon_columns].values[:len(X_train)]\n",
    "lat_lon_repeated = np.repeat(lat_lon_values[:, np.newaxis, :], sequence_length, axis=1)\n",
    "\n",
    "X_train = np.concatenate([lat_lon_repeated, X_train], axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff932eb3-e20c-442b-bb24-0fa0bdab1281",
   "metadata": {},
   "outputs": [],
   "source": [
    "lat_lon_columns = ['lat', 'lon']\n",
    "time_series_columns = valid_df.columns.difference(lat_lon_columns)\n",
    "data = valid_df[time_series_columns].values\n",
    "sequence_length = 12  # for example, using 12 months to predict the next month\n",
    "X_valid = []\n",
    "y_valid = []\n",
    "\n",
    "for i in range(len(data) - sequence_length):\n",
    "    X_valid.append(data[i:i+sequence_length])\n",
    "    y_valid.append(data[i+sequence_length])\n",
    "\n",
    "X_valid = np.array(X_valid)\n",
    "y_valid = np.array(y_valid)\n",
    "\n",
    "lat_lon_values = valid_df[lat_lon_columns].values[:len(X_valid)]\n",
    "lat_lon_repeated = np.repeat(lat_lon_values[:, np.newaxis, :], sequence_length, axis=1)\n",
    "\n",
    "X_valid = np.concatenate([lat_lon_repeated, X_valid], axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1317ffc-8468-4fa7-8f19-c7220769acc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "lat_lon_columns = ['lat', 'lon']\n",
    "time_series_columns = test_df.columns.difference(lat_lon_columns)\n",
    "data = test_df[time_series_columns].values\n",
    "sequence_length = 12  # for example, using 12 months to predict the next month\n",
    "X_test = []\n",
    "y_test = []\n",
    "\n",
    "for i in range(len(data) - sequence_length):\n",
    "    X_test.append(data[i:i+sequence_length])\n",
    "    y_test.append(data[i+sequence_length])\n",
    "\n",
    "X_test = np.array(X_test)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "lat_lon_values = test_df[lat_lon_columns].values[:len(X_test)]\n",
    "lat_lon_repeated = np.repeat(lat_lon_values[:, np.newaxis, :], sequence_length, axis=1)\n",
    "\n",
    "X_test = np.concatenate([lat_lon_repeated, X_test], axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481740b1-edfb-4f5d-a684-291ab13afa4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"X_train shape:\", X_train.shape)  # Expected: (num_samples, sequence_length, num_features + 2)\n",
    "print(\"y_train shape:\", y_train.shape)  # Expected: (num_samples, num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154bd889-eb30-4b2d-ae3f-23b85a562e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"X_valid shape:\", X_valid.shape)  # Expected: (num_samples, sequence_length, num_features + 2)\n",
    "print(\"y_valid shape:\", y_valid.shape)  # Expected: (num_samples, num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b87749-1a2a-4de0-8316-751e2a2c9ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"X_test shape:\", X_test.shape)  # Expected: (num_samples, sequence_length, num_features + 2)\n",
    "print(\"y_test shape:\", y_test.shape)  # Expected: (num_samples, num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6afe80-1349-4897-91cc-96c91472fb7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "27*12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87af5215-87dc-45d0-b71d-aaa71dc56a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_reshaped = X_train.reshape((X_train.shape[0], X_train.shape[1], 1, 1, X_train.shape[2]))\n",
    "X_valid_reshaped = X_valid.reshape((X_valid.shape[0], X_valid.shape[1], 1, 1, X_valid.shape[2]))\n",
    "X_test_reshaped = X_test.reshape((X_test.shape[0], X_test.shape[1], 1, 1, X_test.shape[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9197be11-55c8-4473-86fa-032a603e20e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(hp):\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(ConvLSTM2D(\n",
    "        filters=hp.Int('filters_1', min_value=16, max_value=64, step=16), \n",
    "        kernel_size=hp.Choice('kernel_size_1', values=[1, 1]),\n",
    "        input_shape=(X_train_reshaped.shape[1], 1, 1, X_train_reshaped.shape[4]),\n",
    "        padding='same', \n",
    "        return_sequences=True,\n",
    "        activation=hp.Choice('activation_function_1', values=['relu','tanh','softmax','sigmoid','leaky_relu','swish'])\n",
    "    ))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(ConvLSTM2D(\n",
    "        filters=hp.Int('filters_2', min_value=16, max_value=64, step=16), \n",
    "        kernel_size=hp.Choice('kernel_size_2', values=[1, 1]),\n",
    "        padding='same', \n",
    "        return_sequences=True,\n",
    "        activation=hp.Choice('activation_function_2', values=['relu','tanh','softmax','sigmoid','leaky_relu','swish'])\n",
    "    ))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    # Extract the last time step\n",
    "    model.add(Lambda(lambda x: x[:, -1, :, :, :]))  # Shape becomes [batch_size, height, width, channels]\n",
    "\n",
    "    # Conv2D Layer for output (single frame prediction)\n",
    "    model.add(Conv2D(\n",
    "        filters=1,  # Ensure only one output channel to match y_train's shape\n",
    "        kernel_size=(1, 1),  # Use a 2D kernel size\n",
    "        activation=hp.Choice('activation_function_3', values=['relu','tanh','softmax','sigmoid','leaky_relu','swish']),\n",
    "        padding='same',\n",
    "        data_format='channels_last'\n",
    "    ))\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(\n",
    "            hp.Choice('learning_rate', values=[1e-3, 1e-4, 1e-5])\n",
    "        ),\n",
    "        loss='mse',\n",
    "        metrics=['mae','mse','mape','accuracy']\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44449ade-d631-448c-bc8a-dbbbf59ebf81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(hp):\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(ConvLSTM2D(\n",
    "        filters=64,\n",
    "        kernel_size=(1,1),\n",
    "        input_shape=(X_train_reshaped.shape[1], 1, 1, X_train_reshaped.shape[4]),\n",
    "        padding='same', \n",
    "        return_sequences=True,\n",
    "        activation='swish',\n",
    "    ))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    model.add(ConvLSTM2D(\n",
    "        filters=32,\n",
    "        kernel_size=(1,1),\n",
    "        padding='same', \n",
    "        return_sequences=True,\n",
    "        activation='relu',\n",
    "    ))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    # Extract the last time step\n",
    "    model.add(Lambda(lambda x: x[:, -1, :, :, :]))  # Shape becomes [batch_size, height, width, channels]\n",
    "    \n",
    "    # Conv2D Layer for output (single frame prediction)\n",
    "    model.add(Conv2D(\n",
    "        filters=1,  # Ensure only one output channel to match y_train's shape\n",
    "        kernel_size=(1, 1),  # Use a 2D kernel size\n",
    "        activation='tanh',\n",
    "        padding='same',\n",
    "        data_format='channels_last'\n",
    "    ))\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "        loss='mse',\n",
    "        metrics=['mae','mse','mape','accuracy']\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e75e80-595d-4816-a816-2411bd0b97c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_reshaped = X_train.reshape((X_train.shape[0], X_train.shape[1], 1, 1, X_train.shape[2]))\n",
    "X_valid_reshaped = X_valid.reshape((X_valid.shape[0], X_valid.shape[1], 1, 1, X_valid.shape[2]))\n",
    "X_test_reshaped = X_test.reshape((X_test.shape[0], X_test.shape[1], 1, 1, X_test.shape[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ef22cf-ab0a-4407-ae04-9aa2d9a545f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32  # Adjust based on memory and GPU capability\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "train_dataset = train_dataset.cache().shuffle(1000).batch(batch_size).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "valid_dataset = tf.data.Dataset.from_tensor_slices((X_valid, y_valid))\n",
    "valid_dataset = valid_dataset.batch(batch_size).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
    "test_dataset = test_dataset.batch(batch_size).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2901e437-dbd0-4016-a8b2-fa1ee15faee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define callbacks\n",
    "early_stopping_cb = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "lr_scheduler_cb = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2)\n",
    "checkpoint_cb = ModelCheckpoint('model_checkpoint.keras', save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4bac81-31a2-426d-b034-7e089974a40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_tuner import BayesianOptimization\n",
    "\n",
    "# Define the tuner\n",
    "tuner = BayesianOptimization(\n",
    "    build_model,\n",
    "    objective='val_loss',\n",
    "    max_trials=1,\n",
    "    executions_per_trial=1,\n",
    "    directory='ch4_train',\n",
    "    project_name='ch4_train_conv3dlstm_tuning',\n",
    "    overwrite=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ebfa14-10d2-4c44-bd5f-5b311b7c271f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_reshaped = X_train.reshape((X_train.shape[0], X_train.shape[1], 1, 1, X_train.shape[2]))\n",
    "X_valid_reshaped = X_valid.reshape((X_valid.shape[0], X_valid.shape[1], 1, 1, X_valid.shape[2]))\n",
    "X_test_reshaped = X_test.reshape((X_test.shape[0], X_test.shape[1], 1, 1, X_test.shape[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2fccdf-89e7-4f34-95fe-6549fb0bb3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tuner.search(train_dataset, epochs=10, validation_data=valid_dataset, callbacks=[early_stopping_cb, lr_scheduler_cb])\n",
    "tuner.search(X_train_reshaped, y_train, epochs=10, validation_data=(X_valid_reshaped, y_valid), callbacks=[early_stopping_cb, lr_scheduler_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff9f4eb-fc57-41bd-81f6-bb6bec0dde52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54cc4b39-aff2-4d5c-8303-7f5b5fcf7717",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5c5204-2d1d-433f-bbff-1983a95b1ba0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8daa37c3-3d13-4f78-a8d3-dbe013f13a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# ConvLSTM2D layer\n",
    "model.add(ConvLSTM2D(filters=64, kernel_size=(1, 1), input_shape=(X_train_reshaped.shape[1], 1, 1, X_train_reshaped.shape[4]),\n",
    "                     padding='same', return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(ConvLSTM2D(filters=64, kernel_size=(1, 1), padding='same', return_sequences=False))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1))  # Output layer\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mae','accuracy','mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9bc837f-4508-4aa6-9519-955e5a87b551",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68315e0e-859e-4c3a-aea0-9e44692d614b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06256470-29dd-4198-8223-2400ae2d87cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(model, to_file='/Volumes/JPL/ch4_model.png', show_shapes=True, show_layer_names=True, dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdcff129-42d4-4ab2-ac7e-7e67bf3aa586",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(units=32, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(units=32, return_sequences=False))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1))\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c81cd22-68db-489f-932e-3ecb4b3759f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b036eb-4387-4882-b5d8-cd1ae0a14387",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "checkpoint = ModelCheckpoint('/Volumes/JPL/ch4_train/model.keras', save_best_only=True, monitor='val_loss', mode='min')\n",
    "\n",
    "tf.profiler.experimental.start('logdir')\n",
    "\n",
    "history = model.fit(\n",
    "    X_train_reshaped, y_train,\n",
    "    epochs=10,\n",
    "    batch_size=16,\n",
    "    validation_data=(X_valid_reshaped, y_valid),\n",
    "    callbacks=[early_stopping, checkpoint],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "tf.profiler.experimental.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b140c3c-c2ed-4e77-b5df-2047abe2ed22",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.profiler.experimental.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f070c3-3b2e-4dcc-a3ee-7a27d9d34891",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test set\n",
    "test_loss, test_mae = model.evaluate(X_test, y_test)\n",
    "print(f'Test Loss: {test_loss}, Test MAE: {test_mae}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84e3304-a47e-4c73-96b6-9cfc78ccc38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# You may want to inverse the scaling if you applied scaling to your data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3bf3904-1466-46f0-933e-066e2a65d129",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ae8403-afe8-46c0-98ab-67c66e622619",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546c836e-4750-43dd-aa30-dd09b03fe26b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# alt_valid.interpolate(method='linear', limit_direction='forward').dropna()\n",
    "# alt_valid_pivot = alt_valid.pivot_table(index='datetime', columns=['lat', 'lon'], values='alt')\n",
    "# alt_valid_pivot = alt_valid_pivot.fillna(0)\n",
    "# alt_valid_data = alt_valid_pivot.values\n",
    "# alt_valid_data = alt_valid_data.reshape((alt_valid_data.shape[0], alt_valid_data.shape[1], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239555ca-317a-4e46-bf66-eefc182e71db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sliding_windows(data, window_size):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - window_size):\n",
    "        X.append(data[i:i+window_size])\n",
    "        y.append(data[i+window_size])\n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1b6059-d87c-430c-be3b-4c2a4bc629d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 12  # Adjust based on the length of the sequence you want to forecast\n",
    "combined_df_X, combined_df_y = create_sliding_windows(combined_df_data, window_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7bd6ce3-5a00-4f7c-8f94-1ae0036dcb44",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# window_size = 12  # Adjust based on the length of the sequence you want to forecast\n",
    "# alt_valid_X, alt_valid_y = create_sliding_windows(alt_valid_data, window_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ed126e-ef35-4989-9ff2-c524883baabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(combined_df_X, combined_df_y, test_size=0.2, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.15, random_state=42)\n",
    "#X_train=alt_train_X; y_train=alt_train_y; X_valid=alt_valid_X; y_valid=alt_valid_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2b757e-a690-42f7-9a35-30d7b4e9f9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, y_train.shape, X_valid.shape, y_valid.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8f440b-fea2-44d6-82fc-ee1fcdf56924",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(y_train.reshape(-1)).replace(0,np.nan).dropna().reset_index(drop=True).plot();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da0b809-cc31-4314-9bbe-5c5ca5b97d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "#combined_df_pivot.iloc[147] #1996-06-01 to 2015-04-01\n",
    "#combined_df_pivot.iloc[147:147+44] #2015-05-01 to 2019-03-01\n",
    "#combined_df_pivot.iloc[147+44:] #2019-04-01 to 2022-10-01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1acf458-2ed4-4099-b309-74f640ca2700",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "X_scaler = MinMaxScaler()\n",
    "X_train_sc = X_scaler.fit_transform(X_train.reshape(-1,1)).reshape(X_train.shape)\n",
    "X_valid_sc = X_scaler.transform(X_valid.reshape(-1,1)).reshape(X_valid.shape)\n",
    "X_test_sc = X_scaler.transform(X_test.reshape(-1,1)).reshape(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcdf7105-447d-458e-a545-ff5b549ea1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_scaler = MinMaxScaler()\n",
    "y_train_sc = y_scaler.fit_transform(y_train.reshape(-1, 1)).reshape(y_train.shape)\n",
    "y_valid_sc = y_scaler.transform(y_valid.reshape(-1, 1)).reshape(y_valid.shape)\n",
    "y_test_sc = y_scaler.transform(y_test.reshape(-1, 1)).reshape(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57a07fe-69a4-42a7-a490-9dfcf02bae13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import ConvLSTM2D, Flatten, Dense\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, ConvLSTM2D, BatchNormalization, Lambda\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from keras_tuner import BayesianOptimization\n",
    "\n",
    "window_size = 12\n",
    "\n",
    "def build_model(hp):\n",
    "    model = Sequential()\n",
    "    kernel_height = hp.Choice('kernel_height', values=[1, 3])\n",
    "    kernel_width = hp.Choice('kernel_width', values=[1])\n",
    "    model.add(ConvLSTM2D(\n",
    "        filters=hp.Int('filters_1', min_value=32, max_value=128, step=32),\n",
    "        kernel_size=(kernel_height, kernel_width),\n",
    "        activation=hp.Choice('activation_function_1', values=['relu','tanh','linear','sigmoid','leaky_relu','swish']),\n",
    "        input_shape=(window_size, combined_df_data.shape[1], combined_df_data.shape[2], 1)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(\n",
    "            hp.Choice('learning_rate', values=[1e-3, 1e-4, 1e-5])\n",
    "        ),\n",
    "        loss='mse',\n",
    "        metrics=['mae','mape','accuracy']\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac18e6e-249c-4e55-b28f-97fcebb773f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54343dde-4dc5-43e8-a86e-e69ab73ad99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_dataset = tf.data.Dataset.from_tensor_slices((X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b65e159-b956-48fe-b263-8323f4c9c97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5636be01-8acf-46e3-9b67-86d39c744d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a232845b-b8e8-494e-bfc9-c3e1c2096677",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.cache().shuffle(1000).batch(batch_size).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "valid_dataset = valid_dataset.batch(batch_size).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "test_dataset = test_dataset.batch(batch_size).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e6a4c1-a650-49f1-acc1-658e465625c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define callbacks\n",
    "early_stopping_cb = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "lr_scheduler_cb = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2)\n",
    "checkpoint_cb = ModelCheckpoint('model_checkpoint.keras', save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23081c5f-12d9-476d-8566-a056bde6be2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_tuner import BayesianOptimization\n",
    "\n",
    "# Define the tuner\n",
    "tuner = BayesianOptimization(\n",
    "    build_model,\n",
    "    objective='val_loss',\n",
    "    max_trials=10,\n",
    "    executions_per_trial=1,\n",
    "    directory='/Volumes/JPL/ch4_train',\n",
    "    project_name='ch4_train_conv3dlstm_tuning',\n",
    "    overwrite=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568ad524-6ff0-4815-974f-22885e994689",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.search(train_dataset, epochs=10, validation_data=valid_dataset, callbacks=[early_stopping_cb, lr_scheduler_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5daeb9b1-b619-4f6d-b5b1-17ee3dc44751",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = tuner.get_best_models(num_models=1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48faa1b2-d6f2-4835-a864-911a57cf562d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_mae = best_model.evaluate(test_dataset)\n",
    "print(f\"Test Loss: {test_loss}, Test MAE: {test_mae}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691f53e7-e17a-4b96-997c-ddef47250e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.search(X_train, y_train, epochs=100, validation_data=(, y_train), callbacks=[early_stopping_cb, lr_scheduler_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb4e1dc-4824-4e1c-a331-5927f00d9b57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c486d42d-4976-4eb8-869c-9c822aa15abe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b739c9-ab3e-4195-afd9-2c081998a3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on new data\n",
    "# predictions = best_model.predict(new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8beb727a-df84-470f-9ed8-2b4c1a05a343",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.plot(history.history['loss']);\n",
    "#plt.plot(history.history['val_loss']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09572db8-2284-44a9-b67b-05186c61ad3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_test_loss, final_test_mae = best_model.evaluate(train_dataset)\n",
    "print(f\"Final Test Loss: {final_test_loss}, Final Test MAE: {final_test_mae}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4e4e3a-1a93-41f9-aead-a0bd5d8d66e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test_sc)\n",
    "y_pred_original = y_scaler.inverse_transform(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b85edd3-b4d4-4c09-8558-86bb71b42ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_rescaled = y_scaler.inverse_transform(y_test_sc.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08b38cc-0aa7-4c4a-a7ce-c2c2299ff0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(y_pred_original);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a874b0d-a604-49fb-b765-500dafcb202f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "y_pred_rescaled = y_scaler.inverse_transform(y_pred)\n",
    "y_test_rescaled = y_scaler.inverse_transform(y_test)\n",
    "\n",
    "# Calculate the MSE and MAE on the rescaled data\n",
    "mse_rescaled = np.mean((y_pred_rescaled - y_test_rescaled) ** 2)\n",
    "mae_rescaled = np.mean(np.abs(y_pred_rescaled - y_test_rescaled))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c012e9c3-b853-4d23-a5a6-18fab96d6a37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aba64c4-a374-4da9-a8fa-212a93b689db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f702909-a31f-434e-8c40-fca7318172e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5822e7cb-06cf-4a81-8827-a9919eb8faae",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test_scaled)\n",
    "y_pred_original = y_scaler.inverse_transform(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940f511a-54bb-485e-885c-ffc21b16e790",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d7f35e-2b57-403f-a7fa-b744440badec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de26aed6-9b33-4904-84c1-7708609ccea7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d4aac9-1993-450e-9dc4-53f5ef14a1af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baeca632-11e0-4e8b-ab8c-cf7faa9b8f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_train['lat_grid'] = alt_train['lat'].round(3)  # or use a more sophisticated spatial binning technique\n",
    "alt_train['lon_grid'] = alt_train['lon'].round(3)\n",
    "alt_train_agg = alt_train.groupby(['datetime', 'lat_grid', 'lon_grid']).mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2f87e1-0317-4750-ad4d-40ef4cf28b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_temporal_features(df, past_n_months=12):\n",
    "    df_sorted = df.sort_values(by=['lat_grid', 'lon_grid', 'datetime'])\n",
    "    df['alt_shifted'] = df.groupby(['lat_grid', 'lon_grid'])['alt'].shift(past_n_months)\n",
    "    return df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693d9e27-8aa1-41fd-8d3e-4c686f023ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_train_features = aggregate_temporal_features(alt_train_agg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7341b4-ef19-49bb-b2e2-7783d69e5edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "alt_train_features['alt_scaled'] = scaler.fit_transform(alt_train_features['alt'].values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211dfa9b-117a-4f25-8cc5-9425e3fdc151",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "alt_train_features['alt_scaled'] = scaler.fit_transform(alt_train_features['alt'].values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d96fe8-c9cf-4c5c-a4e5-d3ec058dae66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c80ae5f3-4d9f-4323-89bf-cd429d3c4908",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import mixed_precision\n",
    "mixed_precision.set_global_policy('mixed_float16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b46fc12-ce52-438c-8273-38ecf784591b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "scaler_alt = StandardScaler()\n",
    "train_alt_scaled = scaler_alt.fit_transform(alt_train[['alt']])\n",
    "val_alt_scaled = scaler_alt.transform(alt_valid[['alt']])\n",
    "test_alt_scaled = scaler_alt.transform(alt_test[['alt']])\n",
    "alt_train['alt']=train_alt_scaled\n",
    "alt_valid['alt']=val_alt_scaled\n",
    "alt_test['alt']=test_alt_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c397ad-c950-4ad0-bbbe-bf0b6b931cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#combined_df = pd.concat([alt_train, alt_valid, alt_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2607b5-bfc7-437a-b8c6-1a29d5ada173",
   "metadata": {},
   "outputs": [],
   "source": [
    "#combined_df=combined_df.groupby(['datetime','lat','lon'])['alt'].min().reset_index().set_index('datetime').resample('MS').min().dropna().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa67af4-3a83-47d5-be82-e58f02ff4116",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147666c1-6dae-48ed-9a22-bbd1596eb12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_train_lat_grid = np.sort(alt_train['lat'].unique())\n",
    "alt_train_lon_grid = np.sort(alt_train['lon'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61a23a9-476e-46c6-b4ac-4c5eb2c32c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_valid_lat_grid = np.sort(alt_valid['lat'].unique())\n",
    "alt_valid_lon_grid = np.sort(alt_valid['lon'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bff7eea-6801-4acd-93c2-ffb0cd771792",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_test_lat_grid = np.sort(alt_test['lat'].unique())\n",
    "alt_test_lon_grid = np.sort(alt_test['lon'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b08e51-995b-4a2c-b97a-c08ba3b37282",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pivot_dataframe(df, lat_grid, lon_grid):\n",
    "    df_pivot = df.pivot_table(index='datetime', columns=['lat', 'lon'], values='alt')\n",
    "\n",
    "    # Reindex to ensure the grid is consistent\n",
    "    df_pivot = df_pivot.reindex(index=df['datetime'].unique(), columns=pd.MultiIndex.from_product([lat_grid, lon_grid]))\n",
    "\n",
    "    # Fill missing values\n",
    "    df_pivot = df_pivot.fillna(0)  # or use another method like interpolation\n",
    "\n",
    "    # Convert to numpy array\n",
    "    data_array = df_pivot.values.reshape(len(df_pivot), len(lat_grid), len(lon_grid))\n",
    "\n",
    "    # Add the depth dimension (1 channel for alt)\n",
    "    data_array = np.expand_dims(data_array, axis=-1)\n",
    "\n",
    "    return data_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97580f0e-2dc6-4d8b-a3df-e1506975346b",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_train_array = pivot_dataframe(alt_train, alt_train_lat_grid, alt_train_lon_grid)\n",
    "alt_valid_array = pivot_dataframe(alt_valid, lat_grid, lon_grid)\n",
    "alt_test_array = pivot_dataframe(alt_test, lat_grid, lon_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d572d7-fa84-46e8-8b33-579fa8ca8bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(data_array, time_steps):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data_array) - time_steps):\n",
    "        X_seq = data_array[i:i+time_steps]\n",
    "        y_seq = data_array[i+time_steps]\n",
    "\n",
    "        X.append(X_seq)\n",
    "        y.append(y_seq)\n",
    "\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed893c2-1b44-4e5f-a95c-1bb6f700bf64",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_steps = 12\n",
    "X_train, y_train = create_sequences(alt_train_array, time_steps)\n",
    "X_valid, y_valid = create_sequences(alt_valid_array, time_steps)\n",
    "X_test, y_test = create_sequences(alt_test_array, time_steps)\n",
    "\n",
    "# Print shapes for verification\n",
    "# print(\"X_train shape:\", X_train.shape)\n",
    "# print(\"y_train shape:\", y_train.shape)\n",
    "# print(\"X_valid shape:\", X_valid.shape)\n",
    "# print(\"y_valid shape:\", y_valid.shape)\n",
    "# print(\"X_test shape:\", X_test.shape)\n",
    "# print(\"y_test shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66cc9be-f94f-435c-9a8e-e4848489f658",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, ConvLSTM2D, BatchNormalization, Lambda\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from keras_tuner import BayesianOptimization\n",
    "\n",
    "# def residual_block(x, filters, kernel_size, hp):\n",
    "#     \"\"\"\n",
    "#     A Residual Block with ConvLSTM2D layers.\n",
    "#     \"\"\"\n",
    "#     shortcut = x\n",
    "    \n",
    "#     # First ConvLSTM2D layer\n",
    "#     x = ConvLSTM2D(\n",
    "#         filters=filters,\n",
    "#         kernel_size=kernel_size,\n",
    "#         padding='same',\n",
    "#         return_sequences=True,\n",
    "#         activation=hp.Choice('activation_function_1', values=['relu', 'tanh', 'sigmoid', 'leaky_relu', 'swish', 'linear'])\n",
    "#     )(x)\n",
    "    \n",
    "#     x = BatchNormalization()(x)\n",
    "    \n",
    "#     # Second ConvLSTM2D layer\n",
    "#     x = ConvLSTM2D(\n",
    "#         filters=filters,\n",
    "#         kernel_size=kernel_size,\n",
    "#         padding='same',\n",
    "#         return_sequences=True,\n",
    "#         activation=hp.Choice('activation_function_2', values=['relu', 'tanh', 'sigmoid', 'leaky_relu', 'swish', 'linear'])\n",
    "#     )(x)\n",
    "    \n",
    "#     x = BatchNormalization()(x)\n",
    "    \n",
    "#     # Convolutional layer in the shortcut path to match the filter size of x\n",
    "#     if shortcut.shape[-1] != filters:\n",
    "#         shortcut = ConvLSTM2D(\n",
    "#             filters=filters,\n",
    "#             kernel_size=(1, 1),\n",
    "#             padding='same',\n",
    "#             return_sequences=True,\n",
    "#             activation='linear'\n",
    "#         )(shortcut)\n",
    "    \n",
    "#     # Add the shortcut to the main path\n",
    "#     x = Add()([x, shortcut])\n",
    "#     x = Activation('relu')(x)\n",
    "    \n",
    "#     return x\n",
    "\n",
    "# def build_model(hp):\n",
    "#     input_shape = (time_steps, len(lat_grid), len(lon_grid), 1)\n",
    "#     inputs = Input(shape=input_shape)\n",
    "    \n",
    "#     # Initial ConvLSTM2D layer\n",
    "#     x = ConvLSTM2D(\n",
    "#         filters=hp.Int('filters_1', min_value=64, max_value=128, step=32),\n",
    "#         kernel_size=hp.Choice('kernel_size_1', values=[3, 5]),\n",
    "#         padding='same',\n",
    "#         return_sequences=True,\n",
    "#         activation=hp.Choice('activation_function_3', values=['relu','tanh','sigmoid','leaky_relu','swish','linear']),\n",
    "#     )(inputs)\n",
    "    \n",
    "#     x = BatchNormalization()(x)    \n",
    "#     # Residual Block 1\n",
    "#     x = residual_block(x, filters=hp.Int('filters_2', min_value=64, max_value=128, step=32), kernel_size=hp.Choice('kernel_size_2', values=[3, 5]), hp=hp)\n",
    "    \n",
    "#     # Residual Block 2\n",
    "#     x = residual_block(x, filters=hp.Int('filters_3', min_value=64, max_value=128, step=32), kernel_size=hp.Choice('kernel_size_3', values=[3, 5]), hp=hp)\n",
    "    \n",
    "#     # Reduce the time dimension\n",
    "#     x = Lambda(lambda x: x[:, -1, :, :, :])(x)\n",
    "    \n",
    "#     # 3D Convolutional Layer to combine features\n",
    "#     x = Conv2D(\n",
    "#         filters=hp.Int('filters_4', min_value=32, max_value=128, step=32),\n",
    "#         kernel_size=(3, 3),\n",
    "#         padding='same',\n",
    "#         activation=hp.Choice('activation_function_4', values=['relu','tanh','sigmoid','leaky_relu','swish','linear']),\n",
    "#     )(x)\n",
    "    \n",
    "#     x = BatchNormalization()(x)\n",
    "    \n",
    "#     # Final Conv2D layer for output\n",
    "#     outputs = Conv2D(\n",
    "#         filters=1,\n",
    "#         kernel_size=(3, 3),\n",
    "#         padding='same',\n",
    "#         activation=hp.Choice('activation_function_5', values=['relu','tanh','sigmoid','leaky_relu','swish','linear']),\n",
    "#     )(x)\n",
    "    \n",
    "#     # Compile the model\n",
    "#     model = Model(inputs, outputs)\n",
    "#     model.compile(\n",
    "#         optimizer=tf.keras.optimizers.Adam(\n",
    "#             hp.Choice('learning_rate', values=[1e-4, 1e-5])\n",
    "#         ),\n",
    "#         loss='mse',\n",
    "#         metrics=['mae']\n",
    "#     )\n",
    "    \n",
    "#     return model\n",
    "    \n",
    "# def build_model(hp):\n",
    "#     model = Sequential()\n",
    "    \n",
    "#     # Add more layers or units\n",
    "#     model.add(ConvLSTM2D(\n",
    "#         filters=hp.Int('filters_1', min_value=32, max_value=128, step=32),\n",
    "#         kernel_size=hp.Choice('kernel_size_1', values=[3, 5]),\n",
    "#         input_shape=(time_steps, len(lat_grid), len(lon_grid), 1),\n",
    "#         padding='same', \n",
    "#         return_sequences=True,\n",
    "#         activation=hp.Choice('activation_function_1', values=['relu','tanh','sigmoid','leaky_relu','swish','linear']),\n",
    "#     ))\n",
    "#     model.add(BatchNormalization())\n",
    "    \n",
    "#     # More ConvLSTM layers\n",
    "#     model.add(ConvLSTM2D(\n",
    "#         filters=hp.Int('filters_2', min_value=32, max_value=128, step=32), \n",
    "#         kernel_size=hp.Choice('kernel_size_2', values=[3, 5]),\n",
    "#         padding='same', \n",
    "#         return_sequences=False,\n",
    "#         activation=hp.Choice('activation_function_1', values=['relu','tanh','sigmoid','leaky_relu','swish','linear']),\n",
    "#     ))\n",
    "#     model.add(BatchNormalization())\n",
    "\n",
    "#     # Final output layer\n",
    "#     model.add(Conv2D(\n",
    "#         filters=1, \n",
    "#         kernel_size=(3, 3), \n",
    "#         activation=hp.Choice('activation_function_1', values=['relu','tanh','sigmoid','leaky_relu','swish','linear']),\n",
    "#         padding='same',\n",
    "#     ))\n",
    "\n",
    "#     model.compile(\n",
    "#         optimizer=tf.keras.optimizers.Adam(\n",
    "#             hp.Choice('learning_rate', values=[1e-4, 1e-5])\n",
    "#         ),\n",
    "#         loss='mse',\n",
    "#         metrics=['mae']\n",
    "#     )\n",
    "    \n",
    "#     return model\n",
    "\n",
    "\n",
    "def build_model(hp):\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(ConvLSTM2D(\n",
    "        filters=hp.Int('filters_1', min_value=16, max_value=64, step=16), \n",
    "        kernel_size=hp.Choice('kernel_size_1', values=[3, 5]),\n",
    "        input_shape=(time_steps, len(lat_grid), len(lon_grid), 1),\n",
    "        padding='same', \n",
    "        return_sequences=True,\n",
    "        activation=hp.Choice('activation_function_1', values=['relu','tanh','softmax','sigmoid','leaky_relu','swish'])\n",
    "    ))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(ConvLSTM2D(\n",
    "        filters=hp.Int('filters_2', min_value=16, max_value=64, step=16), \n",
    "        kernel_size=hp.Choice('kernel_size_2', values=[3, 5]),\n",
    "        padding='same', \n",
    "        return_sequences=True,\n",
    "        activation=hp.Choice('activation_function_2', values=['relu','tanh','softmax','sigmoid','leaky_relu','swish'])\n",
    "    ))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    # Extract the last time step\n",
    "    model.add(Lambda(lambda x: x[:, -1, :, :, :]))  # Shape becomes [batch_size, height, width, channels]\n",
    "\n",
    "    # Conv2D Layer for output (single frame prediction)\n",
    "    model.add(Conv2D(\n",
    "        filters=1,  # Ensure only one output channel to match y_train's shape\n",
    "        kernel_size=(3, 3),  # Use a 2D kernel size\n",
    "        activation=hp.Choice('activation_function_3', values=['relu','tanh','softmax','sigmoid','leaky_relu','swish']),\n",
    "        padding='same',\n",
    "        data_format='channels_last'\n",
    "    ))\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(\n",
    "            hp.Choice('learning_rate', values=[1e-3, 1e-4, 1e-5])\n",
    "        ),\n",
    "        loss='mse',\n",
    "        metrics=['mae']\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afda5179-0033-435d-ab93-9dfba617d742",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32  # Adjust based on memory and GPU capability\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "train_dataset = train_dataset.cache().shuffle(1000).batch(batch_size).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "valid_dataset = tf.data.Dataset.from_tensor_slices((X_valid, y_valid))\n",
    "valid_dataset = valid_dataset.batch(batch_size).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
    "test_dataset = test_dataset.batch(batch_size).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8edfb5a9-e9ed-4959-a1e7-f8d4cca9e153",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define callbacks\n",
    "early_stopping_cb = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "lr_scheduler_cb = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2)\n",
    "checkpoint_cb = ModelCheckpoint('model_checkpoint.keras', save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e68c17f-c35e-4cc5-945a-c34ddb6aec77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_tuner import BayesianOptimization\n",
    "\n",
    "# Define the tuner\n",
    "tuner = BayesianOptimization(\n",
    "    build_model,\n",
    "    objective='val_loss',\n",
    "    max_trials=10,\n",
    "    executions_per_trial=1,\n",
    "    directory='/Volumes/JPL/alt_train3',\n",
    "    project_name='alt_train_conv3dlstm_tuning',\n",
    "    overwrite=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e903cec7-eb16-49c8-a6de-aa45e6161636",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.search(train_dataset, epochs=10, validation_data=valid_dataset, callbacks=[early_stopping_cb, lr_scheduler_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1d9d5e-81de-4487-8b84-32218dfbb972",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = tuner.get_best_models(num_models=1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9e3b22-7a9b-4a47-8751-48ef0b5fc003",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_mae = best_model.evaluate(test_dataset)\n",
    "print(f\"Test Loss: {test_loss}, Test MAE: {test_mae}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238de82e-af24-4463-a70b-678c51b543a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58cab46d-d5c1-4150-b034-4dbca8b33604",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484d2d8e-7186-48a5-8c72-5d7bab2bcea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "history=best_model.fit(train_dataset, epochs=10, validation_data=valid_dataset, callbacks=[early_stopping_cb, lr_scheduler_cb, checkpoint_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde61d4c-0196-4aa5-8705-c86a1e635a28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bbd3d58-5b35-499b-9262-9e0440fd8337",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435fa001-d102-4077-ae03-6dc128210215",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_model.fit(np.concatenate([train_alt_scaled, val_alt_scaled]), np.concatenate([y_train_alt, y_val_alt]), \n",
    "#                epochs=100, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583c6631-bf44-4655-b8b7-371d83ed50b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_test_loss, final_test_mae = best_model.evaluate(train_dataset)\n",
    "print(f\"Final Test Loss: {final_test_loss}, Final Test MAE: {final_test_mae}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5579a748-716e-4cd4-8fa5-2f70a2594588",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Extract targets (y values) from train_dataset\n",
    "y_train_scaled = []\n",
    "\n",
    "for _, y in train_dataset:\n",
    "    y_train_scaled.append(y.numpy())\n",
    "\n",
    "y_train_scaled = np.concatenate(y_train_scaled, axis=0)\n",
    "print(\"Scaled y_train shape:\", y_train_scaled.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef0291e-c96b-4194-917f-6cb526c3dec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume 'scaler' is your fitted StandardScaler used during training\n",
    "y_train_original = scaler_alt.inverse_transform(y_train_scaled.reshape(-1, 1)).reshape(y_train_scaled.shape)\n",
    "print(\"Original y_train shape:\", y_train_original.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c911de-71d1-47c8-ad08-7597742783d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Reconstruct the dataset with the original y values\n",
    "train_dataset_inverse_transformed = tf.data.Dataset.from_tensor_slices((X_train, y_train_original))\n",
    "train_dataset_inverse_transformed = train_dataset_inverse_transformed.batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e29bc7-9dc0-4fa4-9495-3daa1029da6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_test_loss, final_test_mae = best_model.evaluate(train_dataset_inverse_transformed)\n",
    "print(f\"Final Test Loss: {final_test_loss}, Final Test MAE: {final_test_mae}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2f1ebc-4825-4e39-a11f-023ab22ccec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_test_loss, final_test_mae = best_model.evaluate(valid_dataset)\n",
    "print(f\"Final Test Loss: {final_test_loss}, Final Test MAE: {final_test_mae}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765ee29e-22c8-474b-b3fe-6ab091dd0dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Extract targets (y values) from train_dataset\n",
    "y_valid_scaled = []\n",
    "\n",
    "for _, y in valid_dataset:\n",
    "    y_valid_scaled.append(y.numpy())\n",
    "\n",
    "y_valid_scaled = np.concatenate(y_valid_scaled, axis=0)\n",
    "print(\"Scaled y_valid shape:\", y_valid_scaled.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4627f241-1256-4c7b-be8a-ab4bdbf3e2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume 'scaler' is your fitted StandardScaler used during training\n",
    "y_valid_original = scaler_alt.inverse_transform(y_valid_scaled.reshape(-1, 1)).reshape(y_valid_scaled.shape)\n",
    "print(\"Original y_valid shape:\", y_valid_original.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9ed093-a5f8-4e3b-b15b-a563ef0d1a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Reconstruct the dataset with the original y values\n",
    "valid_dataset_inverse_transformed = tf.data.Dataset.from_tensor_slices((X_valid, y_valid_original))\n",
    "valid_dataset_inverse_transformed = valid_dataset_inverse_transformed.batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4e8032-1c87-4d34-a388-28b94f6ec5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_test_loss, final_test_mae = best_model.evaluate(valid_dataset_inverse_transformed)\n",
    "print(f\"Final Test Loss: {final_test_loss}, Final Test MAE: {final_test_mae}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35322b6-2149-4704-8b21-6d41aea1d3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_test_loss, final_test_mae = best_model.evaluate(test_dataset)\n",
    "print(f\"Final Test Loss: {final_test_loss}, Final Test MAE: {final_test_mae}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f686f676-9398-4434-baa7-2203eacb9b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Extract targets (y values) from train_dataset\n",
    "y_test_scaled = []\n",
    "\n",
    "for _, y in test_dataset:\n",
    "    y_test_scaled.append(y.numpy())\n",
    "\n",
    "y_test_scaled = np.concatenate(y_test_scaled, axis=0)\n",
    "print(\"Scaled y_test shape:\", y_test_scaled.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384e573f-1798-4106-b4ae-e5fe19505243",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume 'scaler' is your fitted StandardScaler used during training\n",
    "y_test_original = scaler_alt.inverse_transform(y_test_scaled.reshape(-1, 1)).reshape(y_test_scaled.shape)\n",
    "print(\"Original y_test shape:\", y_test_original.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc8f918-3ac2-4d9b-a604-3621531d7218",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Reconstruct the dataset with the original y values\n",
    "test_dataset_inverse_transformed = tf.data.Dataset.from_tensor_slices((X_test, y_test_original))\n",
    "test_dataset_inverse_transformed = test_dataset_inverse_transformed.batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f11cad-2210-46b8-b199-6961ce50e974",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_test_loss, final_test_mae = best_model.evaluate(test_dataset_inverse_transformed)\n",
    "print(f\"Final Test Loss: {final_test_loss}, Final Test MAE: {final_test_mae}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b1bdcb-7f80-4901-8660-d23fe7748b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = best_model.predict(test_dataset)\n",
    "\n",
    "# If using a scaler, inverse transform the predictions and actual values\n",
    "predictions_original = scaler_alt.inverse_transform(predictions.reshape(-1, 1)).reshape(predictions.shape)\n",
    "actual_values_original = scaler_alt.inverse_transform(y_test.reshape(-1, 1)).reshape(y_test.shape)\n",
    "\n",
    "# Compare first few predictions with actual values\n",
    "for i in range(5):\n",
    "    print(f\"Prediction {i+1}: {predictions_original[i, :, :, 0]}\")\n",
    "    print(f\"Actual Value {i+1}: {actual_values_original[i, :, :, 0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c71262-6274-4ff6-bb6e-f6b243a124ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title('Predicted (Original Scale)')\n",
    "plt.imshow(predictions_original[0, :, :, 0], cmap='viridis')\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title('Actual (Original Scale)')\n",
    "plt.imshow(actual_values_original[0, :, :, 0], cmap='viridis')\n",
    "plt.colorbar()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a9bdb6-1209-4ade-9ebb-3e64897b6bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calculate residuals\n",
    "residuals = y_test_original.flatten() - predictions_original.flatten()\n",
    "\n",
    "# Plot residuals\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(residuals, bins=50, color='blue', alpha=0.7)\n",
    "plt.title('Residuals Distribution')\n",
    "plt.xlabel('Residuals')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "# Scatter plot of actual vs predicted\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test_original.flatten(), predictions_original.flatten(), alpha=0.3)\n",
    "plt.plot([min(y_test_original.flatten()), max(y_test_original.flatten())],\n",
    "         [min(y_test_original.flatten()), max(y_test_original.flatten())], color='red')\n",
    "plt.title('Actual vs Predicted')\n",
    "plt.xlabel('Actual Values')\n",
    "plt.ylabel('Predicted Values')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546b52a1-cdd3-44bb-8373-94978181a0a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993b7c30-1d36-41d1-9f4a-1fcf5a54a5ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a363044b-eaa8-4049-9ce6-203fd6991963",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dadd04f9-29fe-44ee-aba2-01db80e5893d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d378cbdb-132c-4359-8898-8e7774a895ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ebc4c1-0290-4721-945c-ab8abee46f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_values_original.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71cb8703-1ad8-4f8a-b3f2-111e8ec4f1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc0f30a-dadc-4262-9fea-ccc9f82ddd9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba44e08a-16cc-4b97-a741-8ff8ee737bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13621cc8-0af1-49e8-954e-c6812fea5ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(best_model, to_file='/Volumes/JPL/alt_model.png', show_shapes=True, show_layer_names=True, dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fbe5c9b-5f4c-4288-928b-9c71a9d0b8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.results_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e11af2-f1ef-4e71-a6fc-835231be7e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.history.model.history.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db019fc1-e43b-47da-a9a2-33f55e5c05de",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_scaled = best_model.predict(test_dataset)\n",
    "\n",
    "# 5. Inverse Transform the Predictions Back to the Original Scale\n",
    "# Reshape predictions to be 2D for inverse transform, then reshape back to original shape\n",
    "predictions_reshaped = predictions_scaled.reshape(-1, 1)\n",
    "predictions_original = scaler_alt.inverse_transform(predictions_reshaped).reshape(predictions_scaled.shape)\n",
    "\n",
    "# Example: Compare the first prediction with the actual value\n",
    "print(\"First prediction (original scale):\", predictions_original[0])\n",
    "print(\"First actual value (original scale):\", scaler_alt.inverse_transform(y_test.reshape(-1, 1)).reshape(y_test.shape)[0])\n",
    "\n",
    "# Visualizing predictions and actual values\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title('Predicted (Original Scale)')\n",
    "plt.imshow(predictions_original[0, :, :, 0], cmap='viridis')\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title('Actual (Original Scale)')\n",
    "plt.imshow(scaler_alt.inverse_transform(y_test.reshape(-1, 1)).reshape(y_test.shape)[0, :, :, 0], cmap='viridis')\n",
    "plt.colorbar()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b594e189-ff21-4a14-8685-5741adac79ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_scaled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5febd8e-e8bd-4ab3-8552-f690b48dfa6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_reshaped.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14087c9a-8cfd-454e-8bb4-db304a02772d",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_original.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f199323-a91f-4a74-8075-93d6260677cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import shap\n",
    "\n",
    "# # Assuming ensemble_model is already trained and X_train_scaled is your standardized data\n",
    "# explainer = shap.KernelExplainer(best_model.predict, X_train)\n",
    "# shap_values = explainer.shap_values(X_train.reshape(3068,363*367))\n",
    "\n",
    "# # Global importance plot\n",
    "# shap.summary_plot(shap_values, X_train, feature_names=['alt'])\n",
    "\n",
    "# # Dependence plot for a specific feature (e.g., alt)\n",
    "# shap.dependence_plot('alt', shap_values, X_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aad9a21-5bc2-4a23-a7fd-88201430726b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "from shap import kmeans\n",
    "import numpy as np\n",
    "\n",
    "# Using shap.sample to reduce the background dataset\n",
    "#background_data = shap.sample(X_train.reshape(3068,363*367))  # Reduce to 100 samples for background\n",
    "background_data = shap.sample(X_train.reshape(3068,363*367), 50) \n",
    "\n",
    "# Or using shap.kmeans for clustering\n",
    "#background_data = shap.kmeans(X_train.reshape(3068,363*367), k=100)  # K-means clustering to find 100 representative samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031a6c4f-77e4-4cc4-bb11-2d437832edec",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_flat = X_test.reshape(X_test.shape[0], -1)\n",
    "background_data_flat = background_data.reshape(background_data.shape[0], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1897d73f-7834-4efa-a98b-a980dadb9c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "background_data=background_data.reshape(50, 1, 363, 367, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718034a0-5b52-41cc-b29a-7b36b3a177e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.KernelExplainer(best_model.predict, background_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1eec50-1dd6-4768-ae9e-69b635130a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values = explainer.shap_values(X_test_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6796876-e777-4701-8de0-28e57231bb06",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values, X_test_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f744a9b2-1315-4433-b4b9-5e6e65703fac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c2f24c-fbc4-478e-803f-e2b14e4acf0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89efc55f-cbad-4f0f-a7c9-ccdcb628a1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values = explainer.shap_values(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eab33e1-c1e2-48e6-8ca9-3ad9af6dc364",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global interpretation\n",
    "shap.summary_plot(shap_values, X_test.reshape(687,363*367), feature_names=['alt'])\n",
    "\n",
    "# Local interpretation for the first instance in the test set\n",
    "shap.force_plot(explainer.expected_value, shap_values[0], X_test[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ce3492-4b94-4510-9648-801844da0f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values = explainer.shap_values(X_test)\n",
    "\n",
    "# Visualize the SHAP values with a summary plot\n",
    "shap.summary_plot(shap_values, X_test, feature_names=['alt'])\n",
    "\n",
    "# For local explanations, you can visualize specific instances\n",
    "shap.force_plot(explainer.expected_value, shap_values[0], X_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb3e8d5-3da6-41dc-b8c0-719aedccc902",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d7f43a-c73f-411d-aeba-a9c94eccb9fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ff2b22-4c60-4e6c-9839-daef2fe65cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.DeepExplainer(best_model, background_data)\n",
    "\n",
    "shap_values = explainer.shap_values(X_test.reshape(687,363*367))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7430e649-baab-42c7-ac95-d9f9fb35b123",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.reshape(3068,363*367)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33fc4b8e-1ab1-45c3-99f4-526a540ee654",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb3f101-b5cf-4db4-8f47-724a02d1fca9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a43d7c1-260c-4e27-89f7-5c6530ad1798",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c33ad8-89ea-4578-91e8-e73fa7914e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "plot_model(best_model, to_file='alt_model.png', show_shapes=True, show_layer_names=True, dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe73646-d4c7-4505-bcbb-c5c78a607973",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "logdir = \"logs/ensemble_model/\"\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir)\n",
    "\n",
    "ensemble_model.fit(X_train, y_train, epochs=10, callbacks=[tensorboard_callback])\n",
    "\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs/ensemble_model/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c70890-8467-4878-906b-7c1dbcddbb6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "# Assuming ensemble_model is already trained and X_train_scaled is your standardized data\n",
    "explainer = shap.KernelExplainer(ensemble_model.predict, X_train_scaled)\n",
    "shap_values = explainer.shap_values(X_train_scaled)\n",
    "\n",
    "# Global importance plot\n",
    "shap.summary_plot(shap_values, X_train_scaled, feature_names=['alt', 'fch4', 'fc'])\n",
    "\n",
    "# Dependence plot for a specific feature (e.g., alt)\n",
    "shap.dependence_plot('alt', shap_values, X_train_scaled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65aa1c93-83ac-4e6c-80ba-1491012424df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explain a single prediction\n",
    "instance_index = 0\n",
    "shap.force_plot(explainer.expected_value, shap_values[instance_index], X_train_scaled[instance_index])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a35704-a0a4-43fe-a3f3-812ae4e8a96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lime\n",
    "import lime.lime_tabular\n",
    "\n",
    "# Create a LIME explainer\n",
    "explainer = lime.lime_tabular.LimeTabularExplainer(X_train_scaled, feature_names=['alt', 'fch4', 'fc'], class_names=['target'], verbose=True, mode='regression')\n",
    "\n",
    "# Explain a single prediction\n",
    "instance_index = 0\n",
    "exp = explainer.explain_instance(X_train_scaled[instance_index], ensemble_model.predict, num_features=5)\n",
    "\n",
    "# Display the explanation\n",
    "exp.show_in_notebook(show_table=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8bb51a-23ef-4aad-a314-96b2bc7ca827",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP force plot for a single instance\n",
    "shap.force_plot(explainer.expected_value, shap_values[instance_index], X_train_scaled[instance_index])\n",
    "\n",
    "# LIME explanation for the same instance\n",
    "exp = explainer.explain_instance(X_train_scaled[instance_index], ensemble_model.predict, num_features=5)\n",
    "exp.show_in_notebook(show_table=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81dcd0c-aad1-41b3-842c-19ac416de735",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the meta-model is an ensemble of sub-models, you can calculate SHAP values for each sub-model's output\n",
    "meta_explainer = shap.KernelExplainer(ensemble_model.predict, [X_train_scaled_convlstm, X_train_scaled_transformer, X_train_scaled_vae])\n",
    "meta_shap_values = meta_explainer.shap_values([X_test_scaled_convlstm, X_test_scaled_transformer, X_test_scaled_vae])\n",
    "\n",
    "# Visualize the importance of each component model's contribution\n",
    "shap.summary_plot(meta_shap_values, [X_test_scaled_convlstm, X_test_scaled_transformer, X_test_scaled_vae], feature_names=['ConvLSTM', 'Transformer', 'VAE'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16cbd563-9969-459f-b3fb-46315ca76a4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "376ddfef-f886-49c8-9814-675590ed0d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(ensemble_model, to_file='ensemble_model.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f205c0-d200-46e5-b13b-c3d541971f15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097c68ba-be86-4848-9894-a27ef0f59cf0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f8f436-0f4a-438a-8170-dc4f653540b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_train.alt.plot();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3fb6079-fab7-4926-8c37-77f87b040108",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "scaler_alt = MinMaxScaler()\n",
    "train_alt_scaled = scaler_alt.fit_transform(alt_train[['alt']])\n",
    "val_alt_scaled = scaler_alt.transform(alt_valid[['alt']])\n",
    "test_alt_scaled = scaler_alt.transform(alt_test[['alt']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca745df-f2db-482f-8b5f-bb083d458c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_train['alt']=train_alt_scaled\n",
    "alt_valid['alt']=val_alt_scaled\n",
    "alt_test['alt']=test_alt_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e88a6c1-30b0-4d4b-929c-27b84756fd17",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_train.alt.plot();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69f1fdc-ce8c-4ba3-a8f7-88c9f1be810f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2090157-bf16-4e36-be32-2dc5d9fb714b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lat_grid = np.sort(alt_train['lat'].unique())\n",
    "lon_grid = np.sort(alt_train['lon'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd2473b-46ad-4e83-a674-d2d0cecbd291",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_train_pivot = alt_train.pivot_table(index='datetime', columns=['lat', 'lon'], values='alt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423ed46c-624e-4666-823c-2173bff2fde4",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_train_pivot = alt_train_pivot.reindex(index=alt_train['datetime'].unique(), columns=pd.MultiIndex.from_product([lat_grid, lon_grid]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bddc0fd-3673-49c2-8dfd-b03b9f4c9a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_train_pivot = alt_train_pivot.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec487459-c1cd-4fc4-b9dd-f05baef3f00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_train_arr = alt_train_pivot.values.reshape(len(alt_train_pivot), len(lat_grid), len(lon_grid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5e06e4-eccb-4563-af97-20a2aaa65fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_train_arr = np.expand_dims(alt_train_arr, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5836041d-fdd0-4650-9cc5-bb1d2db30a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_train_arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2391169-0cd7-4c16-84e7-2edb38e5f16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32 \n",
    "time_steps = alt_train_arr.shape[0] // batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2eabcc-b2e6-4890-9395-9a20672fe3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_train_arr = alt_train_arr[:batch_size*time_steps].reshape(batch_size, time_steps, len(lat_grid), len(lon_grid), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "459be910-eb77-4295-ada1-a83ba2c2ea5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_train_arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a84be96-426c-4547-bb26-a21388cb34e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_steps = 10\n",
    "batch_size, total_time_steps, height, width, depth = alt_train_arr.shape\n",
    "\n",
    "X_train = []\n",
    "y_train = []\n",
    "\n",
    "for i in range(total_time_steps - time_steps):\n",
    "    X_seq = alt_train_arr[:, i:i+time_steps, :, :, :]\n",
    "    y_seq = alt_train_arr[:, i+time_steps, :, :, :]\n",
    "    X_train.append(X_seq)\n",
    "    y_train.append(y_seq)\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "X_train = X_train.reshape(-1, time_steps, height, width, depth)\n",
    "\n",
    "y_train = y_train.reshape(-1, height, width, depth)\n",
    "\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1375eb74-fff0-4ae7-8089-d404f29de514",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv3D, ConvLSTM2D, BatchNormalization, Conv3DTranspose, TimeDistributed\n",
    "from keras_tuner import BayesianOptimization\n",
    "\n",
    "def build_model(hp):\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(ConvLSTM2D(\n",
    "        filters=hp.Int('filters_1', min_value=16, max_value=64, step=16), \n",
    "        kernel_size=hp.Choice('kernel_size_1', values=[3, 5]),\n",
    "        input_shape=(None, 326, 328, 1),\n",
    "        padding='same', \n",
    "        return_sequences=True, \n",
    "        activation='relu'\n",
    "    ))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    # ConvLSTM2D Layer 2\n",
    "    model.add(ConvLSTM2D(\n",
    "        filters=hp.Int('filters_2', min_value=16, max_value=64, step=16), \n",
    "        kernel_size=hp.Choice('kernel_size_2', values=[3, 5]),\n",
    "        padding='same', \n",
    "        return_sequences=True,\n",
    "        activation='relu'\n",
    "    ))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(Lambda(lambda x: x[:, -1, :, :, :]))\n",
    "\n",
    "    # Conv2D Layer for output\n",
    "    model.add(Conv2D(\n",
    "        filters=1,\n",
    "        #filters=hp.Int('filters_3', min_value=1, max_value=16, step=5), \n",
    "        kernel_size=(3, 3),\n",
    "        activation='sigmoid',\n",
    "        padding='same',\n",
    "        data_format='channels_last'\n",
    "    ))\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(\n",
    "            hp.Choice('learning_rate', values=[1e-3, 1e-4, 1e-5])\n",
    "        ),\n",
    "        loss='mse',\n",
    "        metrics=['mae']\n",
    "    )\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f6740e-f26c-4741-bafc-a1233e10f543",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_tuner import BayesianOptimization\n",
    "\n",
    "# Define the tuner\n",
    "tuner = BayesianOptimization(\n",
    "    build_model,\n",
    "    objective='val_loss',\n",
    "    max_trials=20,  # Number of hyperparameter combinations to try\n",
    "    executions_per_trial=10,  # Number of times to train the model per trial\n",
    "    directory='/Volumes/JPL/alt_train',\n",
    "    project_name='alt_train_conv3dlstm_tuning'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd6b1fd-de82-4540-abe9-ad04aa9bbfba",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.search_space_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37f6d02-032f-4176-a346-a1757e6fcfbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data into training and validation sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01439c00-fa12-4a2b-b7c9-fa0f6a576a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the hyperparameter search\n",
    "tuner.search(X_train, y_train, epochs=10, validation_data=(X_val, y_val), batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95df38d4-b274-4008-ba73-6cc926061a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = tuner.get_best_models(num_models=1)[0]\n",
    "\n",
    "# Summary of the best model\n",
    "best_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81bfaaeb-4db8-4b6f-82fa-0d961031ed3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e2bea3-8f44-453d-8d06-cc8b7dbdee06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73fd353a-c58b-4c79-a62d-62c98a90b9fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7d4e51-466d-479e-8964-738ace775e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "newalt=alt.groupby(['datetime','lat', 'lon'])['alt'].mean().reset_index().replace(0,np.nan).dropna()\n",
    "newalt['datetime']=pd.to_datetime(newalt['datetime'])\n",
    "newalt=newalt.sort_values(by='datetime',ascending=True)\n",
    "newalt=newalt.reset_index(drop=True)\n",
    "newalt=newalt[['datetime','lat','lon','alt']]\n",
    "newalt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3f909b-ddfe-4f51-8969-5bc52ee8c529",
   "metadata": {},
   "outputs": [],
   "source": [
    "ch4.fch4.plot();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad49552-6772-4dc1-9715-e3f12ca8bc02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_alt, test_alt = train_test_split(alt, test_size=0.2, random_state=42)\n",
    "train_alt, val_alt = train_test_split(train_alt, test_size=0.25, random_state=42)  # 0.25 x 0.8 = 0.2\n",
    "train_alt=train_alt.sort_values(by='datetime',ascending=True)\n",
    "train_alt=train_alt.reset_index(drop=True)\n",
    "val_alt=val_alt.sort_values(by='datetime',ascending=True)\n",
    "val_alt=val_alt.reset_index(drop=True)\n",
    "test_alt=test_alt.sort_values(by='datetime',ascending=True)\n",
    "test_alt=test_alt.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad7829b-fb77-42ea-ad94-8be77898945f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_alt.to_parquet('/Volumes/JPL/alt_train_random.parquet',engine='pyarrow',compression='snappy')\n",
    "val_alt.to_parquet('/Volumes/JPL/alt_valid_random.parquet',engine='pyarrow',compression='snappy')\n",
    "test_alt.to_parquet('/Volumes/JPL/alt_test_random.parquet',engine='pyarrow',compression='snappy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9e4b4d-d2cc-46d6-a4f1-6812382bd430",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_fch4, test_fch4 = train_test_split(ch4, test_size=0.2, random_state=42)\n",
    "train_fch4, val_fch4 = train_test_split(train_fch4, test_size=0.25, random_state=42)  # 0.25 x 0.8 = 0.2\n",
    "train_fch4=train_fch4.sort_values(by='datetime',ascending=True)\n",
    "train_fch4=train_fch4.reset_index(drop=True)\n",
    "val_fch4=val_fch4.sort_values(by='datetime',ascending=True)\n",
    "val_fch4=val_fch4.reset_index(drop=True)\n",
    "test_fch4=test_fch4.sort_values(by='datetime',ascending=True)\n",
    "test_fch4=test_fch4.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f576fbf-9292-420d-b26b-a9e05bf53f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "ch4.fch4.plot();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f21dd98-ff6e-42d5-91a7-5aae073f63ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fch4.to_parquet('/Volumes/JPL/fch4_train_random.parquet',engine='pyarrow',compression='snappy')\n",
    "val_fch4.to_parquet('/Volumes/JPL/fch4_valid_random.parquet',engine='pyarrow',compression='snappy')\n",
    "test_fch4.to_parquet('/Volumes/JPL/fch4_test_random.parquet',engine='pyarrow',compression='snappy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5779f5d8-5f46-4868-8fde-af28530b6875",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f8e965-3c28-4a79-9c36-30fa411bd0cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851c2cbf-672c-47ee-9622-89b8661e6f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Volumes/JPL/alt_train.parquet','rb') as f:\n",
    "    alt_train=pd.read_parquet(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d3e1fe5-8cbd-4c3f-af38-afef73c0c99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Volumes/JPL/alt_valid.parquet','rb') as f:\n",
    "    alt_valid=pd.read_parquet(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc520cb-852c-4df1-873d-f9f2334580d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Volumes/JPL/alt_test.parquet','rb') as f:\n",
    "    alt_test=pd.read_parquet(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d81a702-8f5f-4057-b697-2d32c7e507ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "scaler_alt = StandardScaler()\n",
    "train_alt_scaled = scaler_alt.fit_transform(alt_train[['alt']])\n",
    "val_alt_scaled = scaler_alt.transform(alt_valid[['alt']])\n",
    "test_alt_scaled = scaler_alt.transform(alt_test[['alt']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7486f2e7-04ad-4843-80aa-b327bcc146a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_train['year'] = alt_train['datetime'].dt.year\n",
    "alt_train['month'] = alt_train['datetime'].dt.month\n",
    "alt_train['day'] = alt_train['datetime'].dt.day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2919a69f-7ef6-4a8b-a7c8-8ce816f43fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_valid['year'] = alt_valid['datetime'].dt.year\n",
    "alt_valid['month'] = alt_valid['datetime'].dt.month\n",
    "alt_valid['day'] = alt_valid['datetime'].dt.day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9cac70e-7194-4e50-bfa9-3f58eeff0e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_test['year'] = alt_test['datetime'].dt.year\n",
    "alt_test['month'] = alt_test['datetime'].dt.month\n",
    "alt_test['day'] = alt_test['datetime'].dt.day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eed134e-069a-41f5-9c8f-fb2d7eaeb777",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_train['month_sin'] = np.sin(2 * np.pi * alt_train['month'] / 12)\n",
    "alt_train['month_cos'] = np.cos(2 * np.pi * alt_train['month'] / 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1218e5a1-045c-40a0-a427-98fe90b15361",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_valid['month_sin'] = np.sin(2 * np.pi * alt_valid['month'] / 12)\n",
    "alt_valid['month_cos'] = np.cos(2 * np.pi * alt_valid['month'] / 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a73caf0-5270-4b65-b5ab-a6ac6f83dc14",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_test['month_sin'] = np.sin(2 * np.pi * alt_test['month'] / 12)\n",
    "alt_test['month_cos'] = np.cos(2 * np.pi * alt_test['month'] / 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c225fd1-28f2-4a1d-afa0-f757c14952b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_train['lat_sin'] = np.sin(np.radians(alt_train['lat']))\n",
    "alt_train['lat_cos'] = np.cos(np.radians(alt_train['lat']))\n",
    "alt_train['lon_sin'] = np.sin(np.radians(alt_train['lon']))\n",
    "alt_train['lon_cos'] = np.cos(np.radians(alt_train['lon']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50347c84-f869-42b4-8c1f-892827479aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_valid['lat_sin'] = np.sin(np.radians(alt_valid['lat']))\n",
    "alt_valid['lat_cos'] = np.cos(np.radians(alt_valid['lat']))\n",
    "alt_valid['lon_sin'] = np.sin(np.radians(alt_valid['lon']))\n",
    "alt_valid['lon_cos'] = np.cos(np.radians(alt_valid['lon']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4721445-a695-4dea-b8b5-eb3b94072805",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_test['lat_sin'] = np.sin(np.radians(alt_test['lat']))\n",
    "alt_test['lat_cos'] = np.cos(np.radians(alt_test['lat']))\n",
    "alt_test['lon_sin'] = np.sin(np.radians(alt_test['lon']))\n",
    "alt_test['lon_cos'] = np.cos(np.radians(alt_test['lon']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3c3f3c-d1bc-4c44-a04a-5a555e8e6507",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_train_features = alt_train[['month_sin', 'month_cos', 'lat_sin', 'lat_cos', 'lon_sin', 'lon_cos', 'alt']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd6769c7-44d2-4a37-aa5b-2e337dfced37",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_valid_features = alt_valid[['month_sin', 'month_cos', 'lat_sin', 'lat_cos', 'lon_sin', 'lon_cos', 'alt']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0711c92c-3505-467b-9b2b-b0860e655f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_test_features = alt_test[['month_sin', 'month_cos', 'lat_sin', 'lat_cos', 'lon_sin', 'lon_cos', 'alt']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5feb324-367d-4b63-a1cd-d5a638a51b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "scaler_alt = StandardScaler()\n",
    "train_alt_scaled = scaler_alt.fit_transform(alt_train[['alt']])\n",
    "val_alt_scaled = scaler_alt.transform(alt_valid[['alt']])\n",
    "test_alt_scaled = scaler_alt.transform(alt_test[['alt']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab937ec9-3939-4d2f-b5e0-7ad676df696f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# # df2=df_aggregated[['datetime','fch4']].set_index('datetime')\n",
    "# newaltmax=alt.groupby(['datetime','lat', 'lon'])['alt'].max().reset_index().replace(0,np.nan).dropna()\n",
    "# newaltmin=alt.groupby(['datetime','lat', 'lon'])['alt'].min().reset_index().replace(0,np.nan).dropna()\n",
    "# newaltmean=alt.groupby(['datetime','lat', 'lon'])['alt'].mean().reset_index().replace(0,np.nan).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a3943b-02fe-4343-8b3f-ba98ae05e0ef",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# newaltmax.alt.plot(alpha=0.2);\n",
    "# newaltmean.alt.plot(alpha=0.2);\n",
    "# newaltmin.alt.plot(alpha=0.2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf949bcc-e3f1-462e-9525-5f5375a7dc63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# newalt=alt.groupby(['datetime','lat', 'lon'])['alt'].mean().reset_index().replace(0,np.nan).dropna()\n",
    "# newalt['datetime']=pd.to_datetime(newalt['datetime'])\n",
    "# newalt=newalt.sort_values(by='datetime',ascending=True)\n",
    "# newalt=newalt.reset_index(drop=True)\n",
    "# newalt=newalt[['datetime','lat','lon','alt']]\n",
    "# newalt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1879b262-cec9-419b-ae0d-f4cce4243851",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# newalt=pd.concat([pd.DataFrame(alt.set_index('datetime').resample('M').mean().reset_index()['lat'].interpolate()),\n",
    "#            pd.DataFrame(alt.set_index('datetime').resample('M').mean().reset_index()['lon'].interpolate()),\n",
    "#            pd.DataFrame(alt.set_index('datetime').resample('M').mean().reset_index()['datetime'].interpolate()),\n",
    "#            pd.DataFrame(alt.set_index('datetime').resample('M').mean().reset_index()['alt'].interpolate())],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ccb6802-873f-4884-a9ae-32208a9a7b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming time_steps is 1 for simplicity here; modify according to your sequence length\n",
    "X = alt_train_features.values.reshape(n_time, n_lat * n_lon, alt_train_features.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e39d020-f28d-49d2-9a7f-5422b4903443",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_train_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393b7db6-c35f-4765-a846-8f76348f0b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = StandardScaler()\n",
    "scaled_df = pd.DataFrame(scaler.fit_transform(alt_train_features), columns=alt_train_features.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885c5f07-f737-429c-98f3-2177dd6ac4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9ec940-a36f-4059-8bef-67c6eae5504d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lat_bins = np.linspace(scaled_df['lat_sin'].min(), scaled_df['lat_sin'].max(), 11)\n",
    "lon_bins = np.linspace(scaled_df['lon_sin'].min(), scaled_df['lon_sin'].max(), 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d212b582-3ade-4702-b414-fc6253fb7f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_df['lat_bin'] = pd.cut(scaled_df['lat_sin'], bins=lat_bins, labels=False)\n",
    "scaled_df['lon_bin'] = pd.cut(scaled_df['lon_sin'], bins=lon_bins, labels=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0002f88-24d2-4019-a0b1-b3084f89c0a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9fd1b3-6c43-4909-ab19-a593651a9ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_df['datetime'] = pd.to_datetime(alt_train['datetime'])  # Ensure datetime is datetime type\n",
    "scaled_df = scaled_df.sort_values('datetime')  # Sort by datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c88d5c2-24e3-45d6-804d-6956c69a076f",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92dadb0d-2501-4a7d-8774-97e8107971ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_steps = 226"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb80b82-34db-4d38-86d0-0542d599c712",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_df.shape[0]/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608346d0-edf4-44b8-bff3-d2822545dbdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = scaled_df.shape[0] // (time_steps * 121)  # Adjust 100 based on 10x10 grid\n",
    "X_reshaped = scaled_df[['month_sin', 'month_cos', 'lat_sin', 'lat_cos', 'lon_sin', 'lon_cos', 'alt']].values\n",
    "X_reshaped = X_reshaped.reshape((samples, 226, 11, 11, 7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ab5cf5-6d3a-4ce8-ab44-cf20d5224237",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_reshaped.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5470220-ddf5-4694-b4e8-cb3247baef0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = scaled_df['alt'].values.reshape(33, time_steps, 11, 11, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6aea9c7-4223-42b4-bff1-91f17e35085e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.reshape(902418, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "befba9a5-8b20-40a0-90b3-b039b0a5bb11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3886de29-9b0e-42f4-a65c-6c77db801998",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv3D, Conv3DTranspose, ConvLSTM2D, Flatten, Dense\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(ConvLSTM2D(filters=64, kernel_size=(3, 3), padding='same', return_sequences=True, \n",
    "                     input_shape=(time_steps, 11, 11, 7)))\n",
    "model.add(ConvLSTM2D(filters=64, kernel_size=(3, 3), padding='same', return_sequences=False))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(1, activation='linear'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58325814-684c-49f4-a90a-1254267fa165",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387a9d26-605d-4b3d-9942-33a8271965d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_reshaped, y_train, epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106fc83c-07e2-41f0-ac55-87ddd83111b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, LSTM, Conv3D, Flatten, Reshape, Conv3DTranspose\n",
    "\n",
    "# Define the input shape\n",
    "input_shape = (n_time, n_lat, n_lon, 3)  # 3 channels for alt, fch4, and fc\n",
    "\n",
    "# Build the model\n",
    "input_layer = Input(shape=input_shape)\n",
    "\n",
    "# Convolutional layers to capture spatiotemporal patterns\n",
    "x = Conv3D(32, (3, 3, 3), activation='relu', padding='same')(input_layer)\n",
    "x = Conv3D(64, (3, 3, 3), activation='relu', padding='same')(x)\n",
    "x = Flatten()(x)\n",
    "\n",
    "# Recurrent layer to capture temporal dependencies\n",
    "x = Reshape((n_time, -1))(x)\n",
    "x = LSTM(128, return_sequences=True)(x)\n",
    "x = LSTM(64)(x)\n",
    "\n",
    "# Dense layer for prediction\n",
    "output_layer = Dense(3, activation='linear')(x)  # Output layer for alt, fch4, fc\n",
    "\n",
    "# Define the model\n",
    "model = Model(inputs=input_layer, outputs=output_layer)\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Print the model summary\n",
    "model.summary()\n",
    "\n",
    "# Convert the dataframe to the input format for the model\n",
    "input_data = combined_df[['alt', 'fch4', 'fc']].values.reshape((n_time, n_lat, n_lon, 3))\n",
    "\n",
    "# Train the model\n",
    "model.fit(input_data, input_data, epochs=10, batch_size=16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab0f6ee-9b44-4c60-a8bd-555db4496d16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0baf54c8-8fa2-4b94-b4f0-300560d69a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.Series(train_alt_scaled.reshape(700253)).plot();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e1bbd1-9584-4e71-b53d-8187695c8ce3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c62ae6c-40bc-4edf-9b15-192999f74017",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras_tuner as kt\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "def build_model(hp):\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.ConvLSTM2D(filters=hp.Int('units', min_value=32, max_value=512, step=32), \n",
    "                                kernel_size=(3, 3), padding='same', return_sequences=True))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Dropout(hp.Float('dropout', 0.0, 0.5, step=0.1)))\n",
    "    model.add(layers.ConvLSTM2D(filters=hp.Int('units', min_value=32, max_value=512, step=32), \n",
    "                                kernel_size=(3, 3), padding='same', return_sequences=False))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(1))\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Initialize the tuner\n",
    "tuner = kt.BayesianOptimization(\n",
    "    build_model,\n",
    "    objective='val_loss',\n",
    "    max_trials=10,\n",
    "    executions_per_trial=3,\n",
    "    directory='my_dir',\n",
    "    project_name='bayesian_tuning')\n",
    "\n",
    "# Perform hyperparameter tuning\n",
    "tuner.search(train_alt_scaled, y_train_alt, epochs=50, validation_data=(val_alt_scaled, y_val_alt))\n",
    "\n",
    "# Get the best model\n",
    "best_model = tuner.get_best_models(num_models=1)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1e389e-9c61-4804-a182-e155db8ec6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrain on the best hyperparameters\n",
    "best_model.fit(np.concatenate([train_alt_scaled, val_alt_scaled]), np.concatenate([y_train_alt, y_val_alt]), \n",
    "               epochs=100, validation_split=0.2)\n",
    "\n",
    "# Evaluate on the test set\n",
    "test_loss, test_mae = best_model.evaluate(test_alt_scaled, y_test_alt)\n",
    "print(f\"Test Loss: {test_loss}, Test MAE: {test_mae}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19e27a1-e9f9-4875-88c7-7fe3aade989f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96316e25-e052-4414-a551-287fa7094ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, ConvLSTM2D, BatchNormalization, Dropout, Flatten, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# ConvLSTM Model\n",
    "def create_convlstm_model(input_shape):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = ConvLSTM2D(filters=64, kernel_size=(3, 3), padding='same', return_sequences=True)(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = ConvLSTM2D(filters=64, kernel_size=(3, 3), padding='same', return_sequences=False)(x)\n",
    "    x = Flatten()(x)\n",
    "    outputs = Dense(1)(x)\n",
    "    model = Model(inputs, outputs)\n",
    "    return model\n",
    "\n",
    "# Example usage\n",
    "input_shape = (None, 10, 10, 1)  # Adjust based on data dimensions\n",
    "convlstm_model = create_convlstm_model(input_shape)\n",
    "convlstm_model.compile(optimizer='adam', loss='mse')\n",
    "convlstm_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714b14e8-a65b-415b-a3bd-daf185e9dd09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import MultiHeadAttention, LayerNormalization, Dropout, Dense, Input, Flatten\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Transformer Block\n",
    "def transformer_block(inputs, head_size, num_heads, ff_dim, dropout=0):\n",
    "    x = MultiHeadAttention(key_dim=head_size, num_heads=num_heads)(inputs, inputs)\n",
    "    x = Dropout(dropout)(x)\n",
    "    x = LayerNormalization(epsilon=1e-6)(x)\n",
    "    res = x + inputs\n",
    "\n",
    "    x = Dense(ff_dim, activation=\"relu\")(res)\n",
    "    x = Dropout(dropout)(x)\n",
    "    x = Dense(inputs.shape[-1])(x)\n",
    "    x = LayerNormalization(epsilon=1e-6)(x)\n",
    "    return x + res\n",
    "\n",
    "# Transformer Model\n",
    "def create_transformer_model(input_shape, head_size, num_heads, ff_dim, num_transformer_blocks, mlp_units, dropout=0):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = inputs\n",
    "    for _ in range(num_transformer_blocks):\n",
    "        x = transformer_block(x, head_size, num_heads, ff_dim, dropout)\n",
    "\n",
    "    x = Flatten()(x)\n",
    "    for dim in mlp_units:\n",
    "        x = Dense(dim, activation=\"relu\")(x)\n",
    "        x = Dropout(dropout)(x)\n",
    "    outputs = Dense(1)(x)\n",
    "    model = Model(inputs, outputs)\n",
    "    return model\n",
    "\n",
    "# Example usage\n",
    "input_shape = (None, 10, 10)  # Adjust based on data dimensions\n",
    "transformer_model = create_transformer_model(input_shape, head_size=256, num_heads=4, ff_dim=4, num_transformer_blocks=4, mlp_units=[128, 64], dropout=0.1)\n",
    "transformer_model.compile(optimizer='adam', loss='mse')\n",
    "transformer_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d91fd1-3305-441a-a911-f1be69127676",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Lambda, Conv2D, Conv2DTranspose, Reshape\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "# Sampling function for VAE\n",
    "def sampling(args):\n",
    "    z_mean, z_log_var = args\n",
    "    batch = K.shape(z_mean)[0]\n",
    "    dim = K.int_shape(z_mean)[1]\n",
    "    epsilon = K.random_normal(shape=(batch, dim))\n",
    "    return z_mean + K.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "# VAE Model\n",
    "def create_vae(input_shape, latent_dim):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = Conv2D(32, 3, padding='same', activation='relu')(inputs)\n",
    "    x = Conv2D(64, 3, padding='same', activation='relu')(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(16, activation='relu')(x)\n",
    "    \n",
    "    z_mean = Dense(latent_dim)(x)\n",
    "    z_log_var = Dense(latent_dim)(x)\n",
    "    z = Lambda(sampling, output_shape=(latent_dim,))([z_mean, z_log_var])\n",
    "\n",
    "    encoder = Model(inputs, [z_mean, z_log_var, z], name='encoder')\n",
    "\n",
    "    latent_inputs = Input(shape=(latent_dim,))\n",
    "    x = Dense(64 * 64 * 64, activation='relu')(latent_inputs)\n",
    "    x = Reshape((64, 64, 64))(x)\n",
    "    x = Conv2DTranspose(64, 3, padding='same', activation='relu')(x)\n",
    "    x = Conv2DTranspose(32, 3, padding='same', activation='relu')(x)\n",
    "    outputs = Conv2DTranspose(1, 3, padding='same', activation='sigmoid')(x)\n",
    "\n",
    "    decoder = Model(latent_inputs, outputs, name='decoder')\n",
    "\n",
    "    outputs = decoder(encoder(inputs)[2])\n",
    "    vae = Model(inputs, outputs, name='vae')\n",
    "    \n",
    "    # VAE loss function\n",
    "    reconstruction_loss = tf.keras.losses.mse(K.flatten(inputs), K.flatten(outputs))\n",
    "    reconstruction_loss *= input_shape[0] * input_shape[1]\n",
    "    kl_loss = 1 + z_log_var - K.square(z_mean) - K.exp(z_log_var)\n",
    "    kl_loss = K.sum(kl_loss, axis=-1)\n",
    "    kl_loss *= -0.5\n",
    "    vae_loss = K.mean(reconstruction_loss + kl_loss)\n",
    "    vae.add_loss(vae_loss)\n",
    "    return vae\n",
    "\n",
    "# Example usage\n",
    "input_shape = (64, 64, 1)  # Adjust based on data dimensions\n",
    "latent_dim = 2\n",
    "vae_model = create_vae(input_shape, latent_dim)\n",
    "vae_model.compile(optimizer='adam')\n",
    "vae_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953040b9-e775-489b-ad6d-5d2b9d40f84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Concatenate\n",
    "\n",
    "def create_ensemble_model(convlstm_model, transformer_model, vae_model):\n",
    "    combined_input = Concatenate()([convlstm_model.output, transformer_model.output, vae_model.output])\n",
    "    x = Dense(128, activation=\"relu\")(combined_input)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Dense(64, activation=\"relu\")(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    outputs = Dense(1)(x)\n",
    "    ensemble_model = Model(inputs=[convlstm_model.input, transformer_model.input, vae_model.input], outputs=outputs)\n",
    "    return ensemble_model\n",
    "\n",
    "# Example usage\n",
    "ensemble_model = create_ensemble_model(convlstm_model, transformer_model, vae_model)\n",
    "ensemble_model.compile(optimizer='adam', loss='mse')\n",
    "ensemble_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017ac221-89db-49a5-b39f-526e43b6bd38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming X_train and y_train are preprocessed and ready\n",
    "\n",
    "# Example training call\n",
    "ensemble_model.fit([X_train_convlstm, X_train_transformer, X_train_vae], y_train, epochs=50, batch_size=32, validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477da09f-c751-40a2-b66f-a3e69fee90fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3399ead6-c55c-4b9e-bee8-191c8ccdff4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e7a223-ca14-41fe-b156-6ee4a464f415",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "af75c2b6-6dfc-438c-93ce-b00f4ec8ec38",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d281a2-02c2-4696-a19e-06179da73728",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#IN SITU\n",
    "reframed_alt.shape, type(reframed_alt), reframed_ch4.shape, type(reframed_ch4), reframed_co2.shape, type(reframed_co2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec60704-83e8-497c-aaf9-18c9186de330",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#SIBBORK\n",
    "sibbork.shape, type(sibbork)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a58b75-ab94-44e2-a490-bc1537abaeb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TCFM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b9f7da-aa23-444f-9127-f52117a1e445",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#AVIRISNG\n",
    "aviris_arr_list\n",
    "aviris_arr_list[0].shape, type(aviris_arr_list[0])\n",
    "#aviris_arr_list[0].shape, type(aviris_arr_list[0])\n",
    "#aviris_arr_list[1].shape, type(aviris_arr_list[0])\n",
    "#aviris_arr_list[2].shape, type(aviris_arr_list[0])\n",
    "#aviris_arr_list[3].shape, type(aviris_arr_list[0])\n",
    "#aviris_raster_list[0].shape, aviris_arr_list[0].shape #filtered by goodbandlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba72a73b-ace0-4e2a-bfb3-334598ba51c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#UAVSAR\n",
    "#plt.imshow(uavsar_arr_list[0][0])\n",
    "uavsar_arr_list\n",
    "uavsar_arr_list[0].shape, type(uavsar_arr_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d064e088-2e0f-4f8f-a65d-a9f8e0a6b58e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8d74aebb-f726-47d2-873e-2ecdbda58f14",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbacc059-5037-4ebf-ba87-d64750360a28",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#X_train.shape, X_valid.shape, X_test.shape, y_train.shape, y_valid.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a2c705-5ee2-4adb-a205-872248927c60",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# X_train_reframed=X_train_scaled.reshape(1772960,1,94);\n",
    "# X_valid_reframed=X_valid.values.reshape(453144,1,94);\n",
    "# X_test_reframed=X_test.values.reshape(215137,1,94)\n",
    "# #X_train_reframed.shape, X_valid_reframed.shape, X_test_reframed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d1d919-bf22-44b4-8268-2d6be71aa4e3",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# y_train_reframed=y_train.values.reshape(1772960,1,1);\n",
    "# y_valid_reframed=y_valid.values.reshape(453144,1,1);\n",
    "# y_test_reframed=y_test.values.reshape(215137,1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eefcf390-dd58-479a-bc5a-4e4590bdd5c5",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# X_train_reframed_sup=X_train_reframed_sup.reshape(1772957,1,376);\n",
    "# X_valid_reframed_sup=X_valid_reframed_sup.reshape(453141,1,376);\n",
    "# X_test_reframed_sup=X_test_reframed_sup.reshape(215134,1,376)\n",
    "# #X_train_reframed.shape, X_valid_reframed.shape, X_test_reframed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3752ef4c-5fba-4536-b257-534009d31ef3",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# y_train_reframed_sup=y_train_reframed_sup.reshape(7091828,1,1);\n",
    "# y_valid_reframed_sup=y_valid_reframed_sup.reshape(1812564,1,1);\n",
    "# y_test_reframed_sup=y_test_reframed_sup.reshape(860536,1,1)\n",
    "# #y_train_reframed.shape, y_valid_reframed.shape, y_test_reframed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58d2a8a-e1b6-4da9-ab3b-c07dd54f3dd7",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# X_train_reframed_sup_tensor=tf.convert_to_tensor(X_train_reframed_sup)#.reshape(1772957, 1, 376))\n",
    "# y_train_reframed_sup_tensor=tf.convert_to_tensor(y_train_reframed_sup)#.reshape(1772957, 1, 4))\n",
    "# X_valid_reframed_sup_tensor=tf.convert_to_tensor(X_valid_reframed_sup)#.reshape(453141, 1, 376))\n",
    "# y_valid_reframed_sup_tensor=tf.convert_to_tensor(y_valid_reframed_sup)#.reshape(453141, 1, 4))\n",
    "# X_test_reframed_sup_tensor=tf.convert_to_tensor(X_test_reframed_sup)#.reshape(215134, 1, 376))\n",
    "# y_test_reframed_sup_tensor=tf.convert_to_tensor(y_test_reframed_sup)#.reshape(215134, 1, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34151e39-bee7-4f22-b343-54c2059c8f49",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### ALT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493fcc81-95d9-4a55-b8ff-1c10449051a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape=(trainXscaltref.shape[1],trainXscaltref.shape[2]))#, X_train_reframed_sup.shape[2]))#, X_train_reframed_sup.shape[3]))\n",
    "inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4351e3-fdaa-4e46-b3b7-302f4890e538",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainXscaltref.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81331fd-7b0c-4614-baeb-23e9538b311a",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04beaee3-1215-4bdb-bd71-bde5c8e8e826",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### CH4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2508e9e5-52c4-4009-a173-c0c6e673a78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape=(trainXscch4ref.shape[1],trainXscch4ref.shape[2]))#, X_train_reframed_sup.shape[2]))#, X_train_reframed_sup.shape[3]))\n",
    "inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81fac49e-b3a5-471e-86cf-28a6647689af",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainXscch4ref.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab29f1bf-8317-4e71-81b6-04de8728c855",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a902ad14-2040-405f-9508-80d0af2380ad",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### CO2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db28f58-2c6e-4825-abab-2cc75bb92bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape=(trainXscco2ref.shape[1],trainXscco2ref.shape[2]))#, X_train_reframed_sup.shape[2]))#, X_train_reframed_sup.shape[3]))\n",
    "inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40924467-f42b-47be-89bf-dd30db8b25ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainXscco2ref.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2296d854-beef-430c-b5df-19d6021ff033",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3a5ff4-6dd7-44b0-bcd1-a655220b1cc2",
   "metadata": {},
   "source": [
    "### Hyperparameterization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9538f56e-4010-440b-9914-8dc6ccac3a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape=(train_alt.shape[1], train_alt.shape[2], train_alt.shape[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47bbc372-732f-4953-b0b1-3b8a34604966",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs = keras.Input(shape=(trainXscch4ref.shape[1], trainXscch4ref.shape[2]))\n",
    "hp = HyperParameters()\n",
    "units=hp.Int(\"units\", min_value=32, max_value = inputs.shape[2], step=16)\n",
    "#batch_size=hp.Int(\"batch_size\", min_value = 32, max_value = 256, step = 32)\n",
    "learning_rate=hp.Choice(\"learning_rate\", [1e-1, 1e-2, 1e-3, 1e-4, 1e-5])\n",
    "#inputs2 = keras.Input(shape=(alt_Xtrainsc.shape[1], alt_Xtrainsc.shape[2]))\n",
    "#units2=hp.Int(\"units2\", min_value=64, max_value = inputs2.shape[2], step=64)\n",
    "#padding=hp.Choice(\"padding\", ['valid','same','causal'])\n",
    "#n_layers=hp.Int(\"n_layers\", min_value = 1, max_value = 9, step=3)\n",
    "#n_layers=hp.Int(\"n_layers\", min_value = 5, max_value = 15) \n",
    "#batch_size=hp.Int(\"batch_size\", min_value = 32, max_value = 256, step = 32)\n",
    "\n",
    "conv1d_filters=hp.Int(\"conv1d_filters\",min_value=hp['units'],max_value=2*hp['units'])\n",
    "conv1d_kernel_size=hp.Int(\"conv1d_kernel_size\", min_value = 3, max_value = 6, step=1)\n",
    "conv1d_activation=hp.Choice(\"conv1d_activation\", ['relu','sigmoid', 'tanh', 'exponential', 'gelu', 'elu', 'linear', 'selu', 'softmax', 'swish'])\n",
    "\n",
    "conv1d2_filters=hp.Int(\"conv1d2_filters\", min_value = hp['units'], max_value=2*hp['units'])\n",
    "conv1d2_kernel_size=hp.Int(\"conv1d2_kernel_size\", min_value = 3, max_value = 6, step=1)\n",
    "conv1d2_activation=hp.Choice(\"conv1d2_activation\", ['relu','sigmoid', 'tanh', 'exponential', 'gelu', 'elu', 'linear', 'selu', 'softmax', 'swish'])\n",
    "\n",
    "# conv2d3_filters=hp.Int(\"conv2d3_filters\",min_value=hp['units'],max_value=2*hp['units'])\n",
    "# conv2d3_kernel_size=hp.Int(\"conv2d3_kernel_size\", min_value = 3, max_value = 6, step=1)\n",
    "# conv2d3_activation=hp.Choice(\"conv2d3_activation\", ['relu','sigmoid', 'tanh', 'exponential', 'gelu', 'elu', 'linear', 'selu', 'softmax', 'swish'])\n",
    "\n",
    "# conv2d4_filters=hp.Int(\"conv2d4_filters\", min_value = 64, max_value=2*hp['units'])\n",
    "# conv2d4_kernel_size=hp.Int(\"conv2d4_kernel_size\", min_value = 3, max_value = 6, step=1)\n",
    "# conv2d4_activation=hp.Choice(\"conv2d4_activation\", ['relu','sigmoid', 'tanh', 'exponential', 'gelu', 'elu', 'linear', 'selu', 'softmax', 'swish'])\n",
    "\n",
    "# conv2d5_filters=hp.Int(\"conv2d5_filters\", min_value = 64, max_value=2*hp['units'])\n",
    "# conv2d5_kernel_size=hp.Int(\"conv2d5_kernel_size\", min_value = 3, max_value = 6, step=1)\n",
    "# conv2d5_activation=hp.Choice(\"conv2d5_activation\", ['relu','sigmoid', 'tanh', 'exponential', 'gelu', 'elu', 'linear', 'selu', 'softmax', 'swish'])\n",
    "\n",
    "bilstm_units = hp.Int(\"bilstm_units\", min_value = hp['units'], max_value = 2*hp['units'])\n",
    "bilstm_activation=hp.Choice(\"bilstm_activation\", ['relu','sigmoid', 'tanh', 'exponential', 'gelu', 'elu', 'linear', 'selu', 'softmax', 'swish'])\n",
    "bilstm_rec_activation=hp.Choice(\"bilstm_rec_activation\", ['relu','sigmoid', 'tanh', 'exponential', 'gelu', 'elu', 'linear', 'selu', 'softmax', 'swish'])\n",
    "bilstm_dropout=hp.Choice(\"bilstm_dropout\", [0.1, 0.2, 0.3, 0.4])\n",
    "bilstm_rec_dropout=hp.Choice(\"bilstm_rec_dropout\", [0.1, 0.2, 0.3, 0.4])\n",
    "bilstm_bias=hp.Boolean(\"bilstm_use_bias\")\n",
    "bilstm_f_bias=hp.Boolean(\"bilstm_forgot_bias\")\n",
    "\n",
    "bilstm2_units = hp.Int(\"bilstm2_units\", min_value = hp['units'], max_value = 2*hp['units'])\n",
    "bilstm2_activation=hp.Choice(\"bilstm2_activation\", ['relu','sigmoid', 'tanh', 'exponential', 'gelu', 'elu', 'linear', 'selu', 'softmax', 'swish'])\n",
    "bilstm2_rec_activation=hp.Choice(\"bilstm2_rec_activation\", ['relu','sigmoid', 'tanh', 'exponential', 'gelu', 'elu', 'linear', 'selu', 'softmax', 'swish'])\n",
    "bilstm2_dropout=hp.Choice(\"bilstm2_dropout\", [0.1, 0.2, 0.3, 0.4])\n",
    "bilstm2_rec_dropout=hp.Choice(\"bilstm2_rec_dropout\", [0.1, 0.2, 0.3, 0.4])\n",
    "bilstm2_bias=hp.Boolean(\"bilstm2_use_bias\")\n",
    "bilstm2_f_bias=hp.Boolean(\"bilstm2_forgot_bias\")\n",
    "\n",
    "bilstm3_units = hp.Int(\"bilstm3_units\", min_value = hp['units'], max_value = 2*hp['units'])\n",
    "bilstm3_activation=hp.Choice(\"bilstm3_activation\", ['relu','sigmoid', 'tanh', 'exponential', 'gelu', 'elu', 'linear', 'selu', 'softmax', 'swish'])\n",
    "bilstm3_rec_activation=hp.Choice(\"bilstm3_rec_activation\", ['relu','sigmoid', 'tanh', 'exponential', 'gelu', 'elu', 'linear', 'selu', 'softmax', 'swish'])\n",
    "bilstm3_dropout=hp.Choice(\"bilstm3_dropout\", [0.1, 0.2, 0.3, 0.4])\n",
    "bilstm3_rec_dropout=hp.Choice(\"bilstm3_rec_dropout\", [0.1, 0.2, 0.3, 0.4])\n",
    "bilstm3_bias=hp.Boolean(\"bilstm3_use_bias\")\n",
    "bilstm3_f_bias=hp.Boolean(\"bilstm3_forgot_bias\")\n",
    "\n",
    "bilstm4_units = hp.Int(\"bilstm4_units\", min_value = hp['units'], max_value = 2*hp['units'])\n",
    "bilstm4_activation=hp.Choice(\"bilstm4_activation\", ['relu','sigmoid', 'tanh', 'exponential', 'gelu', 'elu', 'linear', 'selu', 'softmax', 'swish'])\n",
    "bilstm4_rec_activation=hp.Choice(\"bilstm4_rec_activation\", ['relu','sigmoid', 'tanh', 'exponential', 'gelu', 'elu', 'linear', 'selu', 'softmax', 'swish'])\n",
    "bilstm4_dropout=hp.Choice(\"bilstm4_dropout\", [0.1, 0.2, 0.3, 0.4])\n",
    "bilstm4_rec_dropout=hp.Choice(\"bilstm4_rec_dropout\", [0.1, 0.2, 0.3, 0.4])\n",
    "bilstm4_bias=hp.Boolean(\"bilstm4_use_bias\")\n",
    "bilstm4_f_bias=hp.Boolean(\"bilstm4_forgot_bias\")\n",
    "\n",
    "lstm_units = hp.Int(\"lstm_units\", min_value = hp['units'], max_value = 2*hp['units'])\n",
    "lstm_activation=hp.Choice(\"lstm_activation\", ['relu','sigmoid', 'tanh', 'exponential', 'gelu', 'elu', 'linear', 'selu', 'softmax', 'swish'])\n",
    "lstm_rec_activation=hp.Choice(\"lstm_rec_activation\", ['relu','sigmoid', 'tanh', 'exponential', 'gelu', 'elu', 'linear', 'selu', 'softmax', 'swish'])\n",
    "lstm_dropout=hp.Choice(\"lstm_dropout\", [0.1, 0.2, 0.3, 0.4])\n",
    "lstm_rec_dropout=hp.Choice(\"lstm_rec_dropout\", [0.1, 0.2, 0.3, 0.4])\n",
    "\n",
    "lstm2_units = hp.Int(\"lstm2_units\", min_value = hp['units'], max_value = 2*hp['units'])\n",
    "lstm2_activation=hp.Choice(\"lstm2_activation\", ['relu','sigmoid', 'tanh', 'exponential', 'gelu', 'elu', 'linear', 'selu', 'softmax', 'swish'])\n",
    "lstm2_rec_activation=hp.Choice(\"lstm2_rec_activation\", ['relu','sigmoid', 'tanh', 'exponential', 'gelu', 'elu', 'linear', 'selu', 'softmax', 'swish'])\n",
    "lstm2_dropout=hp.Choice(\"lstm2_dropout\", [0.1, 0.2, 0.3, 0.4])\n",
    "lstm2_rec_dropout=hp.Choice(\"lstm2_rec_dropout\", [0.1, 0.2, 0.3, 0.4])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82bae248-6dde-47a7-bda4-548c0a871044",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1c5b65-ab86-4c4e-ba0d-287407209a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class geocryoai(HyperModel):\n",
    "    def build(self, hp):\n",
    "        #backend.clear_session()\n",
    "        #inputs = keras.Input(shape=(X_train_reframed.shape[1], X_train_reframed.shape[2]))\n",
    "        model = tf.keras.Sequential()\n",
    "        #for i in range(n_layers):\n",
    "        model.add(Conv1D(\n",
    "            filters=hp['conv1d_filters'], \n",
    "            kernel_size=hp['conv1d_kernel_size'], \n",
    "            activation = hp['conv1d_activation'],\n",
    "            padding='same', \n",
    "            input_shape=(inputs.shape[1], inputs.shape[2])))\n",
    "        # model.add(MaxPool1D(pool_size=1))\n",
    "        #for i in range(hp['n_layers']): #TUNE THIS (LAYERS) WHEN ADDING SATELLITE AND MODELING DATA\n",
    "        model.add(Bidirectional(LSTM(\n",
    "            input_shape=(inputs.shape[1], inputs.shape[2]),\n",
    "            return_sequences = True, \n",
    "            #kernel_initializer = tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.00001),\n",
    "            #name = f\"1BiLSTM_layer_{i+1}\",\n",
    "            units = hp['bilstm_units'],\n",
    "            activation = hp['bilstm_activation'],\n",
    "            #recurrent_activation = hp['bilstm_rec_activation'],\n",
    "            use_bias = hp['bilstm_use_bias'],\n",
    "            unit_forget_bias = hp['bilstm_forgot_bias'],\n",
    "            dropout=hp['bilstm_dropout'],\n",
    "            recurrent_dropout = hp['bilstm_rec_dropout'])))\n",
    "        model.add(Bidirectional(LSTM(\n",
    "            input_shape=(inputs.shape[1], inputs.shape[2]),\n",
    "            return_sequences = True, \n",
    "            #kernel_initializer = tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.00001),\n",
    "            #name = f\"1BiLSTM_layer_{i+1}\",\n",
    "            units = hp['bilstm2_units'],\n",
    "            activation = hp['bilstm2_activation'],\n",
    "            #recurrent_activation = hp['bilstm2_rec_activation'],\n",
    "            use_bias = hp['bilstm2_use_bias'],\n",
    "            unit_forget_bias = hp['bilstm2_forgot_bias'],\n",
    "            dropout=hp['bilstm2_dropout'],\n",
    "            recurrent_dropout = hp['bilstm2_rec_dropout'])))\n",
    "        #for i in range(hp['n_layers']):\n",
    "        model.add(LSTM(\n",
    "            units=hp['lstm_units'],\n",
    "            activation = hp['lstm_activation'],\n",
    "            #recurrent_activation=hp['lstm_rec_activation'],\n",
    "            return_sequences=False, \n",
    "            dropout=hp['lstm_dropout'],\n",
    "            recurrent_dropout=hp['lstm_rec_dropout'],\n",
    "            input_shape=(inputs.shape[1], inputs.shape[2])))\n",
    "        #model.add(Bidirectional(LSTM(inputs.shape[-1], activation='relu', return_sequences = False, dropout=0, input_shape=(inputs.shape[1], inputs.shape[2]))))\n",
    "        model.add(RepeatVector(inputs.shape[1]))   #TUNE THIS (LAYERS) WHEN ADDING SATELLITE AND MODELING DATA\n",
    "        model.add(LSTM(\n",
    "            units=hp['lstm2_units'], \n",
    "            activation = hp['lstm2_activation'],\n",
    "            #recurrent_activation= hp['lstm2_rec_activation'],\n",
    "            return_sequences=True,\n",
    "            dropout=hp['lstm2_dropout'],\n",
    "            recurrent_dropout=hp['lstm2_rec_dropout'],\n",
    "            input_shape=(inputs.shape[1], inputs.shape[2])))\n",
    "        #model.add(Bidirectional(LSTM(inputs.shape[-1], activation='relu', return_sequences = True, dropout=0, input_shape=(inputs.shape[1], inputs.shape[2])))) \n",
    "        model.add(Bidirectional(LSTM(\n",
    "            input_shape=(inputs.shape[1], inputs.shape[2]),\n",
    "            return_sequences = True, \n",
    "            #kernel_initializer = tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.00001),\n",
    "            #name = f\"2BiLSTM_layer_{i+1}\",\n",
    "            units = hp['bilstm3_units'],\n",
    "            activation = hp['bilstm3_activation'],\n",
    "            #recurrent_activation = hp['bilstm3_rec_activation'],\n",
    "            use_bias = hp['bilstm3_use_bias'],\n",
    "            unit_forget_bias = hp['bilstm3_forgot_bias'],\n",
    "            dropout=hp['bilstm3_dropout'],\n",
    "            recurrent_dropout = hp['bilstm3_rec_dropout'])))\n",
    "        model.add(Bidirectional(LSTM(\n",
    "            input_shape=(inputs.shape[1], inputs.shape[2]),\n",
    "            return_sequences = hp['bilstm4_units'], \n",
    "            #kernel_initializer = tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.00001),\n",
    "            #name = f\"2BiLSTM_layer_{i+1}\",\n",
    "            units = 32,\n",
    "            activation = hp['bilstm4_activation'],\n",
    "            #recurrent_activation = hp['bilstm4_rec_activation'],\n",
    "            use_bias = hp['bilstm4_use_bias'],\n",
    "            unit_forget_bias = hp['bilstm4_forgot_bias'],\n",
    "            dropout=hp['bilstm4_dropout'],\n",
    "            recurrent_dropout = hp['bilstm4_rec_dropout'])))\n",
    "        model.add(Conv1DTranspose(\n",
    "           filters=hp['conv1d2_filters'], \n",
    "           kernel_size=hp['conv1d2_kernel_size'], \n",
    "           activation = hp['conv1d2_activation'],\n",
    "           padding='same', \n",
    "           input_shape=(inputs.shape[1], inputs.shape[2])))\n",
    "        model.add(TimeDistributed(Dense(trainyscaltref.shape[1])))\n",
    "        #model.add(TimeDistributed(Dense(trainyscch4ref.shape[1])))\n",
    "        #model.add(TimeDistributed(Dense(trainyscco2ref.shape[1])))\n",
    "        model.add(Dense(trainXscaltref.shape[1]))\n",
    "        #model.add(Dense(trainXscch4ref.shape[1]))\n",
    "        #model.add(Dense(trainXscco2ref.shape[1]))\n",
    "        metrics=['mean_squared_error', 'mean_absolute_error', RSquare().build(input_shape = (trainyscaltref.shape[1],))]\n",
    "        #metrics=['mean_squared_error', 'mean_absolute_error', RSquare().build(input_shape = (trainyscch4ref.shape[1],))]\n",
    "        #metrics=['mean_squared_error', 'mean_absolute_error', RSquare().build(input_shape = (trainyscco2ref.shape[1],))]\n",
    "        loss_function = 'mean_squared_error'\n",
    "        model.compile(optimizer = tf.keras.optimizers.legacy.RMSprop(learning_rate = hp.get('learning_rate'), **{\"clipvalue\" : 1000}),loss = loss_function, metrics = metrics)\n",
    "        return model\n",
    "    \n",
    "    def fit(self, hp, model, *args, **kwargs):\n",
    "        return model.fit(*args, batch_size = 128, **kwargs) #hp['batch_size'], **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a8b51e-c253-4f53-aea7-190fe8f89d1b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# model=geocryoai.build(train_alt, hp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b9fa26-d7ca-4b1e-a871-3ed12e7fe0ca",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c124ac42-5f2f-4d50-aea1-b22c4415b788",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# # serialize model to JSON\n",
    "# model_json = model.to_json()\n",
    "# #with open(\"model_070923_insituALT.json\", \"w\") as json_file:\n",
    "# with open(\"model_070923_insituCH4.json\", \"w\") as json_file:\n",
    "# #with open(\"model_070923_insituCO2.json\", \"w\") as json_file:\n",
    "#     json_file.write(model_json)\n",
    "# # serialize weights to HDF5\n",
    "# #model.save_weights(\"model_070923_insituALT_experimental.h5\")\n",
    "# model.save_weights(\"model_070923_insituCH4.h5\")\n",
    "# #model.save_weights(\"model_070923_insituCO2.h5\")\n",
    "# print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6c5c52-94c1-42b6-8e02-a531d5bd941a",
   "metadata": {},
   "outputs": [],
   "source": [
    "bayesian_tuner = BayesianOptimization(\n",
    "                    hypermodel = geocryoai(),\n",
    "                    objective = \"val_loss\",\n",
    "                    max_trials = 10,\n",
    "                    #num_initial_points = 8, #defaults to 3xdimensionality of hyperparameterization space used\n",
    "                    alpha = 0.0001, #0.01, #0.0001 #default; represents the expected amount of noise in the observed performances in Bayesian optimization.\n",
    "                    beta = 2.6, #10, #2.6, #default;  the balancing factor of exploration and exploitation. The larger it is, the more explorative it is\n",
    "                    hyperparameters = hp,\n",
    "                    **{\"tuner_id\" : \"BayesianOptimization_121924_ALT\",\n",
    "                    #**{\"tuner_id\" : \"BayesianOptimization_071223_CH4\",\n",
    "                    #**{\"tuner_id\" : \"BayesianOptimization_071223_CO2\",\n",
    "                      #\"overwrite\" : False,\n",
    "                      \"project_name\" : \"Bayesian_optimization_121924_ALT\"}\n",
    "                      #\"project_name\" : \"bayesian_optimization_071223_CH4\"}\n",
    "                      #\"project_name\" : \"bayesian_optimization_071223_CO2\"}\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d200f9e4-ec54-4bd2-9022-3b58606d3e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_loss', verbose = 1, patience = 10, min_delta = 1e-4, restore_best_weights = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8f9409-117e-4743-b1fa-906112fd47f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainXscaltref.shape, trainyscaltref.shape\n",
    "#trainXscch4ref.shape, trainyscch4ref.shape\n",
    "#trainXscco2ref.shape, trainyscco2ref.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd88f92-801f-4022-aee6-ec375b342f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ALT\n",
    "bayesian_tuner.search(trainXscaltref, \n",
    "                     trainyscaltref, \n",
    "                     steps_per_epoch = None, \n",
    "                     shuffle = False, \n",
    "                     validation_data = (validXscaltref, validyscaltref),\n",
    "                     #validation_split = 0.15,#0.2,\n",
    "                     verbose = 1, #2, #epoch, #1, #progress bar #0, #nothing\n",
    "                     callbacks = [early_stopping, History(), TerminateOnNaN(),keras.callbacks.TensorBoard(\"/tmp/tb_logs\")], \n",
    "                     use_multiprocessing = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40141951-c26f-4ecd-a24f-294052e05b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #CH4\n",
    "# bayesian_tuner.search(trainXscch4ref, \n",
    "#                      trainyscch4ref, \n",
    "#                      steps_per_epoch = None, \n",
    "#                      shuffle = False, \n",
    "#                      validation_data = (validXscch4ref, validyscch4ref),\n",
    "#                      #validation_split = 0.15,#0.2,\n",
    "#                      verbose = 1,\n",
    "#                      callbacks = [early_stopping, History(), TerminateOnNaN(),keras.callbacks.TensorBoard(\"/tmp/tb_logs\")], \n",
    "#                      use_multiprocessing = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a64a296-0ad5-498f-a714-18ac193e05de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #CO2\n",
    "# bayesian_tuner.search(trainXscco2ref, \n",
    "#                      trainyscco2ref, \n",
    "#                      steps_per_epoch = None, \n",
    "#                      shuffle = False, \n",
    "#                      validation_data = (validXscco2ref, validyscco2ref),\n",
    "#                      #validation_split = 0.15,#0.2,\n",
    "#                      verbose = 1,\n",
    "#                      callbacks = [early_stopping, History(), TerminateOnNaN(),keras.callbacks.TensorBoard(\"/tmp/tb_logs\")], \n",
    "#                      use_multiprocessing = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68fea369-b7ac-42f1-a97b-a26f032afac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "bayesian_tuner.results_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa707e3e-98b7-4830-8df5-7def11bae50e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#HP ALT\n",
    "# BEST MODEL: TRIAL24\n",
    "# Trial 27 Complete [01h 42m 02s]\n",
    "# val_loss: 0.10911799222230911\n",
    "\n",
    "# Best val_loss So Far: 0.10056757181882858\n",
    "# Total elapsed time: 16h 43m 38s\n",
    "\n",
    "# Search: Running Trial #28\n",
    "\n",
    "# Value             |Best Value So Far |Hyperparameter\n",
    "# 0.0001            |0.0001            |learning_rate\n",
    "# 64                |64                |batch_size\n",
    "# 64                |64                |units\n",
    "# 91                |96                |conv1d_filters\n",
    "# 3                 |9                 |conv1d_kernel_size\n",
    "# relu              |swish             |conv1d_activation\n",
    "# 107               |97                |bilstm_units\n",
    "# swish             |relu              |bilstm_activation\n",
    "# relu              |relu              |bilstm_rec_activation\n",
    "# 0.1               |0.4               |bilstm_dropout\n",
    "# 0.1               |0.1               |bilstm_rec_dropout\n",
    "# False             |False             |bilstm_use_bias\n",
    "# True              |True              |bilstm_forgot_bias\n",
    "# 88                |64                |lstm_units\n",
    "# gelu              |relu              |lstm_activation\n",
    "# softmax           |relu              |lstm_rec_activation\n",
    "# 0.1               |0.1               |lstm_dropout\n",
    "# 0.1               |0.1               |lstm_rec_dropout\n",
    "# 128               |128               |lstm2_units\n",
    "# relu              |relu              |lstm2_activation\n",
    "# relu              |relu              |lstm2_rec_activation\n",
    "# 0.1               |0.1               |lstm2_dropout\n",
    "# 0.4               |0.4               |lstm2_rec_dropout\n",
    "# 128               |128               |bilstm2_units\n",
    "# relu              |relu              |bilstm2_activation\n",
    "# swish             |sigmoid           |bilstm2_rec_activation\n",
    "# 0.4               |0.4               |bilstm2_dropout\n",
    "# 0.4               |0.4               |bilstm2_rec_dropout\n",
    "# True              |True              |bilstm2_use_bias\n",
    "# False             |False             |bilstm2_forgot_bias\n",
    "# 91                |88                |conv1d2_filters\n",
    "# 9                 |9                 |conv1d2_kernel_size\n",
    "# swish             |swish             |conv1d2_activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e292240b-3e52-437a-9fba-2746800a1c2e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# #ALT Results\n",
    "# Results summary\n",
    "# Results in ./Bayesian_optimization_071423_ALT\n",
    "# Showing 10 best trials\n",
    "# Objective(name=\"val_loss\", direction=\"min\")\n",
    "\n",
    "# Trial 09 summary\n",
    "# Hyperparameters:\n",
    "# units: 224\n",
    "# learning_rate: 0.01\n",
    "# conv1d_filters: 38\n",
    "# conv1d_kernel_size: 6\n",
    "# conv1d_activation: linear\n",
    "# conv1d2_filters: 48\n",
    "# conv1d2_kernel_size: 5\n",
    "# conv1d2_activation: exponential\n",
    "# bilstm_units: 63\n",
    "# bilstm_activation: sigmoid\n",
    "# bilstm_rec_activation: tanh\n",
    "# bilstm_dropout: 0.3\n",
    "# bilstm_rec_dropout: 0.1\n",
    "# bilstm_use_bias: False\n",
    "# bilstm_forgot_bias: True\n",
    "# bilstm2_units: 45\n",
    "# bilstm2_activation: sigmoid\n",
    "# bilstm2_rec_activation: exponential\n",
    "# bilstm2_dropout: 0.1\n",
    "# bilstm2_rec_dropout: 0.1\n",
    "# bilstm2_use_bias: True\n",
    "# bilstm2_forgot_bias: False\n",
    "# bilstm3_units: 46\n",
    "# bilstm3_activation: selu\n",
    "# bilstm3_rec_activation: relu\n",
    "# bilstm3_dropout: 0.4\n",
    "# bilstm3_rec_dropout: 0.1\n",
    "# bilstm3_use_bias: True\n",
    "# bilstm3_forgot_bias: False\n",
    "# bilstm4_units: 47\n",
    "# bilstm4_activation: sigmoid\n",
    "# bilstm4_rec_activation: gelu\n",
    "# bilstm4_dropout: 0.3\n",
    "# bilstm4_rec_dropout: 0.1\n",
    "# bilstm4_use_bias: False\n",
    "# bilstm4_forgot_bias: False\n",
    "# lstm_units: 63\n",
    "# lstm_activation: sigmoid\n",
    "# lstm_rec_activation: gelu\n",
    "# lstm_dropout: 0.3\n",
    "# lstm_rec_dropout: 0.1\n",
    "# lstm2_units: 64\n",
    "# lstm2_activation: exponential\n",
    "# lstm2_rec_activation: selu\n",
    "# lstm2_dropout: 0.3\n",
    "# lstm2_rec_dropout: 0.3\n",
    "# Score: 2.8679637908935547"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf1b3906-6f74-410c-8ef8-1e261c43056c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# #ALT\n",
    "# class geocryoai2(HyperModel):\n",
    "#     def build(self, hp):\n",
    "#         #backend.clear_session()\n",
    "#         #inputs = keras.Input(shape=(X_train_reframed.shape[1], X_train_reframed.shape[2]))\n",
    "#         inputs = keras.Input(shape=(trainXscch4ref.shape[1], trainXscch4ref.shape[2]))        \n",
    "#         model = tf.keras.Sequential()\n",
    "#         #for i in range(n_layers):\n",
    "#         model.add(Conv1D(\n",
    "#             filters=38, \n",
    "#             kernel_size=6, \n",
    "#             activation = 'swish',\n",
    "#             padding='same', \n",
    "#             input_shape=(inputs.shape[1], inputs.shape[2])))\n",
    "#         # model.add(MaxPool1D(pool_size=1))\n",
    "#         #for i in range(hp['n_layers']): #TUNE THIS (LAYERS) WHEN ADDING SATELLITE AND MODELING DATA\n",
    "#         model.add(Bidirectional(LSTM(\n",
    "#             input_shape=(inputs.shape[1], inputs.shape[2]),\n",
    "#             return_sequences = True, \n",
    "#             #kernel_initializer = tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.00001),\n",
    "#             #name = f\"1BiLSTM_layer_{i+1}\",\n",
    "#             units = 97,\n",
    "#             activation = 'relu',\n",
    "#             #recurrent_activation = 'relu',\n",
    "#             use_bias = False,\n",
    "#             unit_forget_bias = True,\n",
    "#             dropout=0.1,\n",
    "#             recurrent_dropout = 0.1)))\n",
    "#         model.add(LSTM(\n",
    "#             units=64,\n",
    "#             activation = 'relu',\n",
    "#             #recurrent_activation = 'relu',\n",
    "#             return_sequences=False, \n",
    "#             dropout=0.1,\n",
    "#             recurrent_dropout=0.1,\n",
    "#             input_shape=(inputs.shape[1], inputs.shape[2])))\n",
    "#         #model.add(Bidirectional(LSTM(inputs.shape[-1], activation='relu', return_sequences = False, dropout=0, input_shape=(inputs.shape[1], inputs.shape[2]))))\n",
    "#         model.add(RepeatVector(inputs.shape[1]))   #TUNE THIS (LAYERS) WHEN ADDING SATELLITE AND MODELING DATA\n",
    "#         model.add(LSTM(\n",
    "#             units=128, \n",
    "#             activation = 'relu',\n",
    "#             #recurrent_activation='relu',\n",
    "#             return_sequences=True,\n",
    "#             dropout=0.1,\n",
    "#             recurrent_dropout=0.1,\n",
    "#             input_shape=(inputs.shape[1], inputs.shape[2])))\n",
    "#         #model.add(Bidirectional(LSTM(inputs.shape[-1], activation='relu', return_sequences = True, dropout=0, input_shape=(inputs.shape[1], inputs.shape[2])))) \n",
    "#         model.add(Bidirectional(LSTM(\n",
    "#             input_shape=(inputs.shape[1], inputs.shape[2]),\n",
    "#             return_sequences = True, \n",
    "#             #kernel_initializer = tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.00001),\n",
    "#             #name = f\"2BiLSTM_layer_{i+1}\",\n",
    "#             units = 128,\n",
    "#             activation = 'relu',\n",
    "#             #recurrent_activation = 'sigmoid',\n",
    "#             use_bias = True,\n",
    "#             unit_forget_bias = False,\n",
    "#             dropout=0.1,\n",
    "#             recurrent_dropout = 0.1)))\n",
    "#         model.add(Conv1DTranspose(\n",
    "#            filters=88, \n",
    "#            kernel_size=9, \n",
    "#            activation = 'swish',\n",
    "#            padding='same', \n",
    "#            input_shape=(inputs.shape[1], inputs.shape[2])))\n",
    "#         model.add(TimeDistributed(Dense(trainyscaltref.shape[1])))\n",
    "#         #model.add(TimeDistributed(Dense(trainyscch4ref.shape[1])))\n",
    "#         #model.add(Dense(trainXscaltref.shape[1]))\n",
    "#         #model.add(Dense(trainXscch4ref.shape[1]))\n",
    "#         metrics=['mean_squared_error', 'mean_absolute_error', RSquare().build(input_shape = (trainyscaltref.shape[1],))]\n",
    "#         #metrics=['mean_squared_error', 'mean_absolute_error', RSquare().build(input_shape = (trainyscch4ref.shape[1],))]\n",
    "#         loss_function = 'mean_squared_error'\n",
    "#         model.compile(optimizer = tf.keras.optimizers.legacy.RMSprop(learning_rate = 0.0001, **{\"clipvalue\" : 1000}),loss = loss_function, metrics = metrics)\n",
    "#         return model\n",
    "    \n",
    "#     def fit(self, hp, model, *args, **kwargs):\n",
    "#         return model.fit(*args, batch_size = 256, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5873e2a5-b58c-460e-98d8-5cb8be2f64e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ALT\n",
    "class geocryoai2(HyperModel):\n",
    "    def build(self, hp):\n",
    "        #backend.clear_session()\n",
    "        inputs = keras.Input(shape=(trainXscaltref.shape[1], trainXscaltref.shape[2]))\n",
    "        #inputs = keras.Input(shape=(trainXscch4ref.shape[1], trainXscch4ref.shape[2]))        \n",
    "        model = tf.keras.Sequential()\n",
    "        #for i in range(n_layers):\n",
    "        model.add(Conv1D(\n",
    "            filters=96, \n",
    "            kernel_size=9, \n",
    "            activation = 'swish',\n",
    "            padding='same', \n",
    "            input_shape=(inputs.shape[1], inputs.shape[2])))\n",
    "        # model.add(MaxPool1D(pool_size=1))\n",
    "        #for i in range(hp['n_layers']): #TUNE THIS (LAYERS) WHEN ADDING SATELLITE AND MODELING DATA\n",
    "        model.add(Bidirectional(LSTM(\n",
    "            input_shape=(inputs.shape[1], inputs.shape[2]),\n",
    "            return_sequences = True, \n",
    "            #kernel_initializer = tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.00001),\n",
    "            #name = f\"1BiLSTM_layer_{i+1}\",\n",
    "            units = 97,\n",
    "            activation = 'relu',\n",
    "            #recurrent_activation = 'relu',\n",
    "            use_bias = False,\n",
    "            unit_forget_bias = True,\n",
    "            dropout=0.1,\n",
    "            recurrent_dropout = 0.1)))\n",
    "        model.add(LSTM(\n",
    "            units=64,\n",
    "            activation = 'relu',\n",
    "            #recurrent_activation = 'relu',\n",
    "            return_sequences=False, \n",
    "            dropout=0.1,\n",
    "            recurrent_dropout=0.1,\n",
    "            input_shape=(inputs.shape[1], inputs.shape[2])))\n",
    "        #model.add(Bidirectional(LSTM(inputs.shape[-1], activation='relu', return_sequences = False, dropout=0, input_shape=(inputs.shape[1], inputs.shape[2]))))\n",
    "        model.add(RepeatVector(inputs.shape[1]))   #TUNE THIS (LAYERS) WHEN ADDING SATELLITE AND MODELING DATA\n",
    "        model.add(LSTM(\n",
    "            units=128, \n",
    "            activation = 'relu',\n",
    "            #recurrent_activation='relu',\n",
    "            return_sequences=True,\n",
    "            dropout=0.1,\n",
    "            recurrent_dropout=0.1,\n",
    "            input_shape=(inputs.shape[1], inputs.shape[2])))\n",
    "        #model.add(Bidirectional(LSTM(inputs.shape[-1], activation='relu', return_sequences = True, dropout=0, input_shape=(inputs.shape[1], inputs.shape[2])))) \n",
    "        model.add(Bidirectional(LSTM(\n",
    "            input_shape=(inputs.shape[1], inputs.shape[2]),\n",
    "            return_sequences = True, \n",
    "            #kernel_initializer = tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.00001),\n",
    "            #name = f\"2BiLSTM_layer_{i+1}\",\n",
    "            units = 128,\n",
    "            activation = 'relu',\n",
    "            #recurrent_activation = 'sigmoid',\n",
    "            use_bias = True,\n",
    "            unit_forget_bias = False,\n",
    "            dropout=0.1,\n",
    "            recurrent_dropout = 0.1)))\n",
    "        model.add(Conv1DTranspose(\n",
    "           filters=88, \n",
    "           kernel_size=9, \n",
    "           activation = 'swish',\n",
    "           padding='same', \n",
    "           input_shape=(inputs.shape[1], inputs.shape[2])))\n",
    "        model.add(TimeDistributed(Dense(trainyscaltref.shape[1])))\n",
    "        #model.add(TimeDistributed(Dense(trainyscch4ref.shape[1])))\n",
    "        #model.add(Dense(trainXscaltref.shape[1]))\n",
    "        #model.add(Dense(trainXscch4ref.shape[1]))\n",
    "        metrics=['mean_squared_error', 'mean_absolute_error', RSquare().build(input_shape = (trainyscaltref.shape[1],))]\n",
    "        #metrics=['mean_squared_error', 'mean_absolute_error', RSquare().build(input_shape = (trainyscch4ref.shape[1],))]\n",
    "        loss_function = 'mean_squared_error'\n",
    "        model.compile(optimizer = tf.keras.optimizers.legacy.RMSprop(learning_rate = 0.0001, **{\"clipvalue\" : 1000}),loss = loss_function, metrics = metrics)\n",
    "        return model\n",
    "    \n",
    "    def fit(self, hp, model, *args, **kwargs):\n",
    "        return model.fit(*args, batch_size = 256, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ab4844-e97d-412a-b72f-75decc52fc3e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#HP CH4\n",
    "#BEST MODEL: TRIAL22\n",
    "# Trial 26 Complete [01h 35m 54s]\n",
    "# val_loss: 0.4272751808166504\n",
    "\n",
    "# Best val_loss So Far: 0.004293057601898909\n",
    "# Total elapsed time: 14h 38m 44s\n",
    "\n",
    "# Search: Running Trial #27\n",
    "\n",
    "# Value             |Best Value So Far |Hyperparameter\n",
    "# 0.001             |0.0001            |learning_rate\n",
    "# 64                |64                |batch_size\n",
    "# 320               |128               |units\n",
    "# 64                |71                |conv1d_filters\n",
    "# 9                 |9                 |conv1d_kernel_size\n",
    "# relu              |relu              |conv1d_activation\n",
    "# 116               |92                |bilstm_units\n",
    "# relu              |relu              |bilstm_activation\n",
    "# relu              |relu              |bilstm_rec_activation\n",
    "# 0.4               |0.3               |bilstm_dropout\n",
    "# 0.4               |0.4               |bilstm_rec_dropout\n",
    "# False             |False             |bilstm_use_bias\n",
    "# True              |True              |bilstm_forgot_bias\n",
    "# 100               |128               |lstm_units\n",
    "# linear            |tanh              |lstm_activation\n",
    "# swish             |elu               |lstm_rec_activation\n",
    "# 0.4               |0.4               |lstm_dropout\n",
    "# 0.4               |0.4               |lstm_rec_dropout\n",
    "# 128               |128               |lstm2_units\n",
    "# relu              |softmax           |lstm2_activation\n",
    "# swish             |swish             |lstm2_rec_activation\n",
    "# 0.4               |0.4               |lstm2_dropout\n",
    "# 0.4               |0.1               |lstm2_rec_dropout\n",
    "# 123               |78                |bilstm2_units\n",
    "# relu              |relu              |bilstm2_activation\n",
    "# swish             |relu              |bilstm2_rec_activation\n",
    "# 0.1               |0.1               |bilstm2_dropout\n",
    "# 0.1               |0.1               |bilstm2_rec_dropout\n",
    "# False             |False             |bilstm2_use_bias\n",
    "# True              |False             |bilstm2_forgot_bias\n",
    "# 128               |117               |conv1d2_filters\n",
    "# 3                 |3                 |conv1d2_kernel_size\n",
    "# sigmoid           |swish             |conv1d2_activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecfc860f-184c-496f-84bc-71cd914a95aa",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#CH4\n",
    "class geocryoai2(HyperModel):\n",
    "    def build(self, hp):\n",
    "        #backend.clear_session()\n",
    "        #inputs = keras.Input(shape=(X_train_reframed.shape[1], X_train_reframed.shape[2]))\n",
    "        #inputs = keras.Input(shape=(trainXscch4ref.shape[1], trainXscch4ref.shape[2]))        \n",
    "        model = tf.keras.Sequential()\n",
    "        #for i in range(n_layers):\n",
    "        model.add(Conv1D(\n",
    "            filters=71, \n",
    "            kernel_size=9, \n",
    "            activation = 'swish',\n",
    "            padding='same', \n",
    "            input_shape=(inputs.shape[1], inputs.shape[2])))\n",
    "        # model.add(MaxPool1D(pool_size=1))\n",
    "        #for i in range(hp['n_layers']): #TUNE THIS (LAYERS) WHEN ADDING SATELLITE AND MODELING DATA\n",
    "        model.add(Bidirectional(LSTM(\n",
    "            input_shape=(inputs.shape[1], inputs.shape[2]),\n",
    "            return_sequences = True, \n",
    "            #kernel_initializer = tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.00001),\n",
    "            #name = f\"1BiLSTM_layer_{i+1}\",\n",
    "            units = 92,\n",
    "            activation = 'relu',\n",
    "            #recurrent_activation = 'linear',\n",
    "            use_bias = False,\n",
    "            unit_forget_bias = True,\n",
    "            dropout=0.1,\n",
    "            recurrent_dropout = 0.1)))\n",
    "        #for i in range(hp['n_layers']):\n",
    "        model.add(LSTM(\n",
    "            units=128,\n",
    "            activation = 'tanh',\n",
    "            #recurrent_activation = 'softmax',\n",
    "            return_sequences=False, \n",
    "            dropout=0.1,\n",
    "            recurrent_dropout=0.1,\n",
    "            input_shape=(inputs.shape[1], inputs.shape[2])))\n",
    "        #model.add(Bidirectional(LSTM(inputs.shape[-1], activation='relu', return_sequences = False, dropout=0, input_shape=(inputs.shape[1], inputs.shape[2]))))\n",
    "        model.add(RepeatVector(inputs.shape[1]))   #TUNE THIS (LAYERS) WHEN ADDING SATELLITE AND MODELING DATA\n",
    "        model.add(LSTM(\n",
    "            units=128, \n",
    "            activation = 'softmax',\n",
    "            #recurrent_activation='elu',\n",
    "            return_sequences=True,\n",
    "            dropout=0.1,\n",
    "            recurrent_dropout=0.1,\n",
    "            input_shape=(inputs.shape[1], inputs.shape[2])))\n",
    "        #model.add(Bidirectional(LSTM(inputs.shape[-1], activation='relu', return_sequences = True, dropout=0, input_shape=(inputs.shape[1], inputs.shape[2])))) \n",
    "        model.add(Bidirectional(LSTM(\n",
    "            input_shape=(inputs.shape[1], inputs.shape[2]),\n",
    "            return_sequences = True, \n",
    "            #kernel_initializer = tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.00001),\n",
    "            #name = f\"2BiLSTM_layer_{i+1}\",\n",
    "            units = 78,\n",
    "            activation = 'relu',\n",
    "            #recurrent_activation = 'relu',\n",
    "            use_bias = False,\n",
    "            unit_forget_bias = False,\n",
    "            dropout=0.1,\n",
    "            recurrent_dropout = 0.1)))\n",
    "        model.add(Conv1DTranspose(\n",
    "           filters=117, \n",
    "           kernel_size=3, \n",
    "           activation = 'swish',\n",
    "           padding='same', \n",
    "           input_shape=(inputs.shape[1], inputs.shape[2])))\n",
    "        #model.add(TimeDistributed(Dense(trainyscaltref.shape[1])))\n",
    "        model.add(TimeDistributed(Dense(trainyscch4ref.shape[1])))\n",
    "        #model.add(TimeDistributed(Dense(trainyscco2ref.shape[1])))\n",
    "        #model.add(Dense(trainXscaltref.shape[1]))\n",
    "        model.add(Dense(trainXscch4ref.shape[1]))\n",
    "        #model.add(Dense(trainXscco2ref.shape[1]))\n",
    "        #metrics=['mean_squared_error', 'mean_absolute_error', RSquare().build(input_shape = (trainyscaltref.shape[1],))]\n",
    "        metrics=['mean_squared_error', 'mean_absolute_error', RSquare().build(input_shape = (trainyscch4ref.shape[1],))]\n",
    "        #metrics=['mean_squared_error', 'mean_absolute_error', RSquare().build(input_shape = (trainyscco2ref.shape[1],))]\n",
    "        loss_function = 'mean_squared_error'\n",
    "        model.compile(optimizer = tf.keras.optimizers.legacy.RMSprop(learning_rate = 0.001, **{\"clipvalue\" : 1000}),loss = loss_function, metrics = metrics)\n",
    "        return model\n",
    "    \n",
    "    def fit(self, hp, model, *args, **kwargs):\n",
    "        return model.fit(*args, batch_size = 256, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a90fbc-1ddd-4c41-8650-12d0e833b508",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#HP CO2\n",
    "#BEST MODEL: TRIAL22\n",
    "# Trial 31 Complete [01h 26m 33s]\n",
    "# val_loss: 0.09103024005889893\n",
    "\n",
    "# Best val_loss So Far: 0.014815938659012318\n",
    "# Total elapsed time: 13h 56m 11s\n",
    "\n",
    "# Search: Running Trial #32\n",
    "\n",
    "# Value             |Best Value So Far |Hyperparameter\n",
    "# 0.0001            |0.0001            |learning_rate\n",
    "# 64                |64                |batch_size\n",
    "# 128               |192               |units\n",
    "# 128               |128               |conv1d_filters\n",
    "# 9                 |6                 |conv1d_kernel_size\n",
    "# relu              |elu               |conv1d_activation\n",
    "# 111               |100               |bilstm_units\n",
    "# relu              |relu              |bilstm_activation\n",
    "# relu              |relu              |bilstm_rec_activation\n",
    "# 0.1               |0.1               |bilstm_dropout\n",
    "# 0.3               |0.3               |bilstm_rec_dropout\n",
    "# False             |True              |bilstm_use_bias\n",
    "# True              |True              |bilstm_forgot_bias\n",
    "# 64                |64                |lstm_units\n",
    "# sigmoid           |tanh              |lstm_activation\n",
    "# sigmoid           |tanh              |lstm_rec_activation\n",
    "# 0.1               |0.1               |lstm_dropout\n",
    "# 0.4               |0.4               |lstm_rec_dropout\n",
    "# 80                |83                |lstm2_units\n",
    "# relu              |relu              |lstm2_activation\n",
    "# relu              |relu              |lstm2_rec_activation\n",
    "# 0.1               |0.1               |lstm2_dropout\n",
    "# 0.3               |0.3               |lstm2_rec_dropout\n",
    "# 64                |64                |bilstm2_units\n",
    "# relu              |relu              |bilstm2_activation\n",
    "# relu              |sigmoid           |bilstm2_rec_activation\n",
    "# 0.4               |0.3               |bilstm2_dropout\n",
    "# 0.4               |0.3               |bilstm2_rec_dropout\n",
    "# False             |False             |bilstm2_use_bias\n",
    "# False             |True              |bilstm2_forgot_bias\n",
    "# 128               |95                |conv1d2_filters\n",
    "# 6                 |6                 |conv1d2_kernel_size\n",
    "# swish             |swish             |conv1d2_activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f18b3d-90c6-48a2-aefd-d1773eb901af",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#CO2\n",
    "class geocryoai2(HyperModel):\n",
    "    def build(self, hp):\n",
    "        #backend.clear_session()\n",
    "        #inputs = keras.Input(shape=(X_train_reframed.shape[1], X_train_reframed.shape[2]))\n",
    "        #inputs = keras.Input(shape=(trainXscch4ref.shape[1], trainXscch4ref.shape[2]))        \n",
    "        model = tf.keras.Sequential()\n",
    "        #for i in range(n_layers):\n",
    "        model.add(Conv1D(\n",
    "            filters=128, \n",
    "            kernel_size=6, \n",
    "            activation = 'elu',\n",
    "            padding='same', \n",
    "            input_shape=(inputs.shape[1], inputs.shape[2])))\n",
    "        # model.add(MaxPool1D(pool_size=1))\n",
    "        #for i in range(hp['n_layers']): #TUNE THIS (LAYERS) WHEN ADDING SATELLITE AND MODELING DATA\n",
    "        model.add(Bidirectional(LSTM(\n",
    "            input_shape=(inputs.shape[1], inputs.shape[2]),\n",
    "            return_sequences = True, \n",
    "            #kernel_initializer = tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.00001),\n",
    "            #name = f\"1BiLSTM_layer_{i+1}\",\n",
    "            units = 120,\n",
    "            activation = 'relu',\n",
    "            #recurrent_activation = 'relu',\n",
    "            use_bias = True,\n",
    "            unit_forget_bias = True,\n",
    "            dropout=0.1,\n",
    "            recurrent_dropout = 0.1)))\n",
    "        #for i in range(hp['n_layers']):\n",
    "        model.add(LSTM(\n",
    "            units=64,\n",
    "            activation = 'tanh',\n",
    "            #recurrent_activation = 'tanh',\n",
    "            return_sequences=False, \n",
    "            dropout=0.1,\n",
    "            recurrent_dropout=0.1,\n",
    "            input_shape=(inputs.shape[1], inputs.shape[2])))\n",
    "        #model.add(Bidirectional(LSTM(inputs.shape[-1], activation='relu', return_sequences = False, dropout=0, input_shape=(inputs.shape[1], inputs.shape[2]))))\n",
    "        model.add(RepeatVector(inputs.shape[1]))   #TUNE THIS (LAYERS) WHEN ADDING SATELLITE AND MODELING DATA\n",
    "        model.add(LSTM(\n",
    "            units=83, \n",
    "            activation = 'relu',\n",
    "            #recurrent_activation='relu',\n",
    "            return_sequences=True,\n",
    "            dropout=0.1,\n",
    "            recurrent_dropout=0.1,\n",
    "            input_shape=(inputs.shape[1], inputs.shape[2])))\n",
    "        #model.add(Bidirectional(LSTM(inputs.shape[-1], activation='relu', return_sequences = True, dropout=0, input_shape=(inputs.shape[1], inputs.shape[2])))) \n",
    "        model.add(Bidirectional(LSTM(\n",
    "            input_shape=(inputs.shape[1], inputs.shape[2]),\n",
    "            return_sequences = True, \n",
    "            #kernel_initializer = tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.00001),\n",
    "            #name = f\"2BiLSTM_layer_{i+1}\",\n",
    "            units = 64,\n",
    "            activation = 'relu',\n",
    "            #recurrent_activation = 'sigmoid',\n",
    "            use_bias = False,\n",
    "            unit_forget_bias = True,\n",
    "            dropout=0.1,\n",
    "            recurrent_dropout = 0.1)))\n",
    "        model.add(Conv1DTranspose(\n",
    "           filters=95, \n",
    "           kernel_size=6, \n",
    "           activation = 'swish',\n",
    "           padding='same', \n",
    "           input_shape=(inputs.shape[1], inputs.shape[2])))\n",
    "        #model.add(TimeDistributed(Dense(trainyscaltref.shape[1])))\n",
    "        #model.add(TimeDistributed(Dense(trainyscch4ref.shape[1])))\n",
    "        model.add(TimeDistributed(Dense(trainyscco2ref.shape[1])))\n",
    "        #model.add(Dense(trainXscaltref.shape[1]))\n",
    "        #model.add(Dense(trainXscch4ref.shape[1]))\n",
    "        model.add(Dense(trainXscco2ref.shape[1]))\n",
    "        #metrics=['mean_squared_error', 'mean_absolute_error', RSquare().build(input_shape = (trainyscaltref.shape[1],))]\n",
    "        #metrics=['mean_squared_error', 'mean_absolute_error', RSquare().build(input_shape = (trainyscch4ref.shape[1],))]\n",
    "        metrics=['mean_squared_error', 'mean_absolute_error', RSquare().build(input_shape = (trainyscco2ref.shape[1],))]\n",
    "        loss_function = 'mean_squared_error'\n",
    "        model.compile(optimizer = tf.keras.optimizers.legacy.RMSprop(learning_rate = 0.0001, **{\"clipvalue\" : 1000}),loss = loss_function, metrics = metrics)\n",
    "        return model\n",
    "    \n",
    "    def fit(self, hp, model, *args, **kwargs):\n",
    "        return model.fit(*args, batch_size = 64, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc2e7cc-d5d7-4354-a525-6be4f9149026",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainXscaltref.shape, trainyscaltref.shape\n",
    "#trainXscch4ref.shape, trainyscch4ref.shape\n",
    "#trainXscco2ref.shape, trainyscco2ref.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0d2dbd-442b-4044-a860-ef12f94e6f29",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model2=geocryoai2.build(trainXscaltref, hp)\n",
    "#model3=geocryoai2.build(trainXscch4ref, hp)\n",
    "#model4=geocryoai2.build(trainXscco2ref, hp)\n",
    "#model3b=geocryoai2.build(trainXscch4ref, hp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7349c7ad-809f-401a-aab8-dbbbb8a7c641",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455adddb-b2c5-44bd-8dd7-1b8059cc1aeb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ba7b1d-8a4c-4e3a-9836-0b4a860ad43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model4.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2acab90-7412-4398-ad94-050f8aab97e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model3b.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052ce43c-0c0a-4196-bff7-228dfc3fa6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#geocryoai(X_train)\n",
    "#img_file = '/Users/bradleygay/Downloads/model_arch.jpeg'\n",
    "img_file = '/Users/bradleygay/Downloads/GeoCryoAI_Arch_071523_insituALT.png'\n",
    "tf.keras.utils.plot_model(model2, to_file=img_file, show_shapes=True, show_layer_names=True, dpi=1000);\n",
    "#img_file = '/Users/bradleygay/Downloads/GeoCryoAI_Arch_071423_insituCH4.png'\n",
    "#tf.keras.utils.plot_model(model3, to_file=img_file, show_shapes=True, show_layer_names=True, dpi=1000);\n",
    "#img_file = '/Users/bradleygay/Downloads/GeoCryoAI_Arch_071423_insituCO2.png'\n",
    "#tf.keras.utils.plot_model(model4, to_file=img_file, show_shapes=True, show_layer_names=True, dpi=1000);\n",
    "#img_file = '/Users/bradleygay/Downloads/GeoCryoAI_Arch_071423_insituCH4model3b.png'\n",
    "#tf.keras.utils.plot_model(model3b, to_file=img_file, show_shapes=True, show_layer_names=True, dpi=1000);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190b6be6-2daf-4d38-b279-173d11bf1a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "root_logdir = os.path.join(os.curdir, 'logs')\n",
    "\n",
    "def get_run_logdir():\n",
    "    import time\n",
    "    run_id = time.strftime('run_%Y_%m_%d-%H_%M_%S')\n",
    "    return os.path.join(root_logdir, run_id)\n",
    "\n",
    "run_logdir = get_run_logdir()\n",
    "\n",
    "log_dir = \"logs/fit/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77ce72c-8161-4a03-b051-665b299b66a0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#ALT\n",
    "# add early stopping criteria t/Volumes/op training if validation score does not improve - cuts down on computational load/speed.\n",
    "filepath=\"weights_geocryoai2.alt_071523.hdf5\"\n",
    "#filepath=\"weights_geocryoai2.best_ch4_071423.hdf5\"\n",
    "#filepath=\"weights_geocryoai2.best__co2_071423.hdf5\"\n",
    "#filepath=\"weights_geocryoai2.best_ch4_model3b_071423.hdf5\"\n",
    "tensorboard_cb = keras.callbacks.TensorBoard('/tmp/tb_logs')\n",
    "checkpoint = keras.callbacks.ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "early_stopping = EarlyStopping(monitor='val_loss', verbose = 1, patience = 10, min_delta = 1e-4, restore_best_weights = True)\n",
    "# fit network\n",
    "start_time = time.time()\n",
    "history2b = model2.fit(trainXscaltref, #trainXscaltref #trainXscch4ref\n",
    "#history3 = model3.fit(trainXscch4ref, #trainXscaltref #trainXscco2ref\n",
    "#history4 = model4.fit(trainXscco2ref, #trainXscaltref #trainXscco2ref\n",
    "#history5 = model3b.fit(trainXscch4ref, #trainXscaltref #trainXscco2ref\n",
    "#                   trainyscch4ref, #trainyscaltref #trainXscco2ref\n",
    "                    trainyscaltref,  \n",
    "                    epochs=10, \n",
    "                    batch_size=256,#128,#512, \n",
    "                    validation_data=(validXscaltref,validyscaltref),\n",
    "                    #validation_data=(validXscch4ref,validyscch4ref), #(validXscaltref,validyscaltref) #(validXscco2ref,validyscco2ref)\n",
    "                    steps_per_epoch = None,\n",
    "                    shuffle=False, \n",
    "                    callbacks=[early_stopping, TerminateOnNaN(),  keras.callbacks.TensorBoard(\"/tmp/tb_logs\")],\n",
    "                    use_multiprocessing = True)\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "print(\"\\nThe first network took {} s to complete training.\".format(round(elapsed_time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd3dc43-0bb1-4161-8ce8-298432df4082",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#CH4\n",
    "# add early stopping criteria t/Volumes/op training if validation score does not improve - cuts down on computational load/speed.\n",
    "#filepath=\"weights_geocryoai2.best_071123.hdf5\"\n",
    "filepath=\"weights_geocryoai2.best_071223_ch4.hdf5\"\n",
    "#filepath=\"weights_geocryoai2.best_071223_co2.hdf5\"\n",
    "tensorboard_cb = keras.callbacks.TensorBoard('/tmp/tb_logs')\n",
    "checkpoint = keras.callbacks.ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "early_stopping = EarlyStopping(monitor='val_loss', verbose = 1, patience = 10, min_delta = 1e-4, restore_best_weights = True)\n",
    "# fit network\n",
    "start_time = time.time()\n",
    "#history2 = model2.fit(trainXscaltref, #trainXscaltref #trainXscch4ref\n",
    "history3 = model3.fit(trainXscch4ref, #trainXscaltref #trainXscco2ref\n",
    "                    trainyscch4ref, #trainyscaltref #trainXscco2ref\n",
    "                    epochs=10, \n",
    "                    batch_size=256,#512, \n",
    "                    validation_data=(validXscch4ref,validyscch4ref), #(validXscaltref,validyscaltref) #(validXscco2ref,validyscco2ref)\n",
    "                    steps_per_epoch = None,\n",
    "                    shuffle=False, \n",
    "                    callbacks=[early_stopping, TerminateOnNaN(),  keras.callbacks.TensorBoard(\"/tmp/tb_logs\")],\n",
    "                    use_multiprocessing = True)\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "print(\"\\nThe first network took {} s to complete training.\".format(round(elapsed_time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc0c687-1bc7-4d8f-98a8-70a8d1d47725",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#CO2\n",
    "# add early stopping criteria t/Volumes/op training if validation score does not improve - cuts down on computational load/speed.\n",
    "#filepath=\"weights_geocryoai2.best_071123.hdf5\"\n",
    "#filepath=\"weights_geocryoai2.best_071223_ch4.hdf5\"\n",
    "filepath=\"weights_geocryoai2.best_071223_co2.hdf5\"\n",
    "tensorboard_cb = keras.callbacks.TensorBoard('/tmp/tb_logs')\n",
    "checkpoint = keras.callbacks.ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "early_stopping = EarlyStopping(monitor='val_loss', verbose = 1, patience = 10, min_delta = 1e-4, restore_best_weights = True)\n",
    "# fit network\n",
    "start_time = time.time()\n",
    "#history2 = model2.fit(trainXscaltref, #trainXscaltref #trainXscch4ref\n",
    "#history3 = model3.fit(trainXscch4ref, #trainXscaltref #trainXscco2ref\n",
    "history4 = model4.fit(trainXscco2ref, #trainXscaltref #trainXscco2ref\n",
    "                    trainyscco2ref, #trainyscaltref #trainXscco2ref\n",
    "                    epochs=10, \n",
    "                    batch_size=256,#512, \n",
    "                    validation_data=(validXscco2ref,validyscco2ref), #(validXscaltref,validyscaltref) #(validXscco2ref,validyscco2ref)\n",
    "                    steps_per_epoch = None,\n",
    "                    shuffle=False, \n",
    "                    callbacks=[early_stopping, TerminateOnNaN(),  keras.callbacks.TensorBoard(\"/tmp/tb_logs\")],\n",
    "                    use_multiprocessing = True)\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "print(\"\\nThe first network took {} s to complete training.\".format(round(elapsed_time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e44128-36b3-4177-b59e-e9335efe7fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# serialize model to JSON\n",
    "model_json = model2b.to_json()\n",
    "#model_json = model3.to_json()\n",
    "#model_json = model4.to_json()\n",
    "#model_json = model3b.to_json()\n",
    "with open(\"model2_071623_insituALT.json\", \"w\") as json_file:\n",
    "#with open(\"model3_071523_insituCH4_2.json\", \"w\") as json_file:\n",
    "#with open(\"model4_071523_insituCO2.json\", \"w\") as json_file:\n",
    "#with open(\"model3_071523_insituCH4_model3b.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model2.save_weights(\"model2_071623_insituALT.h5\")\n",
    "#model3.save_weights(\"model3_071523_insituCH4.h5\")\n",
    "#model4.save_weights(\"model4_071523_insituCO2.h5\")\n",
    "#model3b.save_weights(\"model3_071523_insituCH4_model3b.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36185310-205d-4a69-a71a-a2df5cf6ddec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# convert the history.history dict to a pandas DataFrame:     \n",
    "hist_df = pd.DataFrame(history2b.history)\n",
    "#hist_df = pd.DataFrame(history3.history)\n",
    "#hist_df = pd.DataFrame(history4.history)\n",
    "#hist_df = pd.DataFrame(history5.history)\n",
    "\n",
    "# save to json:  \n",
    "hist_json_file = 'historyALT-071623.json' \n",
    "#hist_json_file = 'historyCH4-071523.json' \n",
    "#hist_json_file = 'historyCO2-071523.json' \n",
    "#hist_json_file = 'historyCH4-071523_model3b.json' \n",
    "with open(hist_json_file, mode='w') as f:\n",
    "    hist_df.to_json(f)\n",
    "\n",
    "# or save to csv: \n",
    "hist_csv_file = 'historyALT-071623.csv'\n",
    "#hist_csv_file = 'historyCH4-071523.csv'\n",
    "#hist_csv_file = 'historyCO2-071523.csv'\n",
    "#hist_csv_file = 'historyCH4-071523_model3b.csv'\n",
    "with open(hist_csv_file, mode='w') as f:\n",
    "    hist_df.to_csv(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36289eb2-0f64-401b-89c9-25c5425cc3d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('trainHistoryALT-071623', 'wb') as file_pi:\n",
    "#with open('trainHistoryCH4-071523', 'wb') as file_pi:\n",
    "#with open('trainHistoryCO2-071523', 'wb') as file_pi:\n",
    "#with open('trainHistoryCH4-071523_model3b', 'wb') as file_pi:\n",
    "    pickle.dump(history2b.history, file_pi)\n",
    "    #pickle.dump(history3.history, file_pi)\n",
    "    #pickle.dump(history4.history, file_pi)\n",
    "    #pickle.dump(history5.history, file_pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188b1ae2-8133-48c2-b050-d0726c7abd4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "testXscaltref.shape\n",
    "#testXscch4ref.shape\n",
    "#testXscco2ref.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5d2465-7726-42b5-8651-38b8b568e1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#testXscaltrefres=testXscaltref.reshape(215136,1,456)\n",
    "#testXscch4refres=testXscch4ref.reshape(161749,1,456)\n",
    "#testXscco2refres=testXscco2ref.reshape(161749,1,456)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfcc2fa2-6011-473b-aa6c-a1a562786559",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "score_2b = model2.evaluate(testXscaltref, testyscaltref, verbose = 1) \n",
    "#score_3 = model3.evaluate(testXscch4ref, testyscch4ref, verbose = 1)\n",
    "#score_4 = model4.evaluate(testXscco2ref, testyscco2ref, verbose = 1)\n",
    "#score_3b = model3b.evaluate(testXscch4ref, testyscch4ref, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a5f9c7-6488-4a2c-9d79-90ace719862e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('Test MAE:', score_2[1])\n",
    "print('Test MSE:', score_2[2])\n",
    "print('Test RMSE:', np.sqrt(score_2[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776a2da1-ce71-4e08-b89f-e1e84993989c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Test MAE:', score_2b[1])\n",
    "print('Test MSE:', score_2b[2])\n",
    "print('Test RMSE:', np.sqrt(score_2b[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d46a9614-6dbf-498c-894b-634d8d1ca102",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Test MAE:', score_3[1])\n",
    "print('Test MSE:', score_3[2])\n",
    "print('Test RMSE:', np.sqrt(score_3[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdeb606f-5fc9-4a6f-8534-67d4188ca07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Test MAE:', score_4[1])\n",
    "print('Test MSE:', score_4[2])\n",
    "print('Test RMSE:', np.sqrt(score_4[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ab1819-67e8-43d2-99f4-917da0bede2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Test MAE:', score_3b[1])\n",
    "print('Test MSE:', score_3b[2])\n",
    "print('Test RMSE:', np.sqrt(score_3b[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc66402-fb30-4cf0-bd1f-583a96b63f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history2b.history['loss'])\n",
    "#plt.plot(history3.history['loss'])\n",
    "#plt.plot(history4.history['loss'])\n",
    "#plt.plot(history5.history['loss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee262634-6583-4e97-916b-1594297f6d08",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8cddee0-3e1f-4e32-acaa-0205661923ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import json\n",
    "#althist_json_file = json.load(open('trainHistoryDictALT_experimental.json', 'r'))\n",
    "with open('/Users/bradleygay/code/historyALT.json.json', 'rb') as file:\n",
    "    althistory=pickle.load(file)\n",
    "with open('/Users/bradleygay/code/trainHistoryDictCH4_experimental', 'rb') as file2:\n",
    "    ch4history=pickle.load(file2)\n",
    "with open('/Users/bradleygay/code/trainHistoryDictCO2_experimental', 'rb') as file3:\n",
    "    co2history=pickle.load(file3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a200dd9-db18-4e01-a3b6-9fefa5e84be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Models\n",
    "from keras.models import model_from_json\n",
    "from keras.models import load_model\n",
    "#ALT\n",
    "# load json file and model\n",
    "alt_json_file = open('model2_070923_insituALT.json', 'r')\n",
    "alt_loaded_model_json = alt_json_file.read()\n",
    "alt_json_file.close()\n",
    "alt_loaded_model_json = model_from_json(alt_loaded_model_json)\n",
    "# load weights for new model\n",
    "alt_loaded_model_json.load_weights(\"model2_070923_insituALT.h5\")\n",
    "print(\"Loaded model from disk\")\n",
    "# save and reload\n",
    "alt_loaded_model_json.save('model2_070923_insituALT.hdf5')\n",
    "alt_loaded_model_json=load_model('model2_070923_insituALT.hdf5')\n",
    "\n",
    "#CH4\n",
    "# load json file and model\n",
    "ch4_json_file = open('model2_070923_insituCH4.json', 'r')\n",
    "ch4_loaded_model_json = ch4_json_file.read()\n",
    "ch4_json_file.close()\n",
    "ch4_loaded_model_json = model_from_json(ch4_loaded_model_json)\n",
    "# load weights for new model\n",
    "ch4_loaded_model_json.load_weights(\"model2_070923_insituCH4.h5\")\n",
    "print(\"Loaded model from disk\")\n",
    "# save and reload\n",
    "ch4_loaded_model_json.save('model2_070923_insituCH4.hdf5')\n",
    "ch4_loaded_model_json=load_model('model2_070923_insituCH4.hdf5')\n",
    "\n",
    "#CO2\n",
    "co2_json_file = open('model2_070923_insituCO2.json', 'r')\n",
    "co2_loaded_model_json = co2_json_file.read()\n",
    "co2_json_file.close()\n",
    "co2_loaded_model_json = model_from_json(co2_loaded_model_json)\n",
    "# load weights for new model\n",
    "co2_loaded_model_json.load_weights(\"model2_070923_insituCO2.h5\")\n",
    "print(\"Loaded model from disk\")\n",
    "# save and reload\n",
    "co2_loaded_model_json.save('model2_070923_insituCO2.hdf5')\n",
    "co2_loaded_model_json=load_model('model2_070923_insituCO2.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1e1548-2e8d-49e5-b556-216394c7a198",
   "metadata": {},
   "outputs": [],
   "source": [
    "#FROM OTHER FILE\n",
    "########################################################\n",
    "########################################################\n",
    "########################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ef6d83-a59f-4b0e-8d67-9416e345273e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(10,6), dpi=1000)\n",
    "# l1=ax.plot(history2.history['loss'], color='dodgerblue', linestyle='solid', label='ALT Training Loss (cm)')\n",
    "# l2=ax.plot(history3.history['loss'], color='magenta', linestyle='solid', label='CH4 Flux Training Loss (cm)')\n",
    "# l3=ax.plot(history4.history['loss'], color='springgreen', linestyle='solid', label='CO2 Flux Training Loss (nmolCH4m-2s-1)')\n",
    "l1=ax.plot(history2.history['val_loss'], color='dodgerblue', linestyle='solid', label='ALT Validation Loss (cm)')\n",
    "l2=ax.plot(history3.history['val_loss'], color='magenta', linestyle='solid', label='CH4 Flux Validation Loss (nmolCH4m-2s-1)')\n",
    "l3=ax.plot(history4.history['val_loss'], color='springgreen', linestyle='solid', label='CO2 Flux Validation Loss (molCO2m-2s-1)')\n",
    "#ax2=ax.twinx();\n",
    "#ln4=ax2.plot(validPredict, color='coral', linestyle='dotted')\n",
    "\n",
    "lines = ln1 + ln2 + ln3 #+ ln4 #+ ln5# ln4 + ln5 + ln6# + ln7 + ln8\n",
    "labs = [line.get_label() for line in lines];\n",
    "ax.legend(lines, labs, loc='best', fontsize=8)#'lower left', fontsize=8)\n",
    "\n",
    "ax.grid(linewidth=0.3);\n",
    "ax.set_xlabel('Epochs', labelpad=8, fontsize=16);\n",
    "ax.set_ylabel('Training Loss', labelpad=8, fontsize=16)\n",
    "#ax.set(xticklabels=[])  # remove the tick labels\n",
    "ax.tick_params(left=False)  # remove the ticks\n",
    "#plt.ylabel('Active Layer Thickness (cm)')\n",
    "plt.title('GeoCryoAI In Situ Module | Bidirectional Conv1DLSTM Autoencoder Loss Functions \\n ALT, CH4 Flux, and CO2 Flux Simulations (1969-2022)', pad=10)\n",
    "\n",
    "#plt.xlabel('Year')\n",
    "#plt.axis([0, 6, 0, 60])\n",
    "#plt.legend(loc='best')\n",
    "#plt.show()\n",
    "#plt.savefig('/Users/bradleygay/Downloads/bilstmae_insitu_CO2_loss.png',dpi=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4431ad-c2dd-4a37-b53e-53ec7d535e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reframed_alt.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007d07b2-fe70-4a50-9109-695dd203df98",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots(figsize=(10,6), dpi=1000);\n",
    "lns1=ax.plot(history2.history['mean_squared_error'], color='dodgerblue', linestyle='solid', label='Training, ALT (cm)');\n",
    "ax2=ax.twinx();\n",
    "lns2=ax2.plot(history2.history['val_mean_squared_error'], color='tomato', linestyle='solid', label='Validation, ALT (cm)');\n",
    "\n",
    "lns = lns1+lns2#+lns3+lns4;\n",
    "labs = [l.get_label() for l in lns];\n",
    "ax2.legend(lns, labs, loc='best', fontsize=12);\n",
    "\n",
    "ax.grid(linewidth=0.3);\n",
    "ax2.grid(linewidth=0.3);\n",
    "ax.set_xlabel('Full Iterations (epochs)', labelpad=12, fontsize=10);\n",
    "ax.set_ylabel('Training MSE, Scaled ALT (cm)', labelpad=12, fontsize=10);\n",
    "ax.tick_params(axis='y', labelcolor='dodgerblue')\n",
    "ax2.set_ylabel('Validation MSE, Scaled ALT (cm)', labelpad=12, fontsize=10)\n",
    "ax2.tick_params(axis='y', labelcolor='tomato')\n",
    "ax.tick_params(left=False)  # remove the ticks\n",
    "ax2.tick_params(right=False)  # remove the ticks\n",
    "plt.title('GeoCryoAI Modeling, Cost Function and Validation Loss of ALT | Alaska [1969-2022] \\n Number of Thaw Depth Samples/Replicates: 2.441M', pad=15, fontsize=14);\n",
    "ax.grid(linewidth=0.3);\n",
    "plt.tight_layout()\n",
    "plt.savefig('ALTstats_1969-2022_071323.png', dpi=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46db3e50-c17a-4e3f-a3f6-9dbe1bfa124c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots(figsize=(10,6), dpi=1000);\n",
    "lns1=ax.plot(history3.history['mean_squared_error'], color='magenta', linestyle='solid', label='Training, CH4 Flux (nmolCH4m-2s-1)');\n",
    "ax2=ax.twinx();\n",
    "lns2=ax2.plot(history3.history['val_mean_squared_error'], color='slateblue', linestyle='solid', label='Validation, CH4 Flux (nmolCH4m-2s-1)');\n",
    "\n",
    "lns = lns1+lns2#+lns3+lns4;\n",
    "labs = [l.get_label() for l in lns];\n",
    "ax2.legend(lns, labs, loc='best', fontsize=12);\n",
    "\n",
    "ax.grid(linewidth=0.3);\n",
    "ax2.grid(linewidth=0.3);\n",
    "ax.set_xlabel('Full Iterations (epochs)', labelpad=12, fontsize=10);\n",
    "ax.set_ylabel('Training MSE, Scaled Ch4 Flux (nmolCH4m-2s-1)', labelpad=12, fontsize=10);\n",
    "ax.tick_params(axis='y', labelcolor='magenta')\n",
    "ax2.set_ylabel('Validation MSE, Scaled CH4 Flux (nmolCH4m-2s-1)', labelpad=12, fontsize=10)\n",
    "ax2.tick_params(axis='y', labelcolor='slateblue')\n",
    "ax.tick_params(left=False)  # remove the ticks\n",
    "ax2.tick_params(right=False)  # remove the ticks\n",
    "plt.title('GeoCryoAI Modeling, Cost Function and Validation Loss of CH4 Flux | Alaska [2011-2021] \\n Number of CH4 Flux Samples/Replicates: 2.083M', pad=15, fontsize=14);\n",
    "ax.grid(linewidth=0.3);\n",
    "plt.tight_layout()\n",
    "#plt.savefig('ALTstats_CNNLSTMSAEmetrics_1969-2022_021323.png', dpi=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0fcf45-41ea-4834-8521-caf03ade64a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots(figsize=(10,6), dpi=1000);\n",
    "lns1=ax.plot(history5.history['mean_squared_error'], color='midnightblue', linestyle='solid', label='Training, CH4 Flux (nmolCH4m-2s-1)');\n",
    "ax2=ax.twinx();\n",
    "lns2=ax2.plot(history5.history['val_mean_squared_error'], color='magenta', linestyle='solid', label='Validation, CH4 Flux (nmolCH4m-2s-1)');\n",
    "\n",
    "lns = lns1+lns2#+lns3+lns4;\n",
    "labs = [l.get_label() for l in lns];\n",
    "ax2.legend(lns, labs, loc='best', fontsize=12);\n",
    "\n",
    "ax.grid(linewidth=0.3);\n",
    "ax2.grid(linewidth=0.3);\n",
    "ax.set_xlabel('Full Iterations (epochs)', labelpad=12, fontsize=10);\n",
    "ax.set_ylabel('Training MSE, Scaled Ch4 Flux (nmolCH4m-2s-1)', labelpad=12, fontsize=10);\n",
    "ax.tick_params(axis='y', labelcolor='midnightblue')\n",
    "ax2.set_ylabel('Validation MSE, Scaled CH4 Flux (nmolCH4m-2s-1)', labelpad=12, fontsize=10)\n",
    "ax2.tick_params(axis='y', labelcolor='magenta')\n",
    "ax.tick_params(left=False)  # remove the ticks\n",
    "ax2.tick_params(right=False)  # remove the ticks\n",
    "plt.title('GeoCryoAI Modeling, Cost Function and Validation Loss of CH4 Flux | Alaska [2011-2021] \\n Number of CH4 Flux Samples/Replicates: 2.083M', pad=15, fontsize=14);\n",
    "ax.grid(linewidth=0.3);\n",
    "plt.tight_layout()\n",
    "plt.savefig('CH4stats_1969-2022_071323.png', dpi=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795caa34-3503-4b35-8d79-05f009c5782e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.plot(reframed_co2.iloc[:,-1]['2006':'2019'].values)\n",
    "reframed_co2.iloc[:,-1]['2006':'2019']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa39f2f5-13ce-407a-989f-d1b311d644af",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots(figsize=(10,6), dpi=1000);\n",
    "lns1=ax.plot(history4.history['mean_squared_error'], color='indigo', linestyle='solid', label='Training, CO2 Flux (olCO2m-2s-1)');\n",
    "ax2=ax.twinx();\n",
    "lns2=ax2.plot(history4.history['val_mean_squared_error'], color='lime', linestyle='solid', label='Validation, CO2 Flux (olCO2m-2s-1)');\n",
    "\n",
    "lns = lns1+lns2#+lns3+lns4;\n",
    "labs = [l.get_label() for l in lns];\n",
    "ax2.legend(lns, labs, loc='best', fontsize=12);\n",
    "\n",
    "ax.grid(linewidth=0.3);\n",
    "ax2.grid(linewidth=0.3);\n",
    "ax.set_xlabel('Full Iterations (epochs)', labelpad=12, fontsize=10);\n",
    "ax.set_ylabel('Training MSE, Scaled CO2 Flux (olCO2m-2s-1)', labelpad=12, fontsize=10);\n",
    "ax.tick_params(axis='y', labelcolor='indigo')\n",
    "ax2.set_ylabel('Validation MSE, Scaled CO2 Flux (olCO2m-2s-1)', labelpad=12, fontsize=10)\n",
    "ax2.tick_params(axis='y', labelcolor='lime')\n",
    "ax.tick_params(left=False)  # remove the ticks\n",
    "ax2.tick_params(right=False)  # remove the ticks\n",
    "plt.title('GeoCryoAI Modeling, Cost Function and Validation Loss of CO2 Flux | Alaska [2006-2019] \\n Number of CO2 Flux Samples/Replicates: 1.966M', pad=15, fontsize=14);\n",
    "ax.grid(linewidth=0.3);\n",
    "plt.tight_layout()\n",
    "plt.savefig('CO2stats_1969-2022_071323.png', dpi=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d662089-f011-415a-ba3b-abf9ad754a85",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# fig,ax=plt.subplots(figsize=(10,7));\n",
    "# #lns1=ax.plot(history2.history['loss'], color='dodgerblue', label='Loss, ALT (cm)');\n",
    "# lns2=ax.plot(history2.history['mean_squared_error'], color='dodgerblue', linestyle='solid', label='RMSE, ALT (cm)');\n",
    "# ax2=ax.twinx();\n",
    "# #lns3=ax2.plot(history2.history['val_loss'], color='gold', label='Validation Loss, ALT (cm)');\n",
    "# lns4=ax2.plot(history2.history['val_mean_squared_error'], color='gold', linestyle='solid', label='Validation RMSE, ALT (cm)');\n",
    "          \n",
    "# lns = lns2+lns4; #lns1+lns2+lns3+lns4;\n",
    "# labs = [l.get_label() for l in lns];\n",
    "# ax2.legend(lns, labs, loc='best', fontsize=8);\n",
    "\n",
    "# ax.grid(linewidth=0.3);\n",
    "# ax.set_xlabel('Full Iterations (epochs)', labelpad=12, fontsize=10);\n",
    "# ax.set_ylabel('Scaled Depth to Refusal (cm)', labelpad=12, fontsize=10);\n",
    "# #ax2.set_ylabel('Scaled Depth to Refusal (cm)', labelpad=6, fontsize=9)\n",
    "# plt.title('Number of Samples/Replicates: 95653', pad=15, fontsize=12, fontweight='ultralight');\n",
    "# plt.suptitle('Cost Function and Validation Loss from Thaw Depth Modeling, GeoCryoAI Framework in Alaska [1969-2022]', fontsize=14);\n",
    "# plt.grid(linewidth=0.3);\n",
    "# #plt.show()\n",
    "# #plt.savefig('ALTstats_CNNLSTMSAEmetrics_1969-2022_021323.png', dpi=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b129cd-db7e-4ff7-8090-7c6a3d11bac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(X_test_reframed, y_test_reframed, verbose = 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86d53df-cc29-4768-bb36-469643d4aefe",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Test MAE:', score[1])\n",
    "print('Test MSE:', score[2])\n",
    "print('Test RMSE:', score[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c205f0-eed3-4bcb-b814-e3909ee0047b",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = model.predict(X_test_reframed, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338a47c2-088c-4153-8929-0767e2353719",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# fig,ax = plt.subplots(figsize=(10,6), dpi=1000)\n",
    "# ln1=ax.plot(y_test_reframed.reshape(215137,1), color='magenta', linestyle='solid', label='Observation, Test Set')\n",
    "# ln2=ax.plot(predict.reshape(215137,1), color='dodgerblue', linestyle='solid', label='Prediction, Test Set')\n",
    "# #ln3=ax.plot(history.history['mean_absolute_error'], color='springgreen', linestyle='dotted', label='MAE')\n",
    "# #ln4=ax.plot(history.history['mean_squared_error'], color='springgreen', linestyle='dashed', label='Seward Peninsula')\n",
    "# #ln4=ax.plot(history.history['root_mean_squared_error'], color='springgreen', linestyle='dashed', label='RMSE')\n",
    "# #ln5=ax.plot(history.history['val_mean_absolute_error'], color='red', linestyle='dotted', label='Val MAE')\n",
    "# #ln7=ax.plot(history.history['val_mean_squared_error'], color='red', linestyle='dashed', label='Seward Peninsula')\n",
    "# #ln6=ax.plot(history.history['val_root_mean_squared_error'], color='red', linestyle='dashed', label='Val RMSE')\n",
    "# #ln2=ax.plot(sib.iloc[2:,7].replace(-9999,np.nan).dropna()color='springgreen', linestyle='dashed', label='Interior')\n",
    "# #ln3=ax.plot(sib.iloc[2:,16].replace(-9999,np.nan).dropna(), color='magenta', linestyle='dotted', label='Seward Peninsula')\n",
    "# #ln4=ax.plot(sib.iloc[2:,39].replace(-9999,np.nan).dropna(), color='dodgerblue', linestyle='dotted', label='Yukon-Kuskokwim Delta')\n",
    "# #ax2=ax.twinx();\n",
    "# #ln4=ax2.plot(validPredict, color='coral', linestyle='dotted')\n",
    "\n",
    "# lines = ln1 + ln2 #+ ln3 + ln4 + ln5 + ln6# + ln7 + ln8\n",
    "# labs = [line.get_label() for line in lines];\n",
    "# plt.legend(lines, labs, loc='lower left', fontsize=8)\n",
    "\n",
    "# ax.grid(linewidth=0.3);\n",
    "# ax.set_xlabel('Samples, 2003-2021', labelpad=10, fontsize=16);\n",
    "# ax.set_ylabel('Carbon Dioxide Flux (umolCm2s-1)', labelpad=10, fontsize=16)\n",
    "# #ax.set(xticklabels=[])  # remove the tick labels\n",
    "# ax.tick_params(left=False)  # remove the ticks\n",
    "# #plt.ylabel('Active Layer Thickness (cm)')\n",
    "# plt.title('GeoCryoAI Modeling, Cost Function and Validation Loss of ALT | Alaska [1969-2022]', pad=15, fontsize=14);\n",
    "# plt.suptitle('Number of Samples/Replicates, ALT: 2.441M', fontsize=12, fontweight='ultralight');\n",
    "# #plt.title('GeoCryoAI In Situ Module | Bidirectional LSTM Autoencoder Loss Function \\n In Situ Carbon Dioxide Flux Simulations (2003-2021)', pad=10)\n",
    "# #plt.xlabel('Year')\n",
    "# #plt.axis([0, 6, 0, 60])\n",
    "# #plt.legend(loc='best')\n",
    "# #plt.show()\n",
    "# plt.savefig('/Users/bradleygay/Downloads/bilstmae_insitu_CO2_loss_predictions_experimental.png',dpi=1000);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d585e51-d670-4066-8e3f-a560c3b91b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(y_testco2_reframed.reshape(215137,1))\n",
    "plt.plot(predict.reshape(215137,1))\n",
    "#plt.axis([0, 130000, -2, 5])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a810c7d9-07b5-4c41-8514-0e7c2c63dc06",
   "metadata": {},
   "outputs": [],
   "source": [
    "#May need to inverse scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355fbe0b-416d-4d8d-8ef8-13a9ed3c3356",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################\n",
    "########################################################\n",
    "########################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d36a21e9-a37c-40b0-bba1-2385c9b96c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig,ax = plt.subplots(figsize=(10,6), dpi=1000)\n",
    "# #l1=ax.plot(history2.history['loss'], color='dodgerblue', linestyle='solid', label='ALT Training Loss (cm)')\n",
    "# #l2=ax.plot(history3.history['loss'], color='magenta', linestyle='solid', label='CH4 Flux Training Loss (cm)')\n",
    "# #l3=ax.plot(history4.history['loss'], color='springgreen', linestyle='solid', label='CO2 Flux Training Loss (nmolCH4m-2s-1)')\n",
    "# l1=ax.plot(history2.history['val_loss'], color='dodgerblue', linestyle='solid', label='ALT Validation Loss (cm)')\n",
    "# ax2=ax.twinx();\n",
    "# l2=ax2.plot(history3.history['val_loss'], color='magenta', linestyle='solid', label='CH4 Flux Validation Loss (nmolCH4m-2s-1)')\n",
    "# l3=ax2.plot(history4.history['val_loss'], color='springgreen', linestyle='solid', label='CO2 Flux Validation Loss (molCO2m-2s-1)')\n",
    "# #l3=ax.plot(history4.history['loss'], color='springgreen', linestyle='solid', label='CO2 Flux Loss (molCO2m-2s-1)')\n",
    "# #l1=ax.plot(history2.history['val_mean_squared_error'], color='dodgerblue', linestyle='solid', label='ALT Loss (cm)')\n",
    "# #l2=ax.plot(history3.history['val_mean_squared_error'], color='magenta', linestyle='solid', label='CH4 Flux Loss (nmolCH4m-2s-1)')\n",
    "# #l3=ax.plot(history4.history['val_mean_squared_error'], color='springgreen', linestyle='solid', label='CO2 Flux Loss (molCO2m-2s-1)')\n",
    "# #ax2=ax.twinx();\n",
    "# #l2=ax2.plot(althistory['val_mean_squared_error'], color='dodgerblue', linestyle='solid', label='Validation Loss (MSE)')\n",
    "# #l2=ax2.plot(ch4history['val_mean_squared_error'], color='coral', linestyle='solid', label='Validation Loss (MSE)')\n",
    "# #l2=ax2.plot(althistory['val_mean_squared_error'], color='coral', linestyle='solid', label='Validation Loss (MSE)')\n",
    "\n",
    "# lns = l1+l2 +l3\n",
    "# labs = [l.get_label() for l in lns];\n",
    "# #ax2.legend(lns, labs, loc='best', fontsize=8);\n",
    "# ax.legend(lns, labs, loc='best', fontsize=12);\n",
    "\n",
    "\n",
    "# ax.set_xlabel('Full Iterations (epochs)', labelpad=15, fontsize=12);\n",
    "# #ax.set_ylabel('Training Loss (units)', labelpad=15, fontsize=12)\n",
    "# ax.set_ylabel('Validation Loss (units)', labelpad=15, fontsize=12)\n",
    "# ax2.set_ylabel('Validation Loss (units)', labelpad=15, fontsize=10)\n",
    "# #ax2.set_ylabel('Validation Loss, MSE (cm)', labelpad=15, fontsize=10)\n",
    "# #ax.set_ylabel('Training Loss (units)', labelpad=15, fontsize=12)\n",
    "# #ax2.set_ylabel('Validation Loss, MSE (nmolCH4m2s-1)', labelpad=15, fontsize=10)\n",
    "# #ax.set_ylabel('Loss, MSE (molCO2m2s-1)', labelpad=15, fontsize=10)\n",
    "# #ax2.set_ylabel('Validation Loss, MSE (molCO2m2s-1)', labelpad=15, fontsize=10)\n",
    "# #ax.set(xticklabels=[])  # remove the tick labels\n",
    "# ax.tick_params(left=False)  # remove the ticks\n",
    "# ax2.tick_params(left=False)  # remove the ticks\n",
    "# #plt.ylabel('Active Layer Thickness (cm)')\n",
    "# ax.grid(linewidth=0.3);\n",
    "# plt.title('GeoCryoAI Modeling, Cost Function and Validation Loss of ALT | Alaska [1969-2022] \\n Number of Samples/Replicates, ALT: 2.441M', pad=15, fontsize=14);\n",
    "# #plt.suptitle('Number of Samples/Replicates, ALT: 2.441M', fontsize=12, fontweight='ultralight');\n",
    "# #plt.title('GeoCryoAI Training and Validation Loss | In Situ Thaw Depth Simulations [1969-2022]', pad=15, fontsize=14)\n",
    "# #plt.title('GeoCryoAI Model Simulations | Cost Functions \\n ALT, CH4 Flux, and CO2 Flux [1969-2022]', pad=15, fontsize=14)\n",
    "# #plt.title('GeoCryoAI Training and Validation Loss | In Situ CO2 Flux Simulations [2006-2019]', pad=15, fontsize=14)\n",
    "# #plt.xlabel('Year')\n",
    "# #plt.axis([0, 6, 0, 60])\n",
    "# #plt.grid(linewidth=0.3);\n",
    "# #plt.show()\n",
    "# #plt.savefig('/Users/bradleygay/Downloads/bilstmae_insitu_ALT_loss.png',dpi=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7d3acc-5057-4bad-9b5b-d411616d10d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_model=alt_loaded_model_json\n",
    "ch4_model=ch4_loaded_model_json\n",
    "co2_model=co2_loaded_model_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55602a9-da09-47bd-a112-e9cc4e969db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = 'mean_squared_error'\n",
    "alt_metrics=['mean_squared_error', 'mean_absolute_error', RSquare().build(input_shape = (trainyscaltref.shape[1],))]\n",
    "alt_model.compile(optimizer = tf.keras.optimizers.legacy.RMSprop(learning_rate = 0.0001, **{\"clipvalue\" : 1000}),\n",
    "                  loss = loss_function, metrics = alt_metrics)\n",
    "ch4_metrics=['mean_squared_error', 'mean_absolute_error', RSquare().build(input_shape = (trainyscch4ref.shape[1],))]\n",
    "ch4_model.compile(optimizer = tf.keras.optimizers.legacy.RMSprop(learning_rate = 0.001, **{\"clipvalue\" : 1000}),\n",
    "                  loss = loss_function, metrics = ch4_metrics)\n",
    "co2_metrics=['mean_squared_error', 'mean_absolute_error', RSquare().build(input_shape = (trainyscco2ref.shape[1],))]\n",
    "co2_model.compile(optimizer = tf.keras.optimizers.legacy.RMSprop(learning_rate = 0.0001, **{\"clipvalue\" : 1000}),\n",
    "                  loss = loss_function, metrics = co2_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb8172e-5eb8-4319-a062-9758c3354918",
   "metadata": {},
   "outputs": [],
   "source": [
    "#score1=alt_model.evaluate(testXscaltref,testyscaltref,verbose=1)\n",
    "#score2=co2_model.evaluate(testXscco2ref,testyscco2ref,verbose=1)\n",
    "score3=ch4_model.evaluate(testXscch4ref,testyscch4ref,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b716cd-f2e0-4039-ac03-70a2da236d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_2, score_3b, score_4#score_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7bc4fd1-6261-4aba-a1c2-99bc36c9c749",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ALT\n",
    "print('Test MAE:', score1[1])\n",
    "print('Test MSE:', score1[2])\n",
    "print('Test RMSE:', np.sqrt(score1[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995d40c4-57fe-4bc9-81fb-8b81cebf4332",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CO2\n",
    "print('Test MAE:', score2[1])\n",
    "print('Test MSE:', score2[2])\n",
    "print('Test RMSE:', np.sqrt(score2[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63266f6-8b67-498c-98ac-5b74ebb386e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CH4\n",
    "print('Test MAE:', score3[1])\n",
    "print('Test MSE:', score3[2])\n",
    "print('Test RMSE:', np.sqrt(score3[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4e575b-3068-48e3-abf5-0110dc00fb36",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainyscaltpred = alt_model.predict(trainXscaltref)\n",
    "validyscaltpred = alt_model.predict(validXscaltref)\n",
    "testyscaltpred = alt_model.predict(testXscaltref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b9f9d5-8f6f-411c-8384-c81cb91dc2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('trainyscaltpred', 'wb') as file_theta:\n",
    "    pickle.dump(trainyscaltpred, file_theta)\n",
    "with open('validyscaltpred', 'wb') as file_alpha:\n",
    "    pickle.dump(validyscaltpred, file_alpha)\n",
    "with open('testyscaltpred', 'wb') as file_zeta:\n",
    "    pickle.dump(testyscaltpred, file_zeta)\n",
    "\n",
    "with open('/Users/bradleygay/code/trainyscaltpred', 'rb') as file_theta:\n",
    "    trainyscaltpred=pickle.load(file_theta)\n",
    "with open('/Users/bradleygay/code/validyscaltpred', 'rb') as file_alpha:\n",
    "    validyscaltpred=pickle.load(file_alpha)\n",
    "with open('/Users/bradleygay/code/testyscaltpred', 'rb') as file_zeta:\n",
    "    testyscaltpred=pickle.load(file_zeta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62880eb0-7371-48a7-ac7f-10a996537a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainyscco2pred = co2_model.predict(trainXscco2ref)\n",
    "validyscco2pred = co2_model.predict(validXscco2ref)\n",
    "testyscco2pred = co2_model.predict(testXscco2ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d69bcc-79b0-4748-b89c-e5834ad63937",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('trainyscco2pred', 'wb') as file_a:\n",
    "    pickle.dump(trainyscco2pred, file_a)\n",
    "with open('validyscco2pred', 'wb') as file_b:\n",
    "    pickle.dump(validyscco2pred, file_b)\n",
    "with open('testyscco2pred', 'wb') as file_c:\n",
    "    pickle.dump(testyscco2pred, file_c)\n",
    "\n",
    "with open('/Users/bradleygay/code/trainyscco2pred', 'rb') as file_a:\n",
    "    trainyscco2pred=pickle.load(file_a)\n",
    "with open('/Users/bradleygay/code/validyscco2pred', 'rb') as file_b:\n",
    "    validyscco2pred=pickle.load(file_b)\n",
    "with open('/Users/bradleygay/code/testyscco2pred', 'rb') as file_c:\n",
    "    testyscco2pred=pickle.load(file_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873d90c1-83f5-4a51-a8ad-680fe34d68e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainyscch4pred = ch4_model.predict(trainXscch4ref)\n",
    "validyscch4pred = ch4_model.predict(validXscch4ref)\n",
    "testyscch4pred = ch4_model.predict(testXscch4ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dee9df0-916a-4a13-a3e6-a34542b38418",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('trainyscch4pred', 'wb') as file_i:\n",
    "    pickle.dump(trainyscch4pred, file_i)\n",
    "with open('validyscch4pred', 'wb') as file_ii:\n",
    "    pickle.dump(validyscch4pred, file_ii)\n",
    "with open('testyscch4pred', 'wb') as file_iii:\n",
    "    pickle.dump(testyscch4pred, file_iii)\n",
    "\n",
    "with open('/Users/bradleygay/code/trainyscch4pred', 'rb') as file_i:\n",
    "    trainyscch4pred=pickle.load(file_i)\n",
    "with open('/Users/bradleygay/code/validyscch4pred', 'rb') as file_ii:\n",
    "    validyscch4pred=pickle.load(file_ii)\n",
    "with open('/Users/bradleygay/code/testyscch4pred', 'rb') as file_iii:\n",
    "    testyscch4pred=pickle.load(file_iii)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04088ae4-913c-4497-8348-feea5dd1944c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model2 == ALT (history2)\n",
    "#model3b == CH4 (history5)\n",
    "#model4 == CO2 (history4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623745ee-e537-41fc-acab-6e592c0f9122",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e79120-6373-413a-bdc0-2f0a30b7023e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#altlist=[]\n",
    "#altlist=np.append(altlist,p)\n",
    "#altlist=np.append(altlist,pp)\n",
    "altlist=np.append(altlist,ppp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a239e2-3467-4afe-a28e-d3835356eb11",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(yscaleralt.inverse_transform(altlist.reshape(-1,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da9e356-0ddb-4693-b9ea-ecc1be40e83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(yscaleralt.inverse_transform(p.reshape(1432318,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a02c3ef-27e9-4bba-b51e-bb05048912a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_val = X_scaler.fit_transform(df.tail(48))\n",
    "   val_rescaled = data_val.reshape(1, data_val.shape[0], data_val.shape[1])\n",
    " pred = lstm_model.predict(val_rescaled)\n",
    " pred_Inverse = Y_scaler.inverse_transform(pred)\n",
    " pred_Inverse "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b1a938-98b2-455d-b014-78ba7e0cff48",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(p.reshape(1432318, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bbc21b1-c292-4473-a4ad-d71aedef1d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(testXscaltref.reshape((testXscaltref.shape[0], testXscaltref.shape[2])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15ce883-0407-4f73-8ab8-20019598be58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a prediction\n",
    "yhat = model2.predict(testXscalt)\n",
    "test_X = testXscalt.reshape((test_X.shape[0], test_X.shape[2]))\n",
    "# invert scaling for forecast\n",
    "inv_yhat = concatenate((yhat, test_X[:, 1:]), axis=1)\n",
    "inv_yhat = scaler.inverse_transform(inv_yhat)\n",
    "inv_yhat = inv_yhat[:,0]\n",
    "# invert scaling for actual\n",
    "test_y = test_y.reshape((len(test_y), 1))\n",
    "inv_y = concatenate((test_y, test_X[:, 1:]), axis=1)\n",
    "inv_y = scaler.inverse_transform(inv_y)\n",
    "inv_y = inv_y[:,0]\n",
    "# calculate RMSE\n",
    "rmse = sqrt(mean_squared_error(inv_y, inv_yhat))\n",
    "print('Test RMSE: %.3f' % rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d6cbe6-3881-4c63-9fec-e07f113f5977",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainXaltdf=pd.DataFrame(trainXalt).to_numpy().reshape(1432318, 1, 273)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ee2732-fdcd-4658-a504-547fd736d183",
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_pred = model2.predict(trainXaltdfres)\n",
    "y_pred_scaled = model2.predict(testXscaltref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9869554d-35c0-4bba-97b0-67f571657927",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47084b53-6d2b-4d28-84d6-dbef770b4f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_scaled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cee2f5e-1fc3-45ba-a071-7490277ffc96",
   "metadata": {},
   "outputs": [],
   "source": [
    "testXalt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7819df7-bf43-4400-90aa-0c025178d57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "testyalt.iloc[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c682862-cbbc-4fc8-91cf-e4845b09d47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "testyscaltpd=pd.DataFrame(testyscalt)\n",
    "testyscaltpd.index=testyalt.iloc[:,0].index\n",
    "testyscaltpd.index=pd.to_datetime(testyscaltpd.index, format='%Y')\n",
    "testyscaltpd.index.name = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9a77b2-1777-4309-b14a-42ce03087727",
   "metadata": {},
   "outputs": [],
   "source": [
    "#testyscaltpd.to_numpy().reshape(215137,1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da26e081-f53a-4d9d-ac05-c528321afef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "testyscaltpd.columns=[testyalt.iloc[:,-1].name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd763c45-e126-44a3-9104-de20eabdd546",
   "metadata": {},
   "outputs": [],
   "source": [
    "testyscaltpd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5502ceb4-04ab-47f7-9720-6faf49712a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "validyscaltpd=pd.DataFrame(validyscalt)\n",
    "validyscaltpd.index=validyalt.iloc[:,0].index\n",
    "validyscaltpd.index=pd.to_datetime(validyscaltpd.index, format='%Y')\n",
    "validyscaltpd.index.name = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb788e98-1c4c-42f6-8d8e-dcd151ecb44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#validyscaltpd.to_numpy().reshape(793782,1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1e40f7-3016-4e1f-97c0-e9468a0392b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "validyscaltpd.columns=[validyalt.iloc[:,-1].name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3807cb1-487f-4de5-8473-48f8ffbc46d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "validyscaltpd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313fe152-3fa7-4171-9061-294f8cf16c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainyscaltpd=pd.DataFrame(trainyscalt)\n",
    "trainyscaltpd.index=trainyalt.iloc[:,0].index\n",
    "trainyscaltpd.index=pd.to_datetime(trainyscaltpd.index, format='%Y')\n",
    "trainyscaltpd.index.name = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2a755b-057c-43d6-90f7-95a42d844e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainyscaltpd.columns=[trainyalt.iloc[:,-1].name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0ccd32-adbd-450c-ac26-39bd5a88413d",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainyscaltpd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b83de7f-009e-41a5-8513-f8b33000d8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(trainyscaltpd)\n",
    "plt.plot(validyscaltpd)\n",
    "plt.plot(testyscaltpd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47fb0da0-a8e6-4813-9edd-1de206e49628",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(trainyalt)\n",
    "plt.plot(validyalt)\n",
    "plt.plot(testyalt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96963798-c8a4-4722-b094-0323abe83eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainXalt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7a9bff-f46c-4f3c-81b9-88d1890ea652",
   "metadata": {},
   "outputs": [],
   "source": [
    "#testyscaltpd.to_numpy().reshape(215137,1,1)\n",
    "#testyscalt.reshape(215137,1,1)\n",
    "t=trainXalt.iloc[:,-1].resample('Y').mean()\n",
    "v=validXalt.iloc[:,-1].resample('Y').mean()\n",
    "r=testXalt.iloc[:,-1].resample('Y').mean()#.reshape(215137,1, 273)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e420658d-bde7-495c-8a22-da9e546b3fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainXalt.iloc[:,-183]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d027d2-8c6e-48b9-8a28-ff7325ced67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "t2=trainXalt.iloc[:,-92].resample('Y').mean()\n",
    "v2=validXalt.iloc[:,-92].resample('Y').mean()\n",
    "r2=testXalt.iloc[:,-92].resample('Y').mean()#.reshape(215137,1, 273)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eee0fda-495e-44f4-a2ab-4243db88a7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "t3=trainXalt.iloc[:,-183].resample('Y').mean()\n",
    "v3=validXalt.iloc[:,-183].resample('Y').mean()\n",
    "r3=testXalt.iloc[:,-183].resample('Y').mean()#.reshape(215137,1, 273)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d1b828-2b66-4871-b87d-e590e14f390b",
   "metadata": {},
   "outputs": [],
   "source": [
    "yup=[]\n",
    "yup=np.append(yup,t.values)\n",
    "yup=np.append(yup,v.values)\n",
    "yup=np.append(yup,r.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d5c85d-7ea9-4619-abb8-51be456b5f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "yup2=[]\n",
    "yup2=np.append(yup2,t2.values)\n",
    "yup2=np.append(yup2,v2.values)\n",
    "yup2=np.append(yup2,r2.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b328c797-b95b-4e6f-af68-fc642a8ab983",
   "metadata": {},
   "outputs": [],
   "source": [
    "yup3=[]\n",
    "yup3=np.append(yup3,t3.values)\n",
    "yup3=np.append(yup3,v3.values)\n",
    "yup3=np.append(yup3,r3.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20599b02-0b58-494b-96ad-612cf4c90711",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(reframed_alt.iloc[:,-1].resample('Y').mean().values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f01064-d90d-4fdc-a812-624746b94c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(df['ALT'].resample('Y').mean().values)\n",
    "#plt.plot(reframed_alt.iloc[:,-1].resample('Y').mean().values)\n",
    "plt.plot(yup, linestyle='dotted')\n",
    "plt.plot(yup2, linestyle='dotted')\n",
    "plt.plot(yup3, linestyle='dotted')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b128bc70-4387-425e-9a88-6d1a06c70dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(df['ALT'].resample('Y').mean().values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7938ba1c-4468-40bd-8347-a342fa026db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(yup, linestyle='dotted')\n",
    "plt.plot(yup2, linestyle='dotted')\n",
    "plt.plot(yup3, linestyle='dotted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec51c4d-09f4-49a0-b88f-cf1d34575ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainXaltdfres=pd.DataFrame(trainXalt).to_numpy().reshape(1432318, 1, 273)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f24f84-21fc-4326-ad15-ab6daa5e6535",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### ALT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4779c2b4-572c-4e64-9024-c2530ec62230",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainXscalt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8ca38a-d3ba-4412-8f47-75f13e1d5e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainXscaltdfres=pd.DataFrame(trainXscalt).to_numpy().reshape(1432318, 1, 273)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "689e3ff4-ef8f-4232-8cf2-0128060058ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainXscaltdfres.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28b57cf-f1be-4b3c-9326-173b5310b557",
   "metadata": {},
   "outputs": [],
   "source": [
    "#p=model2.predict(trainXaltdfres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eeee0a4-6ac6-4a80-8421-b6a8933d2814",
   "metadata": {},
   "outputs": [],
   "source": [
    "p=model2.predict(trainXscaltdfres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ad23db-b6be-4b39-9d7f-43bbd5e43c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "p.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3f43d7-cbd7-418e-9b75-33afe02c1abc",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# plt.plot(trainXalt.to_numpy().reshape(1432318, 1, 273)[:,-1,-1])\n",
    "# plt.plot(validXalt.to_numpy().reshape(793782, 1, 273)[:,-1,-1])\n",
    "# plt.plot(testXalt.to_numpy().reshape(215137, 1, 273)[:,-1,-1])\n",
    "# plt.plot(trainXscalt.reshape(1432318, 1, 273)[:,-1,-1])\n",
    "# plt.plot(validXscalt.reshape(793782, 1, 273)[:,-1,-1])\n",
    "# plt.plot(testXscalt.reshape(215137, 1, 273)[:,-1,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196e3be0-7db7-456f-a80f-51400ad5324b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(p.reshape(1432318,1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4afb134-7cb1-4625-9183-e8a6c52027ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "validXscalt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c56577c-f506-4cef-88b3-fc801806cb5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "validXscaltdfres=pd.DataFrame(validXscalt).to_numpy().reshape(793782, 1, 273)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd40e28b-14fc-468d-9234-2c6abda33120",
   "metadata": {},
   "outputs": [],
   "source": [
    "validXscaltdfres.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b5e641-2dba-437f-9e7e-9465f266921b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pp=model2.predict(validXscaltdfres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9eb9662-ee30-4274-8570-8a8479834dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "pp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c68c222-c7a9-43d6-82f0-27e1a3579e2e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# plt.plot(trainXalt.to_numpy().reshape(1432318, 1, 273)[:,-1,-1])\n",
    "# plt.plot(validXalt.to_numpy().reshape(793782, 1, 273)[:,-1,-1])\n",
    "# plt.plot(testXalt.to_numpy().reshape(215137, 1, 273)[:,-1,-1])\n",
    "# plt.plot(trainXscalt.reshape(1432318, 1, 273)[:,-1,-1])\n",
    "# plt.plot(validXscalt.reshape(793782, 1, 273)[:,-1,-1])\n",
    "# plt.plot(testXscalt.reshape(215137, 1, 273)[:,-1,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b816a21-2490-416f-97ee-86b08016b09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(pp.reshape(793782,1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6052653f-e0f2-4896-9821-f85dbe09cdfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "testXscalt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5af13b4-636d-424d-a85b-634eb660c557",
   "metadata": {},
   "outputs": [],
   "source": [
    "testXscaltdfres=pd.DataFrame(testXscalt).to_numpy().reshape(215137, 1, 273)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6903e7a8-fc30-4eea-ba98-5ffda34b250b",
   "metadata": {},
   "outputs": [],
   "source": [
    "testXscaltdfres.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a354554-08e4-4007-ba7a-2e7ca7b4de44",
   "metadata": {},
   "outputs": [],
   "source": [
    "ppp=model2.predict(testXscaltdfres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31082c7-b7ea-437b-b085-8d91936c17a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ppp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7549a6-cd7f-4c7b-b8b8-4a2a372ea014",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# plt.plot(trainXalt.to_numpy().reshape(1432318, 1, 273)[:,-1,-1])\n",
    "# plt.plot(validXalt.to_numpy().reshape(793782, 1, 273)[:,-1,-1])\n",
    "# plt.plot(testXalt.to_numpy().reshape(215137, 1, 273)[:,-1,-1])\n",
    "# plt.plot(trainXscalt.reshape(1432318, 1, 273)[:,-1,-1])\n",
    "# plt.plot(validXscalt.reshape(793782, 1, 273)[:,-1,-1])\n",
    "# plt.plot(testXscalt.reshape(215137, 1, 273)[:,-1,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e89a753-d2fc-48bf-9d32-8e22fcf33242",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(ppp.reshape(215137,1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f41a7145-74ff-4fcd-8bda-7addd038f6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr=[]\n",
    "arr=np.append(arr,p.reshape(1432318,))\n",
    "arr=np.append(arr,pp.reshape(793782,))\n",
    "arr=np.append(arr,ppp.reshape(215137,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44dc5cdf-e649-41f3-8cbb-7a5514b9420c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1563e2f-869f-4836-bbd4-0f92ea6fb6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr2=[]\n",
    "arr2=np.append(arr2,trainyalt.values.reshape(1432318,))\n",
    "arr2=np.append(arr2,validyalt.values.reshape(793782,))\n",
    "arr2=np.append(arr2,testyalt.values.reshape(215137,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485f9d89-8f43-4423-a63c-e050461795de",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(arr2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d06b8d-92d7-4639-a3fa-71635a5f9049",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(yscaleralt.inverse_transform(arr.reshape(2441237,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf53c054-dc99-4bf5-8b9e-e0bb346d0a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "2441237-215137\n",
    "#testXscalt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "306119ef-f4ff-46c2-800d-1f030f08049c",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c39334-03bf-4cf9-9bda-110c5eb38a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots(figsize=(10,6), dpi=1000);\n",
    "lns1=ax.plot(arr2.reshape(2441237,1), color='dodgerblue', linestyle='solid', label='Thaw Depth Observations, ALT (cm)');\n",
    "#ax2=ax.twinx();\n",
    "lns2=ax.plot(yscaleralt.inverse_transform(arr.reshape(2441237,1)), color='tomato', alpha=0.5, linestyle='solid', label='Thaw Depth Predictions, ALT (cm)');\n",
    "\n",
    "lns = lns1+lns2#+lns3+lns4;\n",
    "labs = [l.get_label() for l in lns];\n",
    "ax.legend(lns, labs, loc='best', fontsize=12);\n",
    "\n",
    "ax.grid(linewidth=0.3);\n",
    "#ax2.grid(linewidth=0.3);\n",
    "ax.set_xlabel('Full Iterations (epochs)', labelpad=12, fontsize=10);\n",
    "ax.set_ylabel('Active Layer Thickness (cm)', labelpad=12, fontsize=10);\n",
    "#ax.tick_params(axis='y', labelcolor='springgreen')\n",
    "#ax2.set_ylabel('Validation MSE, Scaled CO2 Flux (olCO2m-2s-1)', labelpad=12, fontsize=10)\n",
    "#ax2.tick_params(axis='y', labelcolor='yellowgreen')\n",
    "ax.tick_params(left=False)  # remove the ticks\n",
    "#ax2.tick_params(right=False, labelright=False)  # remove the ticks\n",
    "plt.title('GeoCryoAI Modeling, ALT Observations v. Predictions | Alaska [1969-2022] \\n Number of ALT Samples/Replicates: 2.441M', pad=15, fontsize=14);\n",
    "ax.grid(linewidth=0.3);\n",
    "plt.axis([0, 2441237, 0, 300])\n",
    "#plt.axis([2226100, 2441237, 0, 200])\n",
    "plt.tight_layout()\n",
    "plt.savefig('ALT_ObsVPred_1969-2022_071323.svg', dpi=1000)\n",
    "plt.savefig('ALT_ObsVPred_1969-2022_071323.png', dpi=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12802ac0-b4b6-425b-8b74-ef0824d12ce6",
   "metadata": {},
   "source": [
    "### CO2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94da7c48-1607-46ce-a953-6984615c94e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainXscco2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69cf4819-fa1c-4d16-bae0-b6b944e0fb4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainXscco2dfres=pd.DataFrame(trainXscco2).to_numpy().reshape(1432318, 1, 273)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7664a4ed-96f7-41d7-8557-1c730c303f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainXscco2dfres.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a339252-4aed-4881-95a0-fd0b39b6bf94",
   "metadata": {},
   "outputs": [],
   "source": [
    "#p=model2.predict(trainXaltdfres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7e1a81-dba8-4a72-ad5b-f87e1e148e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "q=model4.predict(trainXscco2dfres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d2b48a-fd00-4979-9c71-5cb76757cecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "q.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548bf0e8-2490-4635-9729-8b3a39d5bca5",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# plt.plot(trainXalt.to_numpy().reshape(1432318, 1, 273)[:,-1,-1])\n",
    "# plt.plot(validXalt.to_numpy().reshape(793782, 1, 273)[:,-1,-1])\n",
    "# plt.plot(testXalt.to_numpy().reshape(215137, 1, 273)[:,-1,-1])\n",
    "# plt.plot(trainXscalt.reshape(1432318, 1, 273)[:,-1,-1])\n",
    "# plt.plot(validXscalt.reshape(793782, 1, 273)[:,-1,-1])\n",
    "# plt.plot(testXscalt.reshape(215137, 1, 273)[:,-1,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770fccb9-dd6e-4775-9f71-cb1d79c06f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(q.reshape(1432318,1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84c2b48-b463-47f6-ae42-5560dbeaa8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "validXscco2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b67a833-c786-475b-88a9-c26707c5358b",
   "metadata": {},
   "outputs": [],
   "source": [
    "validXscco2dfres=pd.DataFrame(validXscco2).to_numpy().reshape(793782, 1, 273)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e11d5ef-6b2e-4f5e-b375-7da113f765bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "validXscco2dfres.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d3bc91-ea29-4ba9-aa3d-f621f5eacde2",
   "metadata": {},
   "outputs": [],
   "source": [
    "qq=model4.predict(validXscco2dfres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8b401e-3384-4edf-b5d3-95f1df38cb52",
   "metadata": {},
   "outputs": [],
   "source": [
    "qq.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e47f3c5-50b0-49db-82e1-c0e30a2cdfc1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# plt.plot(trainXalt.to_numpy().reshape(1432318, 1, 273)[:,-1,-1])\n",
    "# plt.plot(validXalt.to_numpy().reshape(793782, 1, 273)[:,-1,-1])\n",
    "# plt.plot(testXalt.to_numpy().reshape(215137, 1, 273)[:,-1,-1])\n",
    "# plt.plot(trainXscalt.reshape(1432318, 1, 273)[:,-1,-1])\n",
    "# plt.plot(validXscalt.reshape(793782, 1, 273)[:,-1,-1])\n",
    "# plt.plot(testXscalt.reshape(215137, 1, 273)[:,-1,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a62c8a07-0e22-41e0-91a2-a3e8d78780d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(qq.reshape(793782,1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b27d129-7c36-4a38-a020-b3e3ccf5806b",
   "metadata": {},
   "outputs": [],
   "source": [
    "testXscco2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b39700-48d2-49f2-8c47-2a0d2cac4dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "testXscco2dfres=pd.DataFrame(testXscco2).to_numpy().reshape(215137, 1, 273)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113b75e8-59af-4050-8fda-7df580e10852",
   "metadata": {},
   "outputs": [],
   "source": [
    "testXscco2dfres.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206348de-dd6b-43c0-8853-25ac9ae185e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "qqq=model4.predict(testXscco2dfres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76228ee1-150d-457c-8ce8-80dbd4ab9f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "qqq.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3e2abe-6acb-4daa-9901-792655bfaa77",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# plt.plot(trainXalt.to_numpy().reshape(1432318, 1, 273)[:,-1,-1])\n",
    "# plt.plot(validXalt.to_numpy().reshape(793782, 1, 273)[:,-1,-1])\n",
    "# plt.plot(testXalt.to_numpy().reshape(215137, 1, 273)[:,-1,-1])\n",
    "# plt.plot(trainXscalt.reshape(1432318, 1, 273)[:,-1,-1])\n",
    "# plt.plot(validXscalt.reshape(793782, 1, 273)[:,-1,-1])\n",
    "# plt.plot(testXscalt.reshape(215137, 1, 273)[:,-1,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ba4511-1b51-424d-8b14-9e1d83915f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(qqq.reshape(215137,1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653b34e4-9596-418a-8be0-e399efd662a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr3=[]\n",
    "arr3=np.append(arr3,q.reshape(1432318,))\n",
    "arr3=np.append(arr3,qq.reshape(793782,))\n",
    "arr3=np.append(arr3,qqq.reshape(215137,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f816b041-9879-4485-be0b-637d3aa39f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(arr3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b96bbcb-9e2c-4f96-aa70-b8c01474db4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr4=[]\n",
    "arr4=np.append(arr4,trainyco2.values.reshape(1432318,))\n",
    "arr4=np.append(arr4,validyco2.values.reshape(793782,))\n",
    "arr4=np.append(arr4,testyco2.values.reshape(215137,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c32757-2586-49fb-bb81-801087e65d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(arr4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8caa684-9637-4811-9025-165ee4ad6dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(yscalerco2.inverse_transform(arr3.reshape(2441237,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3f65c2-9388-46eb-872c-3d7c9179d4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reframed_co2.iloc[:,-1].name\n",
    "#('CO2_1_2_1', 't')\n",
    "#reframed_co2.iloc[:,-1]['2006':'2019']\n",
    "#reframed_co2.shape\n",
    "#2441237-1965628\n",
    "#78115:2043743\n",
    "plt.plot(reframed_co2.iloc[78115:2043743,-1].values)#['2005':'2019']\n",
    "#reframed_co2.iloc[:,-1][:'2019']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1b5caa-42a3-4a66-800a-f021fe612251",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots(figsize=(10,6), dpi=1000);\n",
    "lns1=ax.plot(arr4.reshape(2441237,1), color='indigo', linestyle='solid', label='Flux Observations, CO2 (molCO2m-2s-1)');\n",
    "ax2=ax.twinx();\n",
    "lns2=ax2.plot(yscalerco2.inverse_transform(arr3.reshape(2441237,1)), alpha=0.5, color='lime', linestyle='solid', label='Flux Predictions, CO2 (molCO2m-2s-1)');\n",
    "\n",
    "lns = lns1+lns2#+lns3+lns4;\n",
    "labs = [l.get_label() for l in lns];\n",
    "ax.legend(lns, labs, loc='upper right', fontsize=12);\n",
    "\n",
    "ax.grid(linewidth=0.3);\n",
    "#ax2.grid(linewidth=0.3);\n",
    "ax.set_xlabel('Full Iterations (epochs)', labelpad=12, fontsize=10);\n",
    "ax.set_ylabel('Carbon Dioxide Flux (molCO2m-2s-1)', labelpad=12, fontsize=10);\n",
    "#ax.tick_params(axis='y', labelcolor='springgreen')\n",
    "#ax2.set_ylabel('Validation MSE, Scaled CO2 Flux (olCO2m-2s-1)', labelpad=12, fontsize=10)\n",
    "#ax2.tick_params(axis='y', labelcolor='yellowgreen')\n",
    "ax.tick_params(left=False, labelright=False)  # remove the ticks\n",
    "ax2.tick_params(right=False, labelright=False)  # remove the ticks\n",
    "plt.title('GeoCryoAI Modeling, CO2 Flux Observations v. Predictions | Alaska [2006-2019] \\n Number of CO2 Flux \\\n",
    "Samples/Replicates: 1.966M', pad=15, fontsize=14);\n",
    "ax.grid(linewidth=0.3);\n",
    "#plt.axis([78115, 2043743, 0, 800])\n",
    "#plt.axis([2226100, 2441237, 0, 200])\n",
    "plt.tight_layout()\n",
    "plt.savefig('CO2_ObsVPred_2006-2019_071323.svg', dpi=1000)\n",
    "plt.savefig('CO2_ObsVPred_2006-2019_071323.png', dpi=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011fc2e2-2970-4f32-8867-b705164ce6a3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### CH4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a4e40fc-4cc1-40f7-868d-5f6b8c8cbb70",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainXscch4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5048b4-b6ac-4a65-a5ab-67a61cf321af",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainXscch4dfres=pd.DataFrame(trainXscch4).to_numpy().reshape(1432318, 1, 273)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2ed6ce-2736-4cac-ae70-fcccaac74e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainXscch4dfres.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863cc1de-bf63-4d72-a84b-ba7194b830f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#p=model2.predict(trainXaltdfres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25d3870-4d5f-4e59-a251-e13fc40996e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "o=model3b.predict(trainXscch4dfres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3024244-bc0b-4696-b2ea-f821a0bf83f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "o.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba06ad3-0e9f-4646-8bf1-f1942c098744",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# plt.plot(trainXalt.to_numpy().reshape(1432318, 1, 273)[:,-1,-1])\n",
    "# plt.plot(validXalt.to_numpy().reshape(793782, 1, 273)[:,-1,-1])\n",
    "# plt.plot(testXalt.to_numpy().reshape(215137, 1, 273)[:,-1,-1])\n",
    "# plt.plot(trainXscalt.reshape(1432318, 1, 273)[:,-1,-1])\n",
    "# plt.plot(validXscalt.reshape(793782, 1, 273)[:,-1,-1])\n",
    "# plt.plot(testXscalt.reshape(215137, 1, 273)[:,-1,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98dfb0ef-e8e6-4ed4-bf4e-023f0736e874",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(o.reshape(1432318,1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780e0cd9-9ddf-4556-ab49-125e9b501b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "validXscch4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91bcff57-8461-47d5-bbde-d9ed0db59563",
   "metadata": {},
   "outputs": [],
   "source": [
    "validXscch4dfres=pd.DataFrame(validXscch4).to_numpy().reshape(793782, 1, 273)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4538961c-def3-4e7f-8484-b5c2e1fd41ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "validXscch4dfres.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e64083-6ff8-4a07-b98f-718d3b227cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "oo=model3b.predict(validXscch4dfres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764a3ea1-182b-4fb6-9bbe-a5dfd04fab81",
   "metadata": {},
   "outputs": [],
   "source": [
    "oo.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e235c48-e82c-4f27-bd9b-f981ed3d524f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# plt.plot(trainXalt.to_numpy().reshape(1432318, 1, 273)[:,-1,-1])\n",
    "# plt.plot(validXalt.to_numpy().reshape(793782, 1, 273)[:,-1,-1])\n",
    "# plt.plot(testXalt.to_numpy().reshape(215137, 1, 273)[:,-1,-1])\n",
    "# plt.plot(trainXscalt.reshape(1432318, 1, 273)[:,-1,-1])\n",
    "# plt.plot(validXscalt.reshape(793782, 1, 273)[:,-1,-1])\n",
    "# plt.plot(testXscalt.reshape(215137, 1, 273)[:,-1,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3e71db-d849-472b-ad82-ec5e4759dded",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(oo.reshape(793782,1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7e576d-4819-478b-abf8-0a53912478d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "testXscch4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "029bf568-ba6e-46c8-9531-e7b605c371bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "testXscch4dfres=pd.DataFrame(testXscch4).to_numpy().reshape(215137, 1, 273)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f31ca0c-b766-497b-ab02-1ec276ce8bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "testXscch4dfres.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7cbeb0b-6493-4ca6-83df-7904b3146bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ooo=model3b.predict(testXscch4dfres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e544750c-5ea6-436a-9ef6-3af5e3fae047",
   "metadata": {},
   "outputs": [],
   "source": [
    "ooo.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f2ea40-f3ec-4279-b065-077d52a9af96",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# plt.plot(trainXalt.to_numpy().reshape(1432318, 1, 273)[:,-1,-1])\n",
    "# plt.plot(validXalt.to_numpy().reshape(793782, 1, 273)[:,-1,-1])\n",
    "# plt.plot(testXalt.to_numpy().reshape(215137, 1, 273)[:,-1,-1])\n",
    "# plt.plot(trainXscalt.reshape(1432318, 1, 273)[:,-1,-1])\n",
    "# plt.plot(validXscalt.reshape(793782, 1, 273)[:,-1,-1])\n",
    "# plt.plot(testXscalt.reshape(215137, 1, 273)[:,-1,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baea627e-13ae-4e75-b43e-a80940273ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(ooo.reshape(215137,1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68bf5bdf-94ce-48df-aade-e575048794c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr5=[]\n",
    "arr5=np.append(arr5,o.reshape(1432318,))\n",
    "arr5=np.append(arr5,oo.reshape(793782,))\n",
    "arr5=np.append(arr5,ooo.reshape(215137,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbba2da5-362d-45b0-9fe1-2caa569aca06",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr5.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f099da-23d5-40d4-9baa-3b59b4d81bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(yscalerch4.inverse_transform(arr5.reshape(2441237,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c029f448-e71e-44b9-b64b-abdf59c87c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr6=[]\n",
    "arr6=np.append(arr6,trainych4.values.reshape(1432318,))\n",
    "arr6=np.append(arr6,validych4.values.reshape(793782,))\n",
    "arr6=np.append(arr6,testych4.values.reshape(215137,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ca004d-024b-42c7-ac7b-928a6b56d0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(arr6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620ea49e-d3a1-4e64-a5f3-249a333e7051",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(yscalerch4.inverse_transform(arr5.reshape(2441237,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba0bc1a-0dc5-4cc4-b63b-5a76795747c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reframed_ch4.iloc[:,-1].name\n",
    "#('CH4_1_1_2', 't')\n",
    "#reframed_ch4.iloc[:,-1]['2011':'2021']#2.083M\n",
    "#reframed_ch4.iloc[:,-1][:'2021']\n",
    "#reframed_ch4.iloc[:,-1][:'2021']\n",
    "#reframed_ch4.iloc[:304966,-1]\n",
    "reframed_ch4.iloc[304966:2387849,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2d0eef-ff17-4e21-8b8a-762d4fc9064b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#arr6.reshape(2441237,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31335f39-7a29-46bd-ac5a-9338a0cfe034",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots(figsize=(10,6), dpi=1000);\n",
    "lns1=ax.plot(arr6.reshape(2441237,1), color='midnightblue', linestyle='solid', label='Flux Observations, CH4 (nmolCO2m-2s-1)');\n",
    "ax2=ax.twinx();\n",
    "lns2=ax2.plot(yscalerch4.inverse_transform(arr5.reshape(2441237,1)), color='magenta', alpha=0.5, linestyle='solid', label='Flux Predictions, CH4 (nmolCO2m-2s-1)');\n",
    "\n",
    "lns = lns1+lns2#+lns3+lns4;\n",
    "labs = [l.get_label() for l in lns];\n",
    "ax2.legend(lns, labs, loc='best', fontsize=12);\n",
    "\n",
    "ax.grid(linewidth=0.3);\n",
    "#ax2.grid(linewidth=0.3);\n",
    "ax.set_xlabel('Full Iterations (epochs)', labelpad=12, fontsize=10);\n",
    "ax.set_ylabel('Methane Flux (nmolCO2m-2s-1)', labelpad=12, fontsize=10);\n",
    "#ax.tick_params(axis='y', labelcolor='springgreen')\n",
    "#ax2.set_ylabel('Validation MSE, Scaled CO2 Flux (olCO2m-2s-1)', labelpad=12, fontsize=10)\n",
    "#ax2.tick_params(axis='y', labelcolor='yellowgreen')\n",
    "ax.tick_params(left=False)  # remove the ticks\n",
    "ax2.tick_params(right=False, labelright=False)  # remove the ticks\n",
    "plt.title('GeoCryoAI Modeling, CH4 Flux Observations v. Predictions | Alaska [2011-2021] \\n Number of CH4 Flux Samples/Replicates: 2.083M', pad=15, fontsize=14);\n",
    "ax.grid(linewidth=0.3);\n",
    "#plt.axis([304966, 2387849, 0, 2060])\n",
    "#plt.axis([0, 2441237, 0, 2060])\n",
    "plt.tight_layout()\n",
    "plt.savefig('CH4_ObsVPred_2006-2019_071323.svg', dpi=1000)\n",
    "plt.savefig('CH4_ObsVPred_2006-2019_071323.png', dpi=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2632a898-ef9a-4e80-a2d9-9b44a58f7756",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2310110f-2e1c-46bc-a857-06b97e88cb83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "355544aa-8f60-44e4-96b7-25e216a65af0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7284f56-3305-4bfe-86d1-c5c7610b799f",
   "metadata": {},
   "outputs": [],
   "source": [
    "validXaltdfres=pd.DataFrame(validXalt).to_numpy().reshape(793782, 1, 273)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d4da2e-6935-4e2a-a539-9078df629fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pp=model2.predict(validXaltdfres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322acec6-ce92-42bc-acf8-018066290271",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(pp.reshape(793782,1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81ee275-8f62-4000-950a-16c9f3e3b2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "testXaltdfres=pd.DataFrame(testXalt).to_numpy().reshape(215137, 1, 273)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20614837-916c-4950-86b6-1f5b3172f4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ppp=model2.predict(testXaltdfres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d75d0a5-edf5-4bfd-9dce-b82a92a1e427",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(ppp.reshape(215137,1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca2ceb1-24eb-44e4-b1cb-87c6ec3fa2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(yscaleralt.inverse_transform(p.reshape(1432318,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5caedd8-0697-4394-b073-81ed8b3821a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(yscaleralt.inverse_transform(pp.reshape(793782,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616d0050-57cb-4dca-acb3-8d97a4b7e832",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(yscaleralt.inverse_transform(ppp.reshape(215137,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f607d110-b992-436a-9b98-380bd279d660",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae29a71-1c80-4a01-88aa-c548ef7acbf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.DataFrame(yscaleralt.inverse_transform(p.reshape(1432318,1)))\n",
    "plt.plot(trainyalt.values)\n",
    "plt.plot(yscaleralt.inverse_transform(p.reshape(1432318,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1bfaead-2eb8-4e0c-86de-77544f47ee24",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.DataFrame(yscaleralt.inverse_transform(p.reshape(1432318,1)))\n",
    "plt.plot(validyalt.values)\n",
    "plt.plot(yscaleralt.inverse_transform(pp.reshape(793782,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fcb26d1-3a6c-4b50-8526-d9ec711d599b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(p.reshape(1432318,1))\n",
    "plt.plot(pp.reshape(793782,1))\n",
    "plt.plot(ppp.reshape(215137,1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd10e10-1ab2-405e-b1cf-cc4239bc27ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "newp=p.reshape(1432318,1)\n",
    "newpp=pp.reshape(793782,1)\n",
    "newppp=ppp.reshape(215137,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c7b390-275b-494f-92cb-18c7a0849f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# invert predictions\n",
    "sc1=StandardScaler().fit(newp)\n",
    "newTrain=sc1.inverse_transform(newp)\n",
    "sc2=StandardScaler().fit(newpp)\n",
    "newValid = sc2.inverse_transform(newpp)\n",
    "sc3=StandardScaler().fit(newppp)\n",
    "newTest = sc3.inverse_transform(newppp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d2383b-9ac2-4b32-b9cd-050a3bf7ad9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "newTrain.shape == newp.shape, newValid.shape == newpp.shape, newTest.shape == newppp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1752ef-9d06-4dda-92b8-98aa982a10a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainXaltdfres.shape, newp.shape, newTrain.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6119bbf-4652-4da2-bb4e-6faff9d99a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "testyalt.to_numpy().reshape(215137,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33113b0a-1795-44f2-97f3-6db187bd9ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate root mean squared error\n",
    "trainScore = np.sqrt(keras.losses.mean_squared_error(trainyalt.to_numpy().reshape(1432318,), newp[:,0]))\n",
    "print('Train Score: %.6f RMSE' % (trainScore))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce9033a-404e-426e-a8c7-ce550ecebb53",
   "metadata": {},
   "outputs": [],
   "source": [
    "validScore = np.sqrt(keras.losses.mean_squared_error(validyalt.to_numpy().reshape(793782,), newpp[:,0]))\n",
    "print('Valid Score: %.6f RMSE' % (validScore))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9172042-ad9a-4e8d-8dcf-dbd0940b5b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "testScore = np.sqrt(keras.losses.mean_squared_error(testyalt.to_numpy().reshape(215137,), newppp[:,0]))\n",
    "print('Test Score: %.6f RMSE' % (testScore))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d65a9fc-cb99-4572-a246-296d6756e3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "testScore2 = np.sqrt(keras.losses.mean_squared_error(newppp[0], newTest[:,0]))\n",
    "print('Test Score: %.6f RMSE' % (testScore2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fee9d44-734e-4b9f-84fa-ca7eee732f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "ln1=ax.plot(y_train, color='royalblue', linestyle='solid', label='Observed ALT')\n",
    "ln2=ax.plot(y_test, color='springgreen', linestyle='dashed', label='Tested ALT')\n",
    "ln3=ax.plot(y_valid, color='magenta', linestyle='dotted', label='Validated ALT')\n",
    "ax2=ax.twinx();\n",
    "ln4=ax2.plot(testPredict, color='yellow', linestyle='dotted', label='Test_Predicted ALT')\n",
    "\n",
    "lines = ln1 + ln2 + ln3 + ln4\n",
    "labs = [line.get_label() for line in lines];\n",
    "ax2.legend(lines, labs, loc='best')\n",
    "\n",
    "ax.grid(linewidth=0.3);\n",
    "ax.set_xlabel('Epochs', labelpad=6, fontsize=9);\n",
    "ax.set_ylabel('Loss', labelpad=6, fontsize=9)\n",
    "ax2.set_ylabel('Forecasted ALT')\n",
    "plt.title('LSTM+CNN+VAE Model (GeoCryoAI): \\n Observed v. Forecasted Active Layer Thickness (cm)')\n",
    "plt.ylabel('Scaled Active Layer Thickness (cm)')\n",
    "plt.xlabel('Epoch')\n",
    "plt.axis([0, 12000, -0.1, 1])\n",
    "#plt.legend(loc='best')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# fig,ax=plt.subplots(figsize=(10,5));\n",
    "# lns1=ax.plot(history.history['loss'], color='dodgerblue', label='Loss, Active Layer Thickness (cm)');\n",
    "# lns2=ax.plot(history.history['root_mean_squared_error'], color='dodgerblue', linestyle='dotted', label='RMSE, Active Layer Thickness (cm)');\n",
    "# ax2=ax.twinx();\n",
    "# lns3=ax2.plot(history.history['val_loss'], color='gold', label='Validation Loss, Active Layer Thickness (cm)');\n",
    "# lns4=ax2.plot(history.history['val_root_mean_squared_error'], color='gold', linestyle='dotted', label='Validation RMSE, Active Layer Thickness (cm)');\n",
    "          \n",
    "# lns = lns1+lns2+lns3+lns4;\n",
    "# labs = [l.get_label() for l in lns];\n",
    "# ax2.legend(lns, labs, loc='best', fontsize=8);\n",
    "\n",
    "# ax.grid(linewidth=0.3);\n",
    "# ax.set_xlabel('Epochs', labelpad=6, fontsize=9);\n",
    "# #ax.set_ylabel('Loss', labelpad=6, fontsize=9)\n",
    "# #ax2.set_ylabel('Scaled Depth to Refusal (cm)', labelpad=6, fontsize=9)\n",
    "# plt.title('Number of Samples/Replicates: 95653', pad=12, fontsize=8, fontweight='ultralight');\n",
    "# plt.suptitle('Cost Function and Validation Loss from Thaw Depth Modeling in LSTM-AE Framework, Alaska [1969-2022]', fontsize=11);\n",
    "# plt.grid(linewidth=0.3);\n",
    "# #plt.show()\n",
    "# #plt.savefig('ALTstats_LSTM-AEmetrics_1969-2022.png', dpi=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2afd68cd-d9a0-40ef-843b-8150c22089d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the predicted values\n",
    "yup_pred_scaled = model2.predict(testXscaltref)\n",
    "\n",
    "# Unscale the predicted values\n",
    "yup_pred = yscaleralt.inverse_transform(yup_pred_scaled.reshape(215137,1))\n",
    "yup_test_unscaled = yscaleralt.inverse_transform(testyscaltref.reshape(-1, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae38ebd-6bf4-4e78-bfb3-203ef253dcd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(yup_pred_scaled.reshape(215137,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6015fe6-d157-4ba0-9731-9675033e66fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(yup_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30bca529-9e2d-47d4-9172-e982962e36c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(yup_test_unscaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f532cf1e-bcc4-49ce-a85c-e1cd98c2e8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(trainyalt.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66cc7f7-9d61-473f-bf15-447c5c400650",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(y_testco2_reframed.reshape(215137,1))\n",
    "plt.plot(predict.reshape(215137,1))\n",
    "#plt.axis([0, 130000, -2, 5])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394f461a-beb0-4102-b31e-66423c5e69d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68147eaa-7f71-4dab-b16a-122e3091db8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = X[0:-144], X[-144:]\n",
    "# walk-forward validation\n",
    "history = [x for x in train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9855f3-91df-4a8b-a090-fbf5d0f55b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print(\"Test accuracy for the unscaled ALT data\")\n",
    "print(f\"{accuracy_score(testyaltdfres, y_pred):.2%}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6695526-a871-4b06-84d3-b9f5d632b97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "testyaltdfres=pd.DataFrame(testyalt).to_numpy().reshape(215137, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b97118-105b-47e7-b3b6-5ef750784116",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print(\"Test accuracy for the unscaled ALT data\")\n",
    "print(f\"{accuracy_score(testyaltdfres, y_pred):.2%}\\n\")\n",
    "print(\"Test accuracy for the standardized ALT data\")\n",
    "print(f\"{accuracy_score(testyscalt, y_pred_scaled):.2%}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9975cb91-b209-44ba-bd2e-73f893579ade",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb9b14b-1320-41fb-b8aa-768439939399",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainXaltdfres=pd.DataFrame(trainXalt).to_numpy().reshape(1432318, 1, 273)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aad031d-9f77-46d3-8fe9-d9437197ba9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model2.predict(trainXaltdfres)\n",
    "y_pred_scaled = model2.predict(testXscalt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e46271-1b0a-4849-bd8f-976cd9ce0e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "testyaltdfres=pd.DataFrame(testyalt).to_numpy().reshape(215137, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2e8a8c-5e90-4469-9d1e-a6cfaf11b758",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print(\"Test accuracy for the unscaled ALT data\")\n",
    "print(f\"{accuracy_score(testyaltdfres, y_pred):.2%}\\n\")\n",
    "print(\"Test accuracy for the standardized ALT data\")\n",
    "print(f\"{accuracy_score(testyscalt, y_pred_scaled):.2%}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e561be43-9f59-4388-80e0-215cb49d6dc9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85efa31-23ab-4159-ae95-d5127de6ad3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_predicted_reframed = model2.predict(testXaltdfres, verbose = 1, use_multiprocessing = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b09b9d85-e7ef-4d5e-b4cf-4522060d75cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(testyscalt)\n",
    "plt.plot(Y_predicted_reframed.reshape(215137,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f414768-1f57-47ca-aaba-ab56ef9f677e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_predicted_reframed = bayesian_best_model.predict(X_test_reframed,  batch_size = 384, verbose = 1, use_multiprocessing = True)\n",
    "\n",
    "Y_predicted_scaled = Y_predicted_reframed.reshape(Y_predicted_reframed.shape[0], Y_predicted_reframed.shape[1])\n",
    "\n",
    "Y_predicted = scaler_Y.inverse_transform(Y_predicted_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ada39de-3c28-4053-ba47-06a1e054ffe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(Y_test, Y_predicted, title = \"Test Data and Predictions\", index = None):\n",
    "    if index is None:\n",
    "        index = range(0, Y_test.shape[0])\n",
    "    df_index = pd.DataFrame(data = index, index = range(0, Y_test.shape[0]))\n",
    "    df_index.columns = [\"user_index\"]  \n",
    "\n",
    "    shift = Y_test.shape[0] - Y_predicted.shape[0]\n",
    "\n",
    "    fig, axes = plt.subplots(figsize = (9, 6), sharex = True, nrows = Y_test.shape[1], squeeze = False)\n",
    "    \n",
    "    for target, ax in enumerate(axes.flat):\n",
    "        ax.step(df_index.values, Y_test[:,target], where = \"post\", label = \"Testing Set\", color = \"blue\")\n",
    "        ax.step(df_index.loc[shift:, \"user_index\"].values, Y_predicted[:,target], where = \"post\", \n",
    "                label = \"Predictions\", color = \"red\")\n",
    "    \n",
    "    plt.suptitle(title)\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc12b875-3bd8-4f4c-8ff3-e2a706eeb6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(scaler_Y.inverse_transform(Y_test.values), Y_predicted, index = Y_test.index, \n",
    "             title = \"Active Layer Thickness /n Predictions v. Test Data \\n via 8-layer C1DLSTMSAE Network\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ce14c8-7530-4ae1-82aa-c2dac38b6a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_predicted=Y_predicted.reshape(397492,)#;Y_test=Y_test.reshape(668168,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2987730c-fcc8-46e7-9e40-5387e993f96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def qq_plot(Y_test, Y_predicted, title = \"Test Data and Predictions\", index = None):\n",
    "    if index is None:\n",
    "        index = range(0, Y_test.shape[0])\n",
    "    df_index = pd.DataFrame(data = index, index = range(0, Y_test.shape[0]))\n",
    "    df_index.columns = [\"user_index\"]  \n",
    "\n",
    "    shift = Y_test.shape[0] - Y_predicted.shape[0]\n",
    "\n",
    "    fig, axes = plt.subplots(figsize = (9, 6), sharex = True, nrows = Y_test.shape[1], squeeze = False)\n",
    "    \n",
    "    for target, ax in enumerate(axes.flat):\n",
    "        ax.scatter(Y_test[shift:,target], Y_predicted[:,target], label = \"Predictions\", color = \"red\", s = 5, \n",
    "                   alpha = 0.5)\n",
    "        ax.scatter(Y_test[:,target], Y_test[:,target], label = \"Testing Set\", color = \"blue\", s = 5)\n",
    "\n",
    "    plt.suptitle(title)\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50bbada7-fefb-432f-9b14-8cf35164cf05",
   "metadata": {},
   "outputs": [],
   "source": [
    "qq_plot(Y_test.values, Y_predicted_scaled, index = Y_test.index, \n",
    "        title = \"Active Layer Thickness /n Predictions v. Test Data via \\n 7-layer Sequential Time-Distributed C1DLSTMSAE Network\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465f5388-f499-49e4-9f24-2700cbd3e842",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d543590c-421d-4d1d-bb19-9b12f94fb004",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949337e6-6927-4393-8682-86a11d3098dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_predicted_reframed = model.predict(X_test_reframed)\n",
    "Y_predicted = Y_predicted_reframed.reshape(Y_predicted_reframed.shape[0], Y_predicted_reframed.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0ed53a-d4a6-4a2e-b297-50ceaa0699fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "testim=pd.read_csv(r'/Users/bradleygay/test_store_ALT_2022.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff8ac69-36f0-4ed3-8921-6006ac49e53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "testim.index=testim.iloc[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a36cc9-7b28-4d9e-ae52-24d42bd87f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "testim=testim.drop(testim.columns[0],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee1ece9-d02f-4b65-b6d9-a9806c85af52",
   "metadata": {},
   "outputs": [],
   "source": [
    "testim.index.name = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e5cf17-9e9d-42fb-844b-a55ee8b27c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "#testim.index=pd.to_datetime(testim.index, format='%Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ee7cad-0f90-45ad-b7c9-f1464161d0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "testim=testim.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3818ef-ce35-4c44-8da4-495d97335412",
   "metadata": {},
   "outputs": [],
   "source": [
    "testim.index = pd.to_datetime(testim.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3aad8b-f089-4b2a-bd0e-8b16b4d076ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "testim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de21d4c-a433-4630-a236-fb3be5e6aad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(scaler_Y.inverse_transform(testim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13106ac-c3d8-4c8f-a7cf-105b5de1ab65",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(scaler_Y.inverse_transform(Y_test.values), scaler_Y.inverse_transform(Y_predicted),\n",
    "             index = Y_test.index, title = \"Active Layer Thickness - Predicteed v. Test Data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be565e6f-d6df-4ed5-a479-e0fe4f245ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_scores = model.evaluate(X_test_reframed, Y_test[backward_steps:],\n",
    "                                           batch_size=hp[\"batch_size\"], use_multiprocessing=True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f4e04e-ff56-44ff-bab9-7ef782248e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics_print(test_data,test_predict):\n",
    "    print('Test RMSE: ', round(np.sqrt(sklearn.metrics.mean_squared_error(test_data, test_predict)), 2))\n",
    "    print('Test R^2 : ', round((sklearn.metrics.r2_score(test_data, test_predict)*100), 2) ,\"%\")\n",
    "    print('Test MAPE: ', round(sklearn.metrics.mean_absolute_percentage_error(test_data, test_predict)*100,2), '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c6a361-af8c-417b-aa88-e2c7fe7a4cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"##************** Linear Regression Results **************##\")\n",
    "metrics_print(prediction_df['Observed'], prediction_df['LR'])\n",
    "print(\" \")\n",
    "print(\" \")\n",
    "\n",
    "print(\"##************** Deep Learning Results **************##\")\n",
    "metrics_print(prediction_df['Observed'], prediction_df['DNN'])\n",
    "print(\" \")\n",
    "print(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e3d8de-2f2c-4f07-bf24-30f17c3a87e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fa = plt.figure(figsize=(16,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.scatter(prediction_df['Observed'],prediction_df['LR'])\n",
    "plt.xlabel('True Values [snow_depth]', fontsize=15)\n",
    "plt.ylabel('Predictions [snow_depth]', fontsize=15)\n",
    "plt.title(\"Linear Regression\")\n",
    "\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.scatter(prediction_df['Observed'],prediction_df['DNN'])\n",
    "plt.xlabel('True Values [snow_depth]', fontsize=15)\n",
    "plt.ylabel('Predictions [snow_depth]', fontsize=15)\n",
    "plt.title(\"Deep Neural Network\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2bad50-aef3-4d26-a434-52f62ac675a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "LR_error = prediction_df['Observed'] - prediction_df['LR']\n",
    "DNN_error = prediction_df['Observed'] - prediction_df['DNN']\n",
    "\n",
    "fa = plt.figure(figsize=(16,5))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "LR_error.hist()\n",
    "plt.xlabel('Error', fontsize=15)\n",
    "plt.ylabel('Frequency', fontsize=15)\n",
    "plt.title(\"Linear Regression\")\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "DNN_error.hist()\n",
    "plt.xlabel('Error', fontsize=15)\n",
    "plt.ylabel('Frequency', fontsize=15)\n",
    "plt.title(\"Deep Neural Network\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d57909-a932-4478-8825-4cbf2bfe9c29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa95128e-10b5-4fbc-9d3f-7eb8b486c700",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc6748d-eccc-4c39-b368-5e23f4da3c2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c839704-6bc4-42ba-bc29-03e06efae51d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce89c3f1-8d1f-4612-aac5-e6d11a55c3e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae79015-6888-45b3-a232-804ab7af35b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baebaebe-c63b-4bba-82be-e0a3f09c6e38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c14e3b-0eba-4202-adb7-98de9639cb1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3862e6dd-31d5-4432-a6be-b84e5025db62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae0ea2c-9837-44fa-913f-7828959914a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainXaltdfres=pd.DataFrame(trainXalt).to_numpy().reshape(1432318, 1, 273)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a50035d-9527-4405-b717-a6a1d29e5517",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model2.predict(trainXaltdfres)\n",
    "y_pred_scaled = model2.predict(testXscalt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304c6baa-289e-4008-9ab0-ad2a7726bc69",
   "metadata": {},
   "outputs": [],
   "source": [
    "testyaltdfres=pd.DataFrame(testyalt).to_numpy().reshape(215137, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81aaf5c7-1392-4418-8a17-ce33b6fe6940",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print(\"Test accuracy for the unscaled ALT data\")\n",
    "print(f\"{accuracy_score(testyaltdfres, y_pred):.2%}\\n\")\n",
    "print(\"Test accuracy for the standardized ALT data\")\n",
    "print(f\"{accuracy_score(testyscalt, y_pred_scaled):.2%}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7ec1b0-a4d7-47f6-bbf6-5fcbf58ee39a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe3ae43-eac9-48df-9cb8-c983a53b3fae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7eec57a-cc5a-4eda-ae3e-69bb272d291e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ALT\n",
    "plt.plot(yscaleralt.inverse_transform(testyscalt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581681a0-b226-4593-8727-c091c3d083ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CH4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b92610-9288-44ed-850a-9bf6e6abffb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CO2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054bfb83-5281-4b8b-9dbe-63c57f3f4554",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36a8626-4cf7-46e2-aa59-8af60a1ce630",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c91263b-83de-4cb7-ab3f-02a060bb8953",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8be627-5ed9-4a2e-808a-99984279f70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#alt_model.predict(testXscaltref, testyscaltref, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fae3dfb-c349-4d45-89df-aa37dab33e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "one=np.concatenate((trainyalt.resample('Y').mean(), validyalt.resample('Y').mean()), axis=0)\n",
    "two=np.concatenate((one, testyalt.resample('Y').mean()), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f14416-d903-49af-abbc-063d6dcfea64",
   "metadata": {},
   "outputs": [],
   "source": [
    "abc=pd.DataFrame(yscaler.inverse_transform(testyscaltpredres))\n",
    "#yscaler.inverse_transform(testyscaltpredres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99680703-cc98-4934-bb36-371c95e55363",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(two)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba4868c-8446-4836-afb9-a00de12f0a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainyalt.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e434f0-c38e-484b-bdfd-0d17fe75e191",
   "metadata": {},
   "outputs": [],
   "source": [
    "three=pd.DataFrame(trainyscaltpredinv)\n",
    "#np.concatenate(trainyscaltpredinv, validyscaltpredinv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311ebc49-d7d5-481e-b90f-f0ab79ca77da",
   "metadata": {},
   "outputs": [],
   "source": [
    "three.index=trainyalt.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d633fb5-dcb0-4858-ab3a-026652c59fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(three)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac6cae1-77de-48af-b319-ffc2a3cd4e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "two=np.concatenate(one, validyalt.resample('Y').mean()), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327720e9-4e38-440a-9b1e-129003b3db69",
   "metadata": {},
   "outputs": [],
   "source": [
    "one=np.concatenate((trainyalt.resample('Y').mean(), validyalt.resample('Y').mean()), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a198744f-8911-4040-951e-c17a3e82589e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95689709-894c-4b93-84ed-b287964691ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(trainyalt.resample('Y').mean())\n",
    "plt.plot(validyalt.resample('Y').mean())\n",
    "plt.plot(testyalt.resample('Y').mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdeacdb7-49b2-48d0-855d-1e9f54083572",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions\n",
    "trainPredict = model.predict(trainX)\n",
    "testPredict = model.predict(testX)\n",
    "# invert predictions\n",
    "trainPredict = scaler.inverse_transform(trainPredict)\n",
    "trainY = scaler.inverse_transform([trainY])\n",
    "testPredict = scaler.inverse_transform(testPredict)\n",
    "testY = scaler.inverse_transform([testY])\n",
    "# calculate root mean squared error\n",
    "trainScore = math.sqrt(mean_squared_error(trainY[0], trainPredict[:,0]))\n",
    "print('Train Score: %.2f RMSE' % (trainScore))\n",
    "testScore = math.sqrt(mean_squared_error(testY[0], testPredict[:,0]))\n",
    "print('Test Score: %.2f RMSE' % (testScore))\n",
    "#-----Visualize---------- \n",
    "# shift train predictions for plotting\n",
    "trainPredictPlot = numpy.empty_like(dataset)\n",
    "trainPredictPlot[:, :] = numpy.nan\n",
    "trainPredictPlot[look_back:len(trainPredict)+look_back, :] = trainPredict\n",
    "# shift test predictions for plotting\n",
    "testPredictPlot = numpy.empty_like(dataset)\n",
    "testPredictPlot[:, :] = numpy.nan\n",
    "testPredictPlot[len(trainPredict)+(look_back*2)+1:len(dataset)-1, :] = testPredict\n",
    "# plot baseline and predictions\n",
    "plt.plot(scaler.inverse_transform(dataset))\n",
    "plt.plot(trainPredictPlot)\n",
    "plt.plot(testPredictPlot)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb233c95-5112-43e0-8465-2d4f286ac7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['ALT']['2021':].shape\n",
    "#1432321/2441240#0.5867186347921548\n",
    "#793782/2441240#0.325155248971834\n",
    "#215137/2441240#0.0881261162360112"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9ab9b8-fd6c-4938-9a9f-6b64caccf865",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(df.ALT['1970':'2017'][1:].to_numpy().reshape(1432317,1), trainyalt)\n",
    "# plt.plot(df.ALT['2018':'2020'].values, validyalt)\n",
    "# plt.plot(df.ALT['2021':][1:].to_numpy().reshape(215136,1), testyalt)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d394a7-2713-44df-bc5a-ade01da563e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(10,6))\n",
    "# plt.plot(trainyscalt, label='train')\n",
    "# plt.plot(trainyscaltpredres, label='trainpred')\n",
    "# plt.plot(validyscalt, label='valid')\n",
    "# plt.plot(validyscaltpredres, label='validpred')\n",
    "# plt.plot(testyscalt, label='test')\n",
    "# plt.plot(testyscaltpredres, label='testpred')\n",
    "# plt.legend()\n",
    "# #plt.axis(xmin=0, xmax=250000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8b2038-7bf9-4c93-91d3-fba193043563",
   "metadata": {},
   "outputs": [],
   "source": [
    "testXscalt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259d333f-a2ce-4c90-bbc0-8a33d143f7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "metrics.r2_score(testyscalt, testyscaltpred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4f7213-ec25-48e3-92c0-3da61c9ac706",
   "metadata": {},
   "outputs": [],
   "source": [
    "#invyhat=np.concatenate((testyscaltpredres,testXscalt[:,-1:]),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea218e0-5b7b-4ec8-a664-dc3f92e94056",
   "metadata": {},
   "outputs": [],
   "source": [
    "#invyhat=yscaler.inverse_transform(invyhat)[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ecec4b-2aad-4821-8c5f-d9eb97a8ab31",
   "metadata": {},
   "outputs": [],
   "source": [
    "#invy=np.concatenate((testyscalt.reshape((len(testyscalt), 1)),testXscalt[:,1:]),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f88299-2dbb-4f17-b214-151583fe74ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#invy=yscaler.inverse_transform(invy)[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd011b6-ee1e-4e89-999e-a302873d7c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#rmse = np.sqrt(mean_squared_error(invy, invyhat))\n",
    "#print('Test RMSE: %.3f' % rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "622b345b-13b9-43d9-9889-a203fd908d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# yhat = model3.predict(test_X)\n",
    "# test_X = test_X.reshape((test_X.shape[0], test_X.shape[2]))\n",
    "# # invert scaling for forecast\n",
    "# inv_yhat = concatenate((yhat, test_X[:, 1:]), axis=1)\n",
    "# inv_yhat = scaler.inverse_transform(inv_yhat)\n",
    "# inv_yhat = inv_yhat[:,0]\n",
    "# # invert scaling for actual\n",
    "# test_y = test_y.reshape((len(test_y), 1))\n",
    "# inv_y = concatenate((test_y, test_X[:, 1:]), axis=1)\n",
    "# inv_y = scaler.inverse_transform(inv_y)\n",
    "# inv_y = inv_y[:,0]\n",
    "# # calculate RMSE\n",
    "# rmse = sqrt(mean_squared_error(inv_y, inv_yhat))\n",
    "# print('Test RMSE: %.3f' % rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e3474f-dc1a-429f-bc28-f617955badda",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainXalt.shape, trainXscalt.shape, trainXscaltref.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47abd21c-7872-4834-8abc-c9188154a92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainyscaltpredres=trainyscaltpred.reshape(1432317,1)\n",
    "validyscaltpredres=validyscaltpred.reshape(793782,1)\n",
    "testyscaltpredres=testyscaltpred.reshape(215136,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7896b6-7142-4680-b61c-3b42955304b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(list(trainXalt.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8320f4c4-25f8-4425-880e-3131e0f3111f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ind=testyalt.index.values\n",
    "#ind=trainyalt.index.values\n",
    "ind=validyalt.index.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e24d3a-776c-4e85-bd61-5bce4d734cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(ind,yscaler.inverse_transform(trainyscalt))\n",
    "plt.plot(ind,yscaler.inverse_transform(trainyscaltpredres))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63702cab-be8b-49bc-a848-f1329f313091",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(ind,yscaler.inverse_transform(validyscalt))\n",
    "plt.plot(ind,yscaler.inverse_transform(validyscaltpredres))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84173490-60a7-45cb-9997-d2c0adb97270",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(ind,yscaler.inverse_transform(testyscalt))\n",
    "plt.plot(ind,yscaler.inverse_transform(testyscaltpredres))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d01ae89-d573-498a-b518-a92dde0a016f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804e54fd-50ed-446d-b49a-342a60c12fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainyscaltpredinv=yscaler.inverse_transform(trainyscaltpredres)\n",
    "validyscaltpredinv=yscaler.inverse_transform(validyscaltpredres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe26db95-f0fb-45f4-9178-be07c328e8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainyscaltpredinv.shape, validyscaltpredinv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f22c5c99-5b6f-4f40-b094-5f80f60c8eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "print(mean_squared_error(trainyalt, trainyscaltpredinv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a78e180-3431-4884-96c9-a3d2851f8a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "print(mean_squared_error(train, y_train_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c0ea6e0-d77b-4d5f-9e45-12775ce19a3c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Exploratory Plots (Active Layer Thickness, Carbon Dioxide, Methane)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3dfd56-2f67-42dc-bc09-649caf0c20f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig,ax=plt.subplots(figsize=(10,7));\n",
    "# #lns1=ax.plot(history2.history['loss'], color='dodgerblue', label='Loss, ALT (cm)');\n",
    "# lns2=ax.plot(history2.history['mean_squared_error'], color='dodgerblue', linestyle='solid', label='RMSE, ALT (cm)');\n",
    "# ax2=ax.twinx();\n",
    "# #lns3=ax2.plot(history2.history['val_loss'], color='gold', label='Validation Loss, ALT (cm)');\n",
    "# lns4=ax2.plot(history2.history['val_mean_squared_error'], color='gold', linestyle='solid', label='Validation RMSE, ALT (cm)');\n",
    "          \n",
    "# lns = lns2+lns4; #lns1+lns2+lns3+lns4;\n",
    "# labs = [l.get_label() for l in lns];\n",
    "# ax2.legend(lns, labs, loc='best', fontsize=8);\n",
    "\n",
    "# ax.grid(linewidth=0.3);\n",
    "# ax.set_xlabel('Epochs', labelpad=12, fontsize=10);\n",
    "# ax.set_ylabel('Scaled Depth to Refusal (cm)', labelpad=12, fontsize=10);\n",
    "# #ax2.set_ylabel('Scaled Depth to Refusal (cm)', labelpad=6, fontsize=9)\n",
    "# plt.title('Number of Samples/Replicates: 95653', pad=15, fontsize=12, fontweight='ultralight');\n",
    "# plt.suptitle('Cost Function and Validation Loss from Thaw Depth Modeling, GeoCryoAI Framework in Alaska [1969-2022]', fontsize=14);\n",
    "# plt.grid(linewidth=0.3);\n",
    "# #plt.show()\n",
    "# #plt.savefig('ALTstats_CNNLSTMSAEmetrics_1969-2022_021323.png', dpi=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af0c50ec-fb7a-4e24-87ad-237dff134c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig,ax=plt.subplots(figsize=(10,7));\n",
    "# lns1=ax.plot(history.history['loss'], color='dodgerblue', label='Loss, Carbon Dioxide Mole Fraction (molCO2mol-1ms-1)');\n",
    "# lns2=ax.plot(history.history['root_mean_squared_error'], color='dodgerblue', linestyle='dotted', label='RMSE, Carbon Dioxide Mole Fraction (molCO2mol-1m)');\n",
    "# ax2=ax.twinx();\n",
    "# lns3=ax2.plot(history.history['val_loss'], color='gold', label='Validation Loss, Active Layer Thickness (cm)');\n",
    "# lns4=ax2.plot(history.history['val_root_mean_squared_error'], color='gold', linestyle='dotted', label='Validation RMSE, Carbon Dioxide Mole Fraction (molCO2mol-1m)');\n",
    "          \n",
    "# lns = lns1+lns2+lns3+lns4;\n",
    "# labs = [l.get_label() for l in lns];\n",
    "# ax2.legend(lns, labs, loc='best', fontsize=8);\n",
    "\n",
    "# ax.grid(linewidth=0.3);\n",
    "# ax.set_xlabel('Epochs', labelpad=6, fontsize=9);\n",
    "# #ax.set_ylabel('Loss', labelpad=6, fon#tsize=9)\n",
    "# #ax2.set_ylabel('Scaled Depth to Refusal (cm)', labelpad=6, fontsize=9)\n",
    "# plt.title('Number of Samples/Replicates: 95653', pad=12, fontsize=8, fontweight='ultralight');\n",
    "# plt.suptitle('Cost Function and Validation Loss from Carbon Dioxide Mole Fraction Modeling in LSTM-AE Framework, Alaska [1969-2022]', fontsize=11);\n",
    "# plt.grid(linewidth=0.3);\n",
    "# #plt.show()\n",
    "# #plt.savefig('ALTstats_LSTM-AEmetrics_1969-2022-x2.png', dpi=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11ef0cc-df95-4016-9eba-33d1f36942de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig,ax=plt.subplots(figsize=(10,5));\n",
    "# lns1=ax.plot(history.history['loss'], color='dodgerblue', label='Loss, Methane Flux (nmolCO2m-2s)');\n",
    "# lns2=ax.plot(history.history['root_mean_squared_error'], color='dodgerblue', linestyle='dotted', label='RMSE, Methane Flux (nmolCO2m-2s)');\n",
    "# ax2=ax.twinx();\n",
    "# lns3=ax2.plot(history.history['val_loss'], color='gold', label='Validation Loss, Active Layer Thickness (cm)');\n",
    "# lns4=ax2.plot(history.history['val_root_mean_squared_error'], color='gold', linestyle='dotted', label='Validation RMSE, Methane Flux (nmolCO2m-2s)');\n",
    "          \n",
    "# lns = lns1+lns2+lns3+lns4;\n",
    "# labs = [l.get_label() for l in lns];\n",
    "# ax2.legend(lns, labs, loc='best', fontsize=8);\n",
    "\n",
    "# ax.grid(linewidth=0.3);\n",
    "# ax.set_xlabel('Epochs', labelpad=6, fontsize=9);\n",
    "# #ax.set_ylabel('Loss', labelpad=6, fon#tsize=9)\n",
    "# #ax2.set_ylabel('Scaled Depth to Refusal (cm)', labelpad=6, fontsize=9)\n",
    "# plt.title('Number of Samples/Replicates: 120539', pad=12, fontsize=8, fontweight='ultralight');\n",
    "# plt.suptitle('Cost Function and Validation Loss from Methane Flux Modeling in LSTM-AE Framework, Alaska [2015-2018]', fontsize=11);\n",
    "# plt.grid(linewidth=0.3);\n",
    "# #plt.show()\n",
    "# #plt.savefig('ALTstats_LSTM-AEmetrics_1969-2022-x2.png', dpi=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5852b55b-7ed9-458e-8286-262c8dd0c12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test=alt.loc[\"2022\"].replace(-9999,np.nan).dropna().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8fbc4fd-1798-4fe5-b0ce-5bbb1ff42a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "test=np.reshape(test, (test.shape[0], test.shape[1], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c2b416-e15d-47a0-ae1d-1a177d0c8907",
   "metadata": {},
   "outputs": [],
   "source": [
    "testX=test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b78b5b-ed46-40c9-af15-e128fc603ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "testy=np.reshape(test, (test.shape[0],))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd68907-694a-4528-8c4d-1b43ff38ae3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075251a8-adc3-4a2a-9f81-523548b05c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "testX.shape, testy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9c2f7f-31ba-4268-bac5-bb1bc7bd7c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(X_testsc,y_testsc)\n",
    "#Not an accurate depiction due to scaling; must invert prior to quantifying error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d36917-ac0f-43fa-ac71-ef774d75294c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_Xt=np.array(trainX).reshape(1772962,1)\n",
    "# train_yt=np.array(trainY).reshape(1772962,)\n",
    "# valid_Xt=np.array(validX).reshape(453143,1)\n",
    "# valid_yt=np.array(validY).reshape(453143,)\n",
    "# test_Xt=np.array(testX).reshape(215135,1)\n",
    "# test_yt=np.array(testY).reshape(215135,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65e12ed-7de7-4851-88ce-1c957b8def02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lin_reg = linear_model.LinearRegression()\n",
    "# # train model\n",
    "# lin_reg.fit(train_Xt,train_yt)\n",
    "# # predict\n",
    "# y_train_pred = lin_reg.predict(train_Xt)\n",
    "# y_valid_pred = lin_reg.predict(valid_Xt)\n",
    "# y_test_pred = lin_reg.predict(test_Xt)\n",
    "# # Plot predictions\n",
    "# fig=plt.figure()\n",
    "# plt.scatter(y_train_pred, train_yt, c = \"blue\", marker = \"s\", label = \"Training data\")\n",
    "# plt.scatter(y_valid_pred, valid_yt, c = \"magenta\", marker = \"s\", label = \"Validation data\")\n",
    "# plt.scatter(y_test_pred, test_yt, c = \"lightgreen\", marker = \"s\", label = \"Testing data\")\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4d6949-6de7-426b-b8d6-2c2ac68fb1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import mean_squared_error\n",
    "# error =np.sqrt(mean_squared_error(valid_yt, y_valid_pred))\n",
    "# print(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31b27b0-cc5f-4083-8b64-096d1ce2d336",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction=lin_reg.predict(valid_Xt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082f6057-5054-474b-a457-ec5e0541068a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(valid_yt, linestyle='dotted');\n",
    "# plt.plot(prediction, linestyle='dotted');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4070f9e-46f9-4f07-ad31-8c050b88424b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.ylabel('loss'); plt.xlabel('epoch')\n",
    "plt.semilogy(history.history['loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87df9fac-e244-4b22-b7be-76a6770e9a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "axes=plt.axes()\n",
    "#axes.plot(pd.DataFrame(history.history)['loss'], label='Loss')\n",
    "axes.plot(pd.DataFrame(history.history)['val_loss'], label='Validation Loss')\n",
    "axes.legend(loc=0)\n",
    "axes.set_title('Model fitting performance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b545c21-e06e-44b5-ac38-4cbd4779df42",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1,#3\n",
    "axes=plt.axes()\n",
    "axes.plot(pd.DataFrame(history.history)['loss'], label='Loss')\n",
    "axes.plot(pd.DataFrame(history.history)['val_loss'], label='Validation Loss')\n",
    "axes.legend(loc=0)\n",
    "axes.set_title('Model fitting performance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37024caa-7b68-4b6c-b54a-288718516c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc=StandardScaler()\n",
    "sc.fit_transform(alt)\n",
    "newtest=alt.loc[\"2020\":\"2022\":,:].values\n",
    "newtest=sc.transform(np.reshape(alt.loc[\"2020\":\"2022\":,:].values, (-1, 1)))\n",
    "newtest=np.reshape(newtest, (newtest.shape[0],newtest.shape[1],1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21b7da9-6507-473f-8d5c-82b97a4760a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "newtest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11bd6fef-1808-40c2-8c01-e0acab7209ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds=model.predict(newtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36312f4f-8e70-4826-ac95-463d7a84a619",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds=preds.reshape(11050,1)\n",
    "unspreds=sc.inverse_transform(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e314a3-e592-4645-82ab-6adebdd6bade",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(unspreds, color = '#135485', linestyle='solid', label = \"Predictions\")\n",
    "plt.plot(alt.loc[\"2020\":\"2022\":,:].values, color = 'pink', linestyle='dotted', label = \"Real Data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986fed38-714a-4348-8612-c58783757215",
   "metadata": {},
   "outputs": [],
   "source": [
    "newtest=alt.loc[\"2020\":\"2022\":,:].values\n",
    "testScore = np.sqrt(keras.losses.mean_squared_error(newtest[0], unspreds[:,0]))\n",
    "print('Test Score: %.6f RMSE' % (testScore))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ee21fe-bb67-4388-a653-d34005942f6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "376b34b8-7dcf-4ec6-9153-2ccd71ff2763",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc=StandardScaler()\n",
    "sc.fit_transform(alt)\n",
    "#make predictions\n",
    "trainPredict = model.predict(trainX)\n",
    "validPredict = model.predict(validX)\n",
    "#invert predictions\n",
    "trainPredict=trainPredict.reshape(68801,1)\n",
    "validPredict=validPredict.reshape(18902,1)\n",
    "trainy=trainy.reshape(68801,)\n",
    "validy=validy.reshape(18902,)\n",
    "trainPredict = sc.inverse_transform(trainPredict)\n",
    "trainy = sc.inverse_transform([trainy])\n",
    "validPredict = sc.inverse_transform(validPredict)\n",
    "validy = sc.inverse_transform([validy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a8c071-581f-4524-891d-94b095893d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate root mean squared error\n",
    "trainScore = np.sqrt(keras.losses.mean_squared_error(trainy[0], trainPredict[:,0]))\n",
    "print('Train Score: %.6f RMSE' % (trainScore))\n",
    "validScore = np.sqrt(keras.losses.mean_squared_error(validy[0], validPredict[:,0]))\n",
    "print('Valid Score: %.6f RMSE' % (validScore))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6521afd5-10b2-446e-97d0-c840655127b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test=altsc.loc[\"2020\":\"2022\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a14669-e739-45b1-98cf-4a095d5df6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7caae70d-898b-4b50-8de3-bb219a542d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "testPredict=model.predict(test.values.reshape(3363,1,1))\n",
    "testPredict=testPredict.reshape(3363,1)\n",
    "testy=test.values.reshape(3363,)\n",
    "testPredict=sc.inverse_transform(testPredict)\n",
    "testy=sc.inverse_transform([testy])\n",
    "testScore = np.sqrt(keras.losses.mean_squared_error(testy[0], testPredict[:,0]))\n",
    "print('Test Score: %.6f RMSE' % (testScore))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de960d2b-0e00-4c66-bb89-d60c27a2f6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainPredict.shape, validPredict.shape, testPredict.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a20dd07-4707-4127-b0c8-2b9391f2b1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.plot(trainX.reshape(68801,1))\n",
    "#plt.plot(validX.reshape(18902,1))\n",
    "plt.plot(trainPredict.reshape(68801,1))\n",
    "plt.plot(validPredict.reshape(18902,1))\n",
    "plt.plot(testPredict.reshape(3363,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a1a2af-fd48-47ce-8b9e-8ded215f5da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "testPredict=testPredict.reshape(11050,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8a75d0-86bb-4d0d-ad19-361e1b25cd00",
   "metadata": {},
   "outputs": [],
   "source": [
    "testScore = np.sqrt(keras.losses.mean_squared_error(testX[0], testPredict[:,0]))\n",
    "print('Test Score: %.6f RMSE' % (testScore))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b95ec91-d340-4cff-8e53-7e8eb2f3ad82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions\n",
    "trainPredict = model.predict(trainX)\n",
    "validPredict = model.predict(validX)\n",
    "testPredict = model.predict(testX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dca2fe7-0eb3-4729-b5d7-c0e0eed4216f",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainPredict.shape, validPredict.shape, testPredict.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3eb60b-77e6-4025-8058-ccbe6a801839",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # make predictions\n",
    "# #test_Xt=np.array(test_X)\n",
    "# #test_yt=np.array(test_y)\n",
    "# trainPredict.shape, testPredict.shape\n",
    "# trainPredict=trainPredict.reshape(68801,1)\n",
    "# validPredict=validPredict.reshape(18902,1)\n",
    "# testPredict=testPredict.reshape(11050,1)\n",
    "# trainPredict.shape, testPredict.shape, validPredict.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd89bce3-16e1-4b1d-80e1-419efc3deea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # make predictions\n",
    "# #test_Xt=np.array(test_X)\n",
    "# #test_yt=np.array(test_y)\n",
    "# trainPredict.shape, testPredict.shape\n",
    "# trainPredict=trainPredict.reshape(298498,1)\n",
    "# validPredict=validPredict.reshape(144987,1)\n",
    "# testPredict=testPredict.reshape(103616,1)\n",
    "# trainPredict.shape, testPredict.shape, validPredict.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ff60e4-360f-4ad8-afae-5e163814b274",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions\n",
    "#test_Xt=np.array(test_X)\n",
    "#test_yt=np.array(test_y)\n",
    "trainPredict=trainPredict.reshape(68801,1)\n",
    "validPredict=validPredict.reshape(18902,1)\n",
    "testPredict=testPredict.reshape(11050,1)\n",
    "trainPredict.shape, validPredict.shape, testPredict.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49f2326-900d-405b-a135-c607db972ccd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d29567-bf3d-40b0-88ee-e4e97b7dba34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# invert prediction\n",
    "sc=StandardScaler()\n",
    "newTrain=sc.fit_transform(trainPredict)\n",
    "newTrain=sc.inverse_transform(trainPredict)\n",
    "sc=StandardScaler()\n",
    "newValid = sc.fit(validPredict)\n",
    "newValid = sc.inverse_transform(validPredict)\n",
    "#sc=StandardScaler().fit_transform(testPredict)\n",
    "newTest = testPredict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad90c34e-61ea-402d-bee2-493f4ecdbc99",
   "metadata": {},
   "outputs": [],
   "source": [
    "newTrain.shape == trainPredict.shape, newValid.shape == validPredict.shape, newTest.shape == testPredict.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4ad46b-ae2c-4333-a1da-788166408219",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX.shape, trainPredict.shape, newTrain.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59ccd53-f7ca-49b2-b3de-f7a1a50b1276",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.plot(X_train)\n",
    "#plt.plot(trainPredict)\n",
    "#plt.plot(newTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2379b2d-e3e1-4fa8-9655-ed3eee6a19be",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train\": 1772964*91)\n",
    "print(\"Valid\": 614894*91)\n",
    "print(\"Test\": 53388*91)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b270095c-d027-46de-9d80-c310f575765d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate root mean squared error\n",
    "trainScore = np.sqrt(keras.losses.mean_squared_error(newTrain[0], trainPredict[:,0]))\n",
    "print('Train Score: %.6f RMSE' % (trainScore))\n",
    "validScore = np.sqrt(keras.losses.mean_squared_error(newValid[0], validPredict[:,0]))\n",
    "print('Valid Score: %.6f RMSE' % (validScore))\n",
    "testScore = np.sqrt(keras.losses.mean_squared_error(newTest[0], testPredict[:,0]))\n",
    "print('Test Score: %.6f RMSE' % (testScore))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a2fc57f-851e-436c-bd92-e5b00c1db9f4",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8fb2b7a-2c9e-4dec-8181-16bd75478940",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:geo]",
   "language": "python",
   "name": "conda-env-geo-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
